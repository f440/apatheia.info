<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>tagged lxc - aptheia.info</title><link rel="alternate" type="application/rss+xml" title="apatheia.info" href="/atom.xml"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/490828b4c4131e135f17.css" as="style"/><link rel="stylesheet" href="/_next/static/css/490828b4c4131e135f17.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-1a8a258926ecde76681b.js" defer=""></script><script src="/_next/static/chunks/framework-895f067827ebe11ffe45.js" defer=""></script><script src="/_next/static/chunks/main-a9acf05574b3448968f1.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c8f1e010c17d4a8ac8e7.js" defer=""></script><script src="/_next/static/chunks/915-c287d7adb8a31cf4da35.js" defer=""></script><script src="/_next/static/chunks/pages/blog/categories/%5Btag%5D-c38c66d576a52b73b44b.js" defer=""></script><script src="/_next/static/jhtzWSA_9WoQI8qF29M1i/_buildManifest.js" defer=""></script><script src="/_next/static/jhtzWSA_9WoQI8qF29M1i/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header><div id="site-title"><h1><a href="/">apatheia.info</a></h1></div><nav><ul><li><a href="/">Home</a></li><li><a href="/atom.xml">RSS</a></li></ul></nav></header><main><article><h1>Articles tagged &#x27;<!-- -->lxc<!-- -->&#x27;</h1><section style="margin-bottom:1em"><h2><a href="/blog/2013/06/17/docker/">仮想環境構築に docker を使う</a></h2><span>2013.06.17</span> <span><a href="/blog/categories/docker/">docker</a> </span><span><a href="/blog/categories/lxc/">lxc</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2012/05/13/vps-lxc-xtradb-cluster/">さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす</a></h2><span>2012.05.13</span> <span><a href="/blog/categories/mysql/">mysql</a> </span><span><a href="/blog/categories/lxc/">lxc</a> </span><span><a href="/blog/categories/linux/">linux</a> </span></section></article></main><footer><ul><li>Link:</li><li><a href="https://twitter.com/f440">Twitter</a></li><li><a href="https://github.com/f440">Github</a></li><li><a href="https://pinbaord.in/u:f440">Pinbaord</a></li></ul></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":"lxc","posts":[{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-06-17-docker.markdown","path":"blog/2013/06/17/docker","layout":"post","title":"仮想環境構築に docker を使う","createdAt":"2013-06-17T00:13:00.000Z","kind":"article","comments":true,"tags":["docker","lxc"],"content":"\nちょっと前から [Docker][] を使っているので、その話。\n\n\u003c!-- more --\u003e\n\n## Dockr について\n\n[Docker][] は [dotcloud][] がオープンソースで公開している、コンテナ技術による仮想化ソフトウェア。\n\n以下のテクノロジーベースにしている:\n\n- [LXC](http://lxc.sourceforge.net/)\n  - [前にも書いた](/blog/2012/05/13/vps-lxc-xtradb-cluster/)。Xen とか VirtualBOX みたいにホスト内に仮想マシンを立ち上げるんじゃなくて、ホスト内の隔離された環境で仮想マシンを動かす技術。物理マシンをシミュレーションしているんじゃないってことは、VPS とか EC2 とかの仮想マシン上でも問題なく動くし、マシンを起動するプロセスが不要となるので、一瞬で使い始められるというメリットにつながっている。\n- [AUFS](http://aufs.sourceforge.net/)\n  - UnionFS(ディレクトリを重ね合わせることができる)の実装の一つ。元の仮想マシンイメージを書き換えないで、更新が発生した部分は別の場所に書き込んでいくようになっている。これにより、仮想マシンの立ち上げ時にイメージのコピーが発生しないので、すぐに使い始められる。\n\nDocker を使う前は LXC のラッパーとして取っつきにくさを緩和してくれる、とかそういうレベルだと思ったんだけど、予想はよい方向に裏切られた。\n\n[仮想マシンのイメージを可視化したもの](http://docs.docker.io/en/latest/commandline/command/images/)を見ると、まるで Git のコミットログみたいに見えると思う。実際、情報は差分で管理され、履歴を残したり分岐させたりといった操作が非常に軽量にできていて、Git を操作するかのように仮想マシンを操作できるようになっている。\n\n\n## 動かし方\n\nArch Linux や Debian で動かしている人がいるみたいだけど、公式サポートは今のところ Ubuntu のみ。Ubuntu 12.04 LTS を使っているのであれば、`curl get.docker.io | sh -x` で動くようになる。\n\nちゃんとしたやり方は [ドキュメント](http://docs.docker.io/en/latest/installation/)を見れば、特にはまることもないと思う。できるだけ新しい Ubuntu を使っておけばいい。\n\nすぐに試してみたいんなら、Vagrant 経由で簡単に使い始められる。\n\n    git clone https://github.com/dotcloud/docker.git\n    cd docker\n    vagrant up --provider virtualbox # or vagrant up --provider aws\n\n\n## 基礎的な操作方法\n\nインストールがうまくいって Docker が起動しているものとして、早速使ってみる。\n\n    $ docker\n    Usage: docker [OPTIONS] COMMAND [arg...]\n      -H=\"127.0.0.1:4243\": Host:port to bind/connect to\n\n      A self-sufficient runtime for linux containers.\n\n      Commands:\n      attach    Attach to a running container\n      build     Build a container from a Dockerfile\n      commit    Create a new image from a container's changes\n    (以下省略)\n\nコマンドがずらっと表示されるかと思う。まずは単発のコマンドをコンテナ内で実行してみる。\n\n    $ docker run base /bin/echo hi\n    Pulling repository base from https://index.docker.io/v1\n    Pulling image b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc (latest) from base\n    Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc metadata\n    Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc fs layer\n    Downloading 10240/? (n/a)\n    Pulling 27cf784147099545 metadata\n    Pulling 27cf784147099545 fs layer\n    Downloading 94863360/? (n/a)\n    Pulling image 27cf784147099545 () from base\n    hi\n\n「`docker` コマンドに run サブコマンドを指定して、`base` という仮想マシンで `/bin/echo hi` コマンドを実行する」という意味になる。仮想マシンがダウンロードされるが、これは初回実行時のみ。最後に表示された「hi」というのが今回の実行結果で、このコンテナの役割はこれで終わり。\n\n今度は作ったマシンの中に入ってみるために、`-i` と `-t` オプションで入出力できるようにして `/bin/bash` を起動してみる。\n\n    $ docker run -i -t base /bin/bash\n    root@bc43a290f0ce:/#\n\n端末から抜けるとホスト側に制御が戻る。\n\n    root@bc43a290f0ce:/# exit\n    exit\n    $\n\n今度は `-d` オプションでコマンドを実行しっぱなしにする。\n\n    $ docker run -i -t -d base /bin/ping -i 5 www.aikatsu.net\n    79365b2985c4\n    $\n\nID が返されて、すぐに端末が利用可能になる。稼働中のプロセスを確認してみる。\n\n    $ docker ps\n    ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS\n    79365b2985c4        base:latest         /bin/ping -i 5 www.a   22 seconds ago      Up 21 seconds\n\n次に実行中の出力をのぞいてみよう。\n\n    $ docker logs 79365b2985c4\n    PING www.aikatsu.net (60.32.7.37) 56(84) bytes of data.\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=1 ttl=49 time=282 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=3 ttl=49 time=278 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=4 ttl=49 time=283 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=5 ttl=49 time=266 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=6 ttl=49 time=268 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=8 ttl=49 time=264 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=9 ttl=49 time=270 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=10 ttl=49 time=290 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=11 ttl=49 time=284 ms\n\n順調に動き続けているようなので、このジョブにアタッチしてみる。\n\n    $ docker attach 79365b2985c4\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=18 ttl=49 time=239 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=19 ttl=49 time=291 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=20 ttl=49 time=275 ms\n    (出力が続く)\n\nアタッチ中の端末は `Ctrl-p Ctrl-q` でデタッチできる。(このとき use of closed network connection っていうエラーが出る場合 Ctrl-c で抜けるしかないっぽい。バグレポートは上がっているので、じきに直ると思う。)\n\n最後に`kill`でこのプロセスを消してみる。\n\n    $ docker kill 79365b2985c4\n    $ docker ps\n    $\n\n`ps`からプロセスが消えた。基礎的なコンテナの操作の説明は以上。\n\n## 詳細\n\n### コンテナ\n\nこれまでコマンドを実行したり、`kill` されたコンテナはどうなっているのか。実は全部残っている。停止したコンテナを表示するために`-a`をつける。ついでに、情報を省略しないで表示するために`-notrunc` もつける。\n\n    $ docker ps -a -notrunc\n    ID                                                                 IMAGE               COMMAND                          CREATED             STATUS              PORTS\n    79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2   base:latest         /bin/ping -i 5 www.aikatsu.net   14 minutes ago      Exit 0\n    bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503   base:latest         /bin/bash                        16 minutes ago      Exit 0\n    7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971   base:latest         /bin/echo hi                     16 minutes ago      Exit 0\n    8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb   base:latest         /bin/bash                        21 minutes ago      Exit 0\n    4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a   base:latest         /bin/echo hi                     21 minutes ago      Exit 0\n\n正常終了しているので、すべて`Exit 0`になっている。また、ID は省略表記されていたこともわかる。コンテナの実体は `/var/lib/docker/containers/\u003cID\u003e` 以下に格納されている。\n\n    $ sudo ls /var/lib/docker/containers/\n    4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a\n    79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2\n    7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971\n    8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb\n    bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503\n\nどんどんたまっていくから心配かもしれないけど、各コンテナはベースイメージからの差分しかもたないので、問題にならない。もし、消したくなったら `docker rm \u003cコンテナのID\u003e` で消せる。\n\n作業領域であったコンテナを `commit` するとイメージとして使い回せるようになる。`ユーザー名/名称`にするのが作法っぽい。\n\n    $ docker commit -m \"My first container\" 4637bc634170 f440/first_container\n    02036952e5dc\n    $ docker images\n    REPOSITORY             TAG                 ID                  CREATED\n    base                   latest              b750fe79269d        12 weeks ago\n    base                   ubuntu-quantl       b750fe79269d        12 weeks ago\n    base                   ubuntu-quantal      b750fe79269d        12 weeks ago\n    base                   ubuntu-12.10        b750fe79269d        12 weeks ago\n    f440/first_container   latest              02036952e5dc        3 seconds ago\n\nこれで今後は `docker run f440/first_container` をベースにしたコンテナを作れるようになる。\n\n### イメージ\n\nもう一回イメージの一覧を内容を確認してみよう。\n\n    $ docker images\n    REPOSITORY             TAG                 ID                  CREATED\n    f440/first-container   latest              141fef9a2f57        14 seconds ago\n    base                   latest              b750fe79269d        12 weeks ago\n    base                   ubuntu-12.10        b750fe79269d        12 weeks ago\n    base                   ubuntu-quantl       b750fe79269d        12 weeks ago\n    base                   ubuntu-quantal      b750fe79269d        12 weeks ago\n\nbase イメージは latest, ubuntu-quantl, ubuntu-quantal, ubuntu-12.10 といった複数のタグがついていることがわかる。イメージは複数の名称をタグ付けできるようになっており、`base:latest`, `base:ubuntu-12.10` といった形で異なるイメージを呼び出せるようになっている。省略時は `base:latest` と同じ。\n\npull してくるイメージは [https://index.docker.io/](https://index.docker.io/) から情報を持ってくる。コマンドラインで検索したい場合は `search` コマンドを利用する。\n\n    $ docker search centos\n    Found 4 results matching your query (\"centos\")\n    NAME                          DESCRIPTION\n    centos\n    backjlack/centos-6.4-x86_64\n    creack/centos\n    mbkan/lamp                    centos with ssh, LAMP, PHPMyAdmin(root pas...\n\nローカルにキャッシュされたイメージを消すには `docker rmi \u003cイメージのID\u003e`でいい。\n\n自前で作ったイメージを [https://index.docker.io/](https://index.docker.io/)  に登録するには、あらかじめサイト上でアカウントを作っておき、 `docker login` した後に `docker push` する。イメージ名にアンダーバー使っていると `push` で失敗するのと、アップロードしたイメージを消す機能がまだなかったりするので注意。\n\nイメージの実体は `/var/lib/docker/graph/` にある。\n\n    $ docker images -a -notrunc\n    REPOSITORY          TAG                 ID                                                                 CREATED\n    base                latest              b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-12.10        b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-quantl       b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-quantal      b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    \u003cnone\u003e              \u003cnone\u003e              27cf784147099545                                                   12 weeks ago\n\n    $ sudo ls -1 /var/lib/docker/graph\n    141fef9a2f57e86dd6d9aa58fe9318b0d9d71d91053079842051d9738bad6e45\n    27cf784147099545\n    b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\n    checksums\n    :tmp:\n\nここで images に ID: 27cf784147099545 というのが現れた。これは何か。`inspect` を使うとイメージの詳細を表示できる。\n\n    $ docker inspect base\n    {\n        \"id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n        \"parent\": \"27cf784147099545\",\n        \"created\": \"2013-03-23T22:24:18.818426-07:00\",\n        \"container\": \"3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0\",\n        \"container_config\": {\n            \"Hostname\": \"\",\n            \"User\": \"\",\n            \"Memory\": 0,\n            \"MemorySwap\": 0,\n            \"CpuShares\": 0,\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"PortSpecs\": null,\n            \"Tty\": true,\n            \"OpenStdin\": true,\n            \"StdinOnce\": false,\n            \"Env\": null,\n            \"Cmd\": [\n                \"/bin/bash\"\n            ],\n            \"Dns\": null,\n            \"Image\": \"base\",\n            \"Volumes\": null,\n            \"VolumesFrom\": \"\"\n        }\n    }\n\nID: 27cf784147099545 は base イメージの親イメージの ID であることがわかる。イメージは差分になっているので、親のイメージが必要ということで初回実行のタイミングで base と一緒に 27cf784147099545 もダウンロードされていたのだった。\n\n### ネットワーク\n\n`docker run` 時に `-p` をつけることで、コンテナから外部にさらすポートを決められる。コンテナ側のポートはホスト側のポートに変換される際、ポート番号が変更される(49153以降になる)ので、`docker port \u003cジョブのID\u003e \u003cポート番号\u003e` あるいは `docker ps ` でポートの対応状況を確認する必要がある。\n\nドキュメントの [Expose a service on a TCP port](https://github.com/dotcloud/docker#expose-a-service-on-a-tcp-port) がわかりやすい。\n\n    # 以下、コメントは書き換えてある\n    # また、途中経過がわかりやすいように set -x しておく\n    set -x\n\n    # 4444 を晒すよう -p オプションをつけて docker run しつつ、\n    # コンテナは netcat で4444を待ち受ける\n    JOB=$(docker run -d -p 4444 base /bin/nc -l -p 4444)\n    ++ docker run -d -p 4444 base /bin/nc -l -p 4444\n    + JOB=c86c892574f7\n\n    # 4444 がローカルのどのポートに対応するのか確認\n    # docker ps でも調べることはできる\n    PORT=$(docker port $JOB 4444)\n    ++ docker port c86c892574f7 4444\n    + PORT=49166\n\n    # ルーティングによっては localhost とか 127.0.0.1 だと\n    # うまくいかないことがあるので、eth0 のIPアドレスを使おう、\n    # ってことらしい\n    IP=$(ifconfig eth0 | perl -n -e 'if (m/inet addr:([\\d\\.]+)/g) { print $1 }')\n    ++ perl -n -e 'if (m/inet addr:([\\d\\.]+)/g) { print $1 }'\n    ++ ifconfig eth0\n    + IP=10.156.137.111\n    echo hello world | nc $IP $PORT\n    + nc 10.156.137.111 49166\n    + echo hello world\n\n    # コンテナが受信したメッセージを logs で表示\n    echo \"Daemon received: $(docker logs $JOB)\"\n    ++ docker logs c86c892574f7\n    + echo 'Daemon received: hello world'\n    Daemon received: hello world\n\n### Dockerfile\n\nDSLで書かれた設定(通常ファイル名は`Dockerfile`とする)をあらかじめ用意することで、手順に従ってイメージを作ることができる。\n\n    読み込ませ方 (1)\n    docker build \u003cDockerfileのあるディレクトリ\u003e\n    # ex. docker build .\n\n    読み込ませ方 (2)\n    docker build -\n    # ex. docker build - \u003c /foo/bar/Dockerfile\n\nDockerfile の例\n\n    FROM base\n    RUN /bin/echo hi\n\nこれで、`docker build` すれば `docker run base /bin/echo hi` と同じ効果が得られる。\n\n指定できるはコマンドは以下の通り。大文字小文字は区別しないけど、引数と見分けやすいように大文字が使われる。\n\n- `FROM \u003cimage\u003e` ベースとなるイメージを指定\n- `MAINTAINER \u003cname\u003e` メンテナの名前を指定\n- `RUN \u003ccommand\u003e` ビルド中に実行したいコマンドを指定\n- `CMD \u003ccommand\u003e` 起動後のコンテナで実行したいコマンドを指定\n- `EXPOSE \u003cport\u003e [\u003cport\u003e ...]` 外部に晒すポートの指定\n- `ENV \u003ckey\u003e \u003cvalue\u003e` 環境変数の設定\n- `INSERT \u003cfile url\u003e \u003cpath\u003e` deprecated なので ADD を利用すること\n- `ADD \u003csrc\u003e \u003cdest\u003e` ファイルを配置\n\n`RUN` と `CMD` の違いがわかりにくいかもしれない。例を出す。\n\n    # RUN, CMD で指定したコマンドが実行されたとき、\n    # 標準出力と /tmp/*.log に記録を残す\n\n    $ cat \u003c\u003cSCRIPT \u003eDockerfile\n    \u003e FROM base\n    \u003e RUN /bin/echo run | tee /tmp/run.log\n    \u003e CMD /bin/echo cmd | tee /tmp/cmd.log\n    \u003e SCRIPT\n\n    # ビルドの実行\n\n    $ docker build .\n    Caching Context 10240/? (n/a)\n    FROM base ()\n    ===\u003e b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\n    RUN /bin/echo run | tee /tmp/run.log (b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc)\n    ===\u003e d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\n    CMD /bin/echo cmd | tee /tmp/cmd.log (d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d)\n    ===\u003e 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\n    Build successful.\n    ===\u003e 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\n\n    # run, cmd の実行結果を確認\n    # =\u003e run だけが実行されている\n\n    $ docker run 60671e99691 /bin/ls /tmp/\n    run.log\n\n    # イメージを inspect する\n    # =\u003e どうやらコンテナは記憶していることがわかる\n\n    $ docker inspect 60671e99691\n    {\n        \"id\": \"60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\",\n        \"parent\": \"d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\",\n        \"created\": \"2013-06-16T16:29:14.602237Z\",\n        \"container\": \"4c54683cec90500f329dfaad2e0856cc408483be0ae3166018121d4d4b9b3282\",\n        \"container_config\": {\n            \"Hostname\": \"78c72f8ba6ad\",\n            \"User\": \"\",\n            \"Memory\": 0,\n            \"MemorySwap\": 0,\n            \"CpuShares\": 0,\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"PortSpecs\": null,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": null,\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) CMD [/bin/sh -c /bin/echo cmd | tee /tmp/cmd.log]\"\n            ],\n            \"Dns\": null,\n            \"Image\": \"d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\",\n            \"Volumes\": null,\n\n    # 引数でコマンドを指定せずに run を実行\n    # =\u003e cmd で登録した内容が実行される\n\n    $ docker run 60671e99691\n    cmd\n\nつまり、`RUN` は `Dockerfile` を元にビルドしているときに参照され、`CMD` はコンテナを実行する際に参照されるということがわかる。パッケージをインストールしたりといった用途では通常 `RUN` を使う。\n\n## まとめ\n\n仮想環境の発達でプログラマブルなインフラストラクチャーは実現できてきているけど、マシンを上げたり下げたりするのにどうしても時間がかかるし、それは仕方が無いものと我慢していた。`Docker` を使ってみると、今までのそういった不満から解放されることができそう。一応開発中というステータスなのでプロダクション環境では使いづらいけど、開発やテスト、とくに構成管理ツールを設定するときなどは、この俊敏性、柔軟性は有効になると思う。\n\n## 参考\n\n- [Documentation](http://docs.docker.io/en/latest/)\n\n\n[dotCloud]: https://www.dotcloud.com/\n[Docker]: http://www.docker.io/\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-05-13-vps-lxc-xtradb-cluster.markdown","path":"blog/2012/05/13/vps-lxc-xtradb-cluster","layout":"post","title":"さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす","createdAt":"2012-05-12T15:00:00.000Z","kind":"article","comments":true,"tags":["mysql","lxc","linux"],"content":"ほんの数年前までVPSといえばLinode、Slicehostなどの海外のサーバーしか選択肢がなかった。ls を実行しても一呼吸おくほどのレイテンシーがあるような環境で、128MBくらいのメモリを何とかやりくりしてRailsを動かしていたが、現在では月1000円程度で用途によっては手に余るようなスペックが手に入るようになっている。そんなVPSの余ったリソースを使って、仮想環境をたてようというのが今回の目的だ。\n\n\u003c!-- more --\u003e\n\n[LXC](http://lxc.sourceforge.net/)は、他の仮想化方式と比べてオーバーヘッドが少なくきびきび動くし、必要であればCPUやメモリの制限をつけることもできる。RPMやDebのパッケージ作成をしたり、Chefのcookbook作成などで独立した環境を手軽に構築したい人には特に有用に思う。また、簡単にネットワークが作れるので、複数台構成のソフトウェアを1台のマシンのなかで動かすことが出来る。今回は動作確認として [PerconaXtraDB Cluster](http://www.percona.com/software/percona-xtradb-cluster/)を動かしてみることにする。\n\n## 前提について\n\n作業環境は以下を想定している。\n\n  * さくらのVPS(v3) 1G \n    * CentOS 6.2 x86_64\n  * LXC 0.7.5\n\nCentOSを使っているのはデフォルトのOSイメージだからというのが理由。\n\n今回の内容をEC2上で実行する場合、Amazon Linux でもほとんど同様の設定で作業を行えることを確認しているけど、もっ と新しいOS、たとえば Ubuntu 12.04 LTS を使えば後述する cgroupの設定、bridgeの設定が不要となるためより簡単に行える。CentOS 6で実施したときだけ遭遇するような問題に何度もぶつかったので、出来るだけ新しいOSを使った方がいい。\n\n仮想環境としては、lxcに同梱されているテンプレートを利用してUbuntuを、またOSイメージの作成からCentOSを構築する。\n\n## 構築方法\n\n以降の作業はすべて root で行うものとする。\n\n### ネットワークの設定\n\n仮想環境とのやりとりで使うブリッジを作る。\n\n    \n    # yum install bridge-utils\n    # vim /etc/sysconfig/network-scripts/ifcfg-lxcbr0\n    \n        DEVICE=lxcbr0\n        TYPE=Bridge\n        BOOTPROTO=none\n        IPADDR=10.0.3.1\n        NETMASK=255.255.255.0\n        ONBOOT=yes\n    \n    # ifup lxcbr0 # 起動\n    \n\n### cgroup\n\n    \n    # mount | grep cgroup # cgroup がないこと確認\n    # mkdir -p /cgroup\n    # printf \"none\t\t\t/cgroup\t\tcgroup\tdefaults\t\t0 0\n    \" \u003e\u003e /etc/fstab\n    # mount -a\n    # mount | grep cgroup # cgroup があること確認\n    \n\n### lxc セットアップ\n\n    \n    # yum install libcap-devel docbook-utils\n    # yum groupinstall \"Development Tools\"\n    \n    # wget [http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz](http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz)\n    # tar xf lxc-0.7.5.tar.gz\n    # cd lxc-0.7.5\n    # ./configure\n    # make rpm # この途中で /usr/lib64/lxc/{template,rootfs} がインストールされるのかなり狂ってる\n    # rpm -ivh ~/rpmbuild/RPMS/x86_64/lxc-0.7.5-1.x86_64.rpm\n       (~/rpmbuild になければ、/usr/src/rpm から探す)\n    # mkdir -p /var/lib/lxc\n    \n\n### dnsmasq (DHCP, DNS サーバー) セットアップ\n\n環境を増やすごとに毎回NICの設定を編集するのは手間なので、ホスト側で dncp, dns の設定をする。\n\n    \n    # yum install dnsmasq\n    # vim /etc/dnsmasq.conf\n    \n        コメントを外して有効化する、編集するなどで以下の設定を行う\n        domain は自分の使いたい名前にすればいい\n    \n        domain-needed\n        bogus-priv\n        interface = lxcbr0\n        listen-address = 127.0.0.1\n        listen-address = 10.0.3.1\n        expand-hosts\n        domain = lxc\n        dhcp-range = 10.0.3.50,10.0.3.200,1h\n    \n    # service dnsmasq reload\n    \n\n### ネットワークセットアップ\n\n仮想環境から外部へのやりとりが出来るようにネットワークの設定を変更する。\n\n    \n    # sysctl -w net.ipv4.ip_forward=1\n    # sed -i -re s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/ /etc/sysctl.conf\n    # iptables -A POSTROUTING -s 10.0.3.0/24 -t nat -j MASQUERADE\n    # service iptables save # 設定を /etc/sysconfig/iptables に保存\n    \n\n### 仮想環境構築 (1) 同梱のスクリプトを使った Ubuntu のインストール\n\nlxcに同梱のスクリプト /usr/lib64/lxc/templates/lxc-ubuntu を使ってUbuntuをインストールする。\n\n基本的な設定ファイルを作る。\n\n    \n    # cd\n    # vim lxc.conf\n    \n        lxc.network.type=veth\n        lxc.network.link=lxcbr0\n        lxc.network.flags=up\n    \n\n今回は Ubuntu を導入するので、そのために必要なプログラムをインストールする。\n\n    \n    # yum install --enablerepo=epel debootstrap dpkg\n    \n\nこれで準備が出来たので、実際に仮想環境を動かしてみる。\n\n    \n    # lxc-create -t ubuntu -f lxc.conf -n vm0\n       -t がテンプレートの名前。 -t ubuntu なら /usr/lib64/lxc/templates/lxc-ubuntu が読み込まれる\n          オプションでバージョンが指定可能だが、lxc 0.7.5 に同梱されているテンプレートのデフォルトだと Ubuntu 10.04 が選ばれる。\n       -f がさっき作った設定ファイルの場所\n       -n が環境の名前。今回は vm0 とした。 /var/lib/lxc/vm0 にファイルがおかれる\n    # lxc-start -n vm0 -l debug -o debug.out -d\n       -l はデバッグレベル、-o はデバッグの場所を指定。安定して起動するようになったらつけなくていい\n    # lxc-console -n vm0\n      一回エンター押した後、ユーザー root パスワード root でログイン\n      抜けるときは Ctrl-a q\n    \n      lxc-console をしても何も表示されない状態になったら、以下を施して再起動\n    \n    # vim /var/lib/lxc/vm0/rootfs/etc/init/lxc.conf\n    \n      telinit を差し込む\n    \n        --- /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf.orig 2012-02-07 10:28:25.000000000 +0900\n        +++ /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf      2012-05-06 22:43:21.606098530 +0900\n        @@ -12,5 +12,6 @@\n            touch /var/run/utmp\n            chown root:utmp /var/run/utmp\n            initctl emit --no-wait net-device-added INTERFACE=lo || true\n        +   telinit 3\n            exit 0\n         end script\n    \n\nlxc-console だとCtrl-aが使えなくて不便なので、今後はsshでログインしたい。テンプレートが自動的にOpenSSHをインストールしてくれるが、ちゃんと起動しない。仕方が無いので、update-rc.d で起動するように設定\n\n    \n      仮想環境内で実行\n    # update-rc.d ssh enable\n    \n\n固定IPアドレスを振りたい場合は、設定を変更する。\n\n    \n      ホスト側からの変更\n    # vim /var/lib/lxc/vm0/config\n    \n      lxc.network.ipv4 = 10.0.3.2/24\n    \n      仮想環境の中で変更\n    # vim /etc/network/interfaces\n    \n        変更前\n        auto lo\n        iface lo inet loopback\n    \n        auto eth0\n        iface eth0 inet dhcp\n    \n        変更後\n        auto lo\n        iface lo inet loopback\n    \n        iface eth0 inet static\n            address 10.0.3.2\n            netmask 255.255.255.0\n            gateway 10.0.3.1\n    \n\n仮想環境の破棄は lxc-destroy で行う\n\n    \n    # lxc-destroy -n vm0\n    \n\n### 仮想環境構築 (2) 独自に構築した CentOS 6 のインストール\n\nlxc-console の標準テンプレートでは CentOS が用意されていないので、自力でセットアップする。\n\n#### イメージ作成\n\n基本的に [Centos6/Installation/Minimal installation using yum](http://wiki.1tux.org/wiki/Centos6/Installation/Minimal_installation_using_yum) の通り。ただし 64 bit 版をインストールする\n\n    \n    # mkdir /t\n    # cd /t\n    # wget [http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm](http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm)\n    # rpm2cpio centos-release-6-2.el6.centos.7.x86_64.rpm  | cpio -idm\n    # sed -i s/$releasever/6/g ./etc/yum.repos.d/*\n    # yum --installroot=/t groupinstall base\n    # yum --installroot=/t install dhclient\n    # rm centos-release*.rpm\n    # chroot /t\n    \n      // ここから後はchroot内\n    \n    # passwd # パスワード変更\n    \n    # rm -f /dev/null\n    # mknod -m 666 /dev/null c 1 3\n    # mknod -m 666 /dev/zero c 1 5\n    # mknod -m 666 /dev/urandom c 1 9\n    # ln -s /dev/urandom /dev/random\n    # mknod -m 600 /dev/console c 5 1\n    # mknod -m 660 /dev/tty1 c 4 1\n    # chown root:tty /dev/tty1\n    \n    # mkdir -p /dev/shm\n    # chmod 1777 /dev/shm\n    # mkdir -p /dev/pts\n    # chmod 755 /dev/pts\n    \n    # cp -a /etc/skel/. /root/.\n    \n    # cat \u003e /etc/resolv.conf \u003c\u003c END\n    # Google public DNS\n    nameserver 8.8.8.8\n    nameserver 8.8.4.4\n    END\n    \n    # cat \u003e /etc/hosts \u003c\u003c END\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n    END\n    \n    # cat \u003e /etc/sysconfig/network \u003c\u003c END\n    NETWORKING=yes\n    HOSTNAME=localhost\n    END\n    \n    # cat \u003e /etc/sysconfig/network-scripts/ifcfg-eth0  \u003c\u003c END\n    DEVICE=eth0\n    ONBOOT=yes\n    BOOTPROTO=dhcp\n    END\n    \n    # cat \u003e /etc/fstab \u003c\u003c END\n    /dev/root               /                       rootfs   defaults        0 0\n    none                    /dev/shm                tmpfs    nosuid,nodev    0 0\n    END\n    \n    # cat \u003e /etc/init/lxc-sysinit.conf \u003c\u003c END\n    start on startup\n    env container\n    \n    pre-start script\n            if [ \"x$container\" != \"xlxc\" -a \"x$container\" != \"xlibvirt\" ]; then\n                    stop;\n            fi\n            telinit 3\n            initctl start tty TTY=console\n            exit 0;\n    end script\n    END\n    \n    # exit\n    \n    // ここから後はchroot外\n    \n    # cd /t\n    # tar cvfz /centos6-lxc-root.tgz .\n    \n\n#### 設定\n\n    \n    # mkdir /var/lib/lxc/vm0\n    # cd /var/lib/lxc/vm0\n    # mkdir rootfs\n    # cd rootfs\n    # tar xfz /centos6-lxc-root.tgz --numeric-owner\n    # cd /var/lib/lxc/vm0\n    \n    # cat \u003e/var/lib/lxc/vm0/config \u003c\u003c END\n    lxc.network.type=veth\n    lxc.network.link=lxcbr0\n    lxc.network.flags=up\n    lxc.network.veth.pair=veth-vm0\n    lxc.utsname = vm0\n    \n    lxc.tty = 1\n    lxc.pts = 1024\n    lxc.rootfs = /var/lib/lxc/vm0/rootfs\n    lxc.mount  = /var/lib/lxc/vm0/fstab\n    lxc.arch = x86_64\n    lxc.cap.drop = sys_module mac_admin\n    \n    lxc.cgroup.devices.deny = a\n    # Allow any mknod (but not using the node)\n    lxc.cgroup.devices.allow = c *:* m\n    lxc.cgroup.devices.allow = b *:* m\n    # /dev/null and zero\n    lxc.cgroup.devices.allow = c 1:3 rwm\n    lxc.cgroup.devices.allow = c 1:5 rwm\n    # consoles\n    lxc.cgroup.devices.allow = c 5:1 rwm\n    lxc.cgroup.devices.allow = c 5:0 rwm\n    # /dev/{,u}random\n    lxc.cgroup.devices.allow = c 1:9 rwm\n    lxc.cgroup.devices.allow = c 1:8 rwm\n    lxc.cgroup.devices.allow = c 136:* rwm\n    lxc.cgroup.devices.allow = c 5:2 rwm\n    # rtc\n    lxc.cgroup.devices.allow = c 254:0 rwm\n    #fuse\n    lxc.cgroup.devices.allow = c 10:229 rwm\n    #tun\n    lxc.cgroup.devices.allow = c 10:200 rwm\n    #full\n    lxc.cgroup.devices.allow = c 1:7 rwm\n    #hpet\n    lxc.cgroup.devices.allow = c 10:228 rwm\n    #kvm\n    lxc.cgroup.devices.allow = c 10:232 rwm\n    END\n    \n    # cat \u003e fstab  \u003c\u003c END\n    proc            /var/lib/lxc/vm0/rootfs/proc         proc    nodev,noexec,nosuid 0 0\n    sysfs           /var/lib/lxc/vm0/rootfs/sys          sysfs defaults  0 0\n    END\n    \n\n#### 起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.out -d\n    # lxc-console -n vm0\n    \n    OpenSSH がなければ入れておく\n    # yum install openssh-server\n    # service sshd start\n    \n\n## 動作確認 (Percona XtraDB Cluster の稼働確認)\n\n動作確認として Percona XtraDB Cluster を動かしてみる。\n\nすでにこれまでの作業を通して vm0 としてCentOS 6がインストール済みとする。\n\n### ホスト側設定\n\n  * 構成 \n    * ホスト, IPアドレス 10.0.3.1\n    * 仮想0 vm0, IPアドレス 10.0.3.2\n    * 仮想1 vm1, IPアドレス 10.0.3.3\n    * 仮想2 vm2, IPアドレス 10.0.3.4\n\n各仮想環境に簡単にアクセスできるように hosts を設定しておく。ホスト側に設定しておけば、dnsmasq のおかげで仮想側でも名前が引けるようになる。\n\n    \n    # vim /etc/hosts\n        以下を追記\n        10.0.3.2 vm0\n        10.0.3.3 vm1\n        10.0.3.4 vm2\n    \n\n### コピー元(vm0) 設定\n\n    \n    # ssh vm0\n      ここからはvm0の中\n    \n      固定IPアドレスを設定\n    # vim /var/lib/lxc/vm1/rootfs/etc/sysconfig/network-scripts/ifcfg-eth0\n        DEVICE=eth0\n        ONBOOT=yes\n        BOOTPROTO=static\n        IPADDR=10.0.3.3\n        NETMASK=255.255.255.0\n        GATEWAY=10.0.3.1\n    \n      XtraDB Cluster インストール\n    # rpm -Uhv [http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm](http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm)\n    # rpm -Uhv [http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm](http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm)\n    # yum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client\n    # cat \u003e /etc/my.cnf \u003c\u003cEND\n    [mysqld]\n    binlog_format=ROW\n    wsrep_provider=/usr/lib64/libgalera_smm.so\n    wsrep_cluster_address=gcomm://\n    wsrep_slave_threads=2\n    wsrep_cluster_name=lxccluster\n    wsrep_sst_method=rsync\n    wsrep_node_name=node0\n    innodb_locks_unsafe_for_binlog=1\n    innodb_autoinc_lock_mode=2\n    END\n    \n    # poweroff\n    \n\n### コピー、起動\n\n    \n    # lxc-clone -n vm1 -o vm0\n      -n はこれから作る仮想環境の名前\n      -o はコピー元の仮想環境の名前\n    # lxc-clone -n vm1 -o vm0\n    # vim /var/lib/lxc/vm1/config\n      vm0をvm1に置換 (vm2ではvm2に置換)\n      IPアドレスを10.0.3.2 -\u003e 10.0.3.3 に変更 (vm2では 10.0.3.4に変更)\n    # vim /var/lib/lxc/vm1/rootfs/etc/my.cnf\n        wsrep_cluster_address=gcomm:// をwsrep_cluster_address=gcomm://10.0.3.2 に変更\n        wsrep_node_name=node0 を wsrep_node_name=node1 に変更 (vm2ではnode2に変更)\n    \n      同様にvm0からvm2のコピーを実施\n    \n\n3つの環境が完成したら起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.0.out -d\n    # lxc-start -n vm1 -l debug -o debug.1.out -d\n    # lxc-start -n vm2 -l debug -o debug.2.out -d\n    \n\n### 動作確認\n\nvm0 にログインして実行\n\n    \n    # mysql -u root\n      データベース、テーブル作成\n    mysql\u003e create database t;\n    mysql\u003e use t;\n    mysql\u003e create table sample (\n    id int not null primary key auto_increment,\n    value int\n    );\n    \n    データ投入\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    \n\nIDがスキップしながらインサートされることがわかる。引き続き、他の環境でもデータを入れてみる。\n\nvm1 にログインして実行\n\n    \n    mysql\u003e use t;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    |  9 |     1 |\n    | 12 |     1 |\n    | 15 |     1 |\n    +----+-------+\n    \n\n同様のことがvm2でも起きる。\n\nこれにより、XtraDB Cluster の以下の動作が確認出来た。\n\n  * すべてのサーバーで書き込みと参照がおこなえること\n  * オートインクリメントがバッティングしないように、値が自動的にオフセットをつけて挿入されること\n\n## メモ\n\n### 外部から仮想環境へ直接アクセスしたい場合\n\nたとえば、外部からポート10080でアクセスされたとき、仮想環境の 10.0.3.51 のポート 80 へ転送させたい場合は iptables\nで以下のような設定をする。\n\n    \n    # vim /etc/syscofig/iptables\n        -A POSTROUTING -s 10.0.3.0/24 -j MASQUERADE の下に以下を追加\n        -A PREROUTING -i eth0 -p tcp --dport 10080 -j DNAT --to-destination 10.0.3.51:80\n    # service iptables condrestart\n    # iptables -L -t nat # NATテーブルから設定追加を確認\n    \n\n### 新しい Ubuntu を入れたい場合\n\n元の手順だとlucid (10.04) がインストールされるが、たとえば oneiric (11.10) であれば以下でインストール可能。\n\n    \n    # cp -a /usr/share/debootstrap/scripts/lucid  /usr/share/debootstrap/scripts/oneiric\n        lucid は /usr/share/debootstrap/scripts/gutsy のシンボリックリンクで、他のリリースも同様。とにかくファ イル名が参照できるようにシンボリックリンクをコピーしておけばいい。\n    # lxc-create -t ubuntu -f lxc.conf -n vm0 -- --trim -r oneiric\n        lxc-create ではなく -r はテンプレートへの引数\n    \n\n### 他の OS もインストールしてみたい場合\n\n/usr/lib64/lxc/templates/ には lxc-busybox,lxc-debian,lxc-fedora,lxc-lenny,lxc-\nopensuse,lxc-sshd,lxc-ubuntu の テンプレートがある。これ以外の環境が必要であれば、「lxc guset\nOS名」とかで検索してみる。\n\n## 参考\n\n  * [http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc](http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc)\n  * [http://wiki.debian.org/LXC](http://wiki.debian.org/LXC)\n  * [https://help.ubuntu.com/12.04/serverguide/lxc.html](https://help.ubuntu.com/12.04/serverguide/lxc.html)\n  * [http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104](http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104)\n  * [http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6](http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6)\n  * [http://www.percona.com/doc/percona-xtradb-cluster/index.html](http://www.percona.com/doc/percona-xtradb-cluster/index.html)\n\n"}]},"__N_SSG":true},"page":"/blog/categories/[tag]","query":{"tag":"lxc"},"buildId":"jhtzWSA_9WoQI8qF29M1i","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>