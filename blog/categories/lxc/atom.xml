<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lxc | apatheia.info]]></title>
  <link href="http://apatheia.info/blog/categories/lxc/atom.xml" rel="self"/>
  <link href="http://apatheia.info/"/>
  <updated>2013-06-22T23:31:03+09:00</updated>
  <id>http://apatheia.info/</id>
  <author>
    <name><![CDATA[f440]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[仮想環境構築に docker を使う]]></title>
    <link href="http://apatheia.info/blog/2013/06/17/docker/"/>
    <updated>2013-06-17T09:13:00+09:00</updated>
    <id>http://apatheia.info/blog/2013/06/17/docker</id>
    <content type="html"><![CDATA[<p>ちょっと前から <a href="http://www.docker.io/">Docker</a> を使っているので、その話。</p>

<!-- more -->


<h1>Dockr について</h1>

<p><a href="http://www.docker.io/">Docker</a> は <a href="https://www.dotcloud.com/">dotcloud</a> がオープンソースで公開している、コンテナ技術による仮想化ソフトウェア。</p>

<p>以下のテクノロジーベースにしている:</p>

<ul>
<li><a href="http://lxc.sourceforge.net/">LXC</a>

<ul>
<li><a href="/blog/2012/05/13/vps-lxc-xtradb-cluster/">前にも書いた</a>。Xen とか VirtualBOX みたいにホスト内に仮想マシンを立ち上げるんじゃなくて、ホスト内の隔離された環境で仮想マシンを動かす技術。物理マシンをシミュレーションしているんじゃないってことは、VPS とか EC2 とかの仮想マシン上でも問題なく動くし、マシンを起動するプロセスが不要となるので、一瞬で使い始められるというメリットにつながっている。</li>
</ul>
</li>
<li><a href="http://aufs.sourceforge.net/">AUFS</a>

<ul>
<li>UnionFS(ディレクトリを重ね合わせることができる)の実装の一つ。元の仮想マシンイメージを書き換えないで、更新が発生した部分は別の場所に書き込んでいくようになっている。これにより、仮想マシンの立ち上げ時にイメージのコピーが発生しないので、すぐに使い始められる。</li>
</ul>
</li>
</ul>


<p>Docker を使う前は LXC のラッパーとして取っつきにくさを緩和してくれる、とかそういうレベルだと思ったんだけど、予想はよい方向に裏切られた。</p>

<p><a href="http://docs.docker.io/en/latest/commandline/command/images/">仮想マシンのイメージを可視化したもの</a>を見ると、まるで Git のコミットログみたいに見えると思う。実際、情報は差分で管理され、履歴を残したり分岐させたりといった操作が非常に軽量にできていて、Git を操作するかのように仮想マシンを操作できるようになっている。</p>

<h1>動かし方</h1>

<p>Arch Linux や Debian で動かしている人がいるみたいだけど、公式サポートは今のところ Ubuntu のみ。Ubuntu 12.04 LTS を使っているのであれば、<code>curl get.docker.io | sh -x</code> で動くようになる。</p>

<p>ちゃんとしたやり方は <a href="http://docs.docker.io/en/latest/installation/">ドキュメント</a>を見れば、特にはまることもないと思う。できるだけ新しい Ubuntu を使っておけばいい。</p>

<p>すぐに試してみたいんなら、Vagrant 経由で簡単に使い始められる。</p>

<pre><code>git clone https://github.com/dotcloud/docker.git
cd docker
vagrant up --provider virtualbox # or vagrant up --provider aws
</code></pre>

<h1>基礎的な操作方法</h1>

<p>インストールがうまくいって Docker が起動しているものとして、早速使ってみる。</p>

<pre><code>$ docker
Usage: docker [OPTIONS] COMMAND [arg...]
  -H="127.0.0.1:4243": Host:port to bind/connect to

  A self-sufficient runtime for linux containers.

  Commands:
  attach    Attach to a running container
  build     Build a container from a Dockerfile
  commit    Create a new image from a container's changes
(以下省略)
</code></pre>

<p>コマンドがずらっと表示されるかと思う。まずは単発のコマンドをコンテナ内で実行してみる。</p>

<pre><code>$ docker run base /bin/echo hi
Pulling repository base from https://index.docker.io/v1
Pulling image b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc (latest) from base
Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc metadata
Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc fs layer
Downloading 10240/? (n/a)
Pulling 27cf784147099545 metadata
Pulling 27cf784147099545 fs layer
Downloading 94863360/? (n/a)
Pulling image 27cf784147099545 () from base
hi
</code></pre>

<p>「<code>docker</code> コマンドに run サブコマンドを指定して、<code>base</code> という仮想マシンで <code>/bin/echo hi</code> コマンドを実行する」という意味になる。仮想マシンがダウンロードされるが、これは初回実行時のみ。最後に表示された「hi」というのが今回の実行結果で、このコンテナの役割はこれで終わり。</p>

<p>今度は作ったマシンの中に入ってみるために、<code>-i</code> と <code>-t</code> オプションで入出力できるようにして <code>/bin/bash</code> を起動してみる。</p>

<pre><code>$ docker run -i -t base /bin/bash
root@bc43a290f0ce:/#
</code></pre>

<p>端末から抜けるとホスト側に制御が戻る。</p>

<pre><code>root@bc43a290f0ce:/# exit
exit
$
</code></pre>

<p>今度は <code>-d</code> オプションでコマンドを実行しっぱなしにする。</p>

<pre><code>$ docker run -i -t -d base /bin/ping -i 5 www.aikatsu.net
79365b2985c4
$
</code></pre>

<p>ID が返されて、すぐに端末が利用可能になる。稼働中のプロセスを確認してみる。</p>

<pre><code>$ docker ps
ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS
79365b2985c4        base:latest         /bin/ping -i 5 www.a   22 seconds ago      Up 21 seconds
</code></pre>

<p>次に実行中の出力をのぞいてみよう。</p>

<pre><code>$ docker logs 79365b2985c4
PING www.aikatsu.net (60.32.7.37) 56(84) bytes of data.
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=1 ttl=49 time=282 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=3 ttl=49 time=278 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=4 ttl=49 time=283 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=5 ttl=49 time=266 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=6 ttl=49 time=268 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=8 ttl=49 time=264 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=9 ttl=49 time=270 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=10 ttl=49 time=290 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=11 ttl=49 time=284 ms
</code></pre>

<p>順調に動き続けているようなので、このジョブにアタッチしてみる。</p>

<pre><code>$ docker attach 79365b2985c4
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=18 ttl=49 time=239 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=19 ttl=49 time=291 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=20 ttl=49 time=275 ms
(出力が続く)
</code></pre>

<p>アタッチ中の端末は <code>Ctrl-p Ctrl-q</code> でデタッチできる。(このとき use of closed network connection っていうエラーが出る場合 Ctrl-c で抜けるしかないっぽい。バグレポートは上がっているので、じきに直ると思う。)</p>

<p>最後に<code>kill</code>でこのプロセスを消してみる。</p>

<pre><code>$ docker kill 79365b2985c4
$ docker ps
$
</code></pre>

<p><code>ps</code>からプロセスが消えた。基礎的なコンテナの操作の説明は以上。</p>

<h1>詳細</h1>

<h3>コンテナ</h3>

<p>これまでコマンドを実行したり、<code>kill</code> されたコンテナはどうなっているのか。実は全部残っている。停止したコンテナを表示するために<code>-a</code>をつける。ついでに、情報を省略しないで表示するために<code>-notrunc</code> もつける。</p>

<pre><code>$ docker ps -a -notrunc
ID                                                                 IMAGE               COMMAND                          CREATED             STATUS              PORTS
79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2   base:latest         /bin/ping -i 5 www.aikatsu.net   14 minutes ago      Exit 0
bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503   base:latest         /bin/bash                        16 minutes ago      Exit 0
7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971   base:latest         /bin/echo hi                     16 minutes ago      Exit 0
8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb   base:latest         /bin/bash                        21 minutes ago      Exit 0
4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a   base:latest         /bin/echo hi                     21 minutes ago      Exit 0
</code></pre>

<p>正常終了しているので、すべて<code>Exit 0</code>になっている。また、ID は省略表記されていたこともわかる。コンテナの実体は <code>/var/lib/docker/containers/&lt;ID&gt;</code> 以下に格納されている。</p>

<pre><code>$ sudo ls /var/lib/docker/containers/
4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a
79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2
7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971
8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb
bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503
</code></pre>

<p>どんどんたまっていくから心配かもしれないけど、各コンテナはベースイメージからの差分しかもたないので、問題にならない。もし、消したくなったら <code>docker rm &lt;コンテナのID&gt;</code> で消せる。</p>

<p>作業領域であったコンテナを <code>commit</code> するとイメージとして使い回せるようになる。<code>ユーザー名/名称</code>にするのが作法っぽい。</p>

<pre><code>$ docker commit -m "My first container" 4637bc634170 f440/first_container
02036952e5dc
$ docker images
REPOSITORY             TAG                 ID                  CREATED
base                   latest              b750fe79269d        12 weeks ago
base                   ubuntu-quantl       b750fe79269d        12 weeks ago
base                   ubuntu-quantal      b750fe79269d        12 weeks ago
base                   ubuntu-12.10        b750fe79269d        12 weeks ago
f440/first_container   latest              02036952e5dc        3 seconds ago
</code></pre>

<p>これで今後は <code>docker run f440/first_container</code> をベースにしたコンテナを作れるようになる。</p>

<h3>イメージ</h3>

<p>もう一回イメージの一覧を内容を確認してみよう。</p>

<pre><code>$ docker images
REPOSITORY             TAG                 ID                  CREATED
f440/first-container   latest              141fef9a2f57        14 seconds ago
base                   latest              b750fe79269d        12 weeks ago
base                   ubuntu-12.10        b750fe79269d        12 weeks ago
base                   ubuntu-quantl       b750fe79269d        12 weeks ago
base                   ubuntu-quantal      b750fe79269d        12 weeks ago
</code></pre>

<p>base イメージは latest, ubuntu-quantl, ubuntu-quantal, ubuntu-12.10 といった複数のタグがついていることがわかる。イメージは複数の名称をタグ付けできるようになっており、<code>base:latest</code>, <code>base:ubuntu-12.10</code> といった形で異なるイメージを呼び出せるようになっている。省略時は <code>base:latest</code> と同じ。</p>

<p>pull してくるイメージは <a href="https://index.docker.io/">https://index.docker.io/</a> から情報を持ってくる。コマンドラインで検索したい場合は <code>search</code> コマンドを利用する。</p>

<pre><code>$ docker search centos
Found 4 results matching your query ("centos")
NAME                          DESCRIPTION
centos
backjlack/centos-6.4-x86_64
creack/centos
mbkan/lamp                    centos with ssh, LAMP, PHPMyAdmin(root pas...
</code></pre>

<p>ローカルにキャッシュされたイメージを消すには <code>docker rmi &lt;イメージのID&gt;</code>でいい。</p>

<p>自前で作ったイメージを <a href="https://index.docker.io/">https://index.docker.io/</a>  に登録するには、あらかじめサイト上でアカウントを作っておき、 <code>docker login</code> した後に <code>docker push</code> する。イメージ名にアンダーバー使っていると <code>push</code> で失敗するのと、アップロードしたイメージを消す機能がまだなかったりするので注意。</p>

<p>イメージの実体は <code>/var/lib/docker/graph/</code> にある。</p>

<pre><code>$ docker images -a -notrunc
REPOSITORY          TAG                 ID                                                                 CREATED
base                latest              b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-12.10        b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-quantl       b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-quantal      b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
&lt;none&gt;              &lt;none&gt;              27cf784147099545                                                   12 weeks ago

$ sudo ls -1 /var/lib/docker/graph
141fef9a2f57e86dd6d9aa58fe9318b0d9d71d91053079842051d9738bad6e45
27cf784147099545
b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc
checksums
:tmp:
</code></pre>

<p>ここで images に ID: 27cf784147099545 というのが現れた。これは何か。<code>inspect</code> を使うとイメージの詳細を表示できる。</p>

<pre><code>$ docker inspect base
{
    "id": "b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc",
    "parent": "27cf784147099545",
    "created": "2013-03-23T22:24:18.818426-07:00",
    "container": "3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0",
    "container_config": {
        "Hostname": "",
        "User": "",
        "Memory": 0,
        "MemorySwap": 0,
        "CpuShares": 0,
        "AttachStdin": false,
        "AttachStdout": false,
        "AttachStderr": false,
        "PortSpecs": null,
        "Tty": true,
        "OpenStdin": true,
        "StdinOnce": false,
        "Env": null,
        "Cmd": [
            "/bin/bash"
        ],
        "Dns": null,
        "Image": "base",
        "Volumes": null,
        "VolumesFrom": ""
    }
}
</code></pre>

<p>ID: 27cf784147099545 は base イメージの親イメージの ID であることがわかる。イメージは差分になっているので、親のイメージが必要ということで初回実行のタイミングで base と一緒に 27cf784147099545 もダウンロードされていたのだった。</p>

<h3>ネットワーク</h3>

<p><code>docker run</code> 時に <code>-p</code> をつけることで、コンテナから外部にさらすポートを決められる。コンテナ側のポートはホスト側のポートに変換される際、ポート番号が変更される(49153以降になる)ので、<code>docker port &lt;ジョブのID&gt; &lt;ポート番号&gt;</code> あるいは <code>docker ps</code> でポートの対応状況を確認する必要がある。</p>

<p>ドキュメントの <a href="https://github.com/dotcloud/docker#expose-a-service-on-a-tcp-port">Expose a service on a TCP port</a> がわかりやすい。</p>

<pre><code># 以下、コメントは書き換えてある
# また、途中経過がわかりやすいように set -x しておく
set -x

# 4444 を晒すよう -p オプションをつけて docker run しつつ、
# コンテナは netcat で4444を待ち受ける
JOB=$(docker run -d -p 4444 base /bin/nc -l -p 4444)
++ docker run -d -p 4444 base /bin/nc -l -p 4444
+ JOB=c86c892574f7

# 4444 がローカルのどのポートに対応するのか確認
# docker ps でも調べることはできる
PORT=$(docker port $JOB 4444)
++ docker port c86c892574f7 4444
+ PORT=49166

# ルーティングによっては localhost とか 127.0.0.1 だと
# うまくいかないことがあるので、eth0 のIPアドレスを使おう、
# ってことらしい
IP=$(ifconfig eth0 | perl -n -e 'if (m/inet addr:([\d\.]+)/g) { print $1 }')
++ perl -n -e 'if (m/inet addr:([\d\.]+)/g) { print $1 }'
++ ifconfig eth0
+ IP=10.156.137.111
echo hello world | nc $IP $PORT
+ nc 10.156.137.111 49166
+ echo hello world

# コンテナが受信したメッセージを logs で表示
echo "Daemon received: $(docker logs $JOB)"
++ docker logs c86c892574f7
+ echo 'Daemon received: hello world'
Daemon received: hello world
</code></pre>

<h3>Dockerfile</h3>

<p>DSLで書かれた設定(通常ファイル名は<code>Dockerfile</code>とする)をあらかじめ用意することで、手順に従ってイメージを作ることができる。</p>

<pre><code>読み込ませ方 (1)
docker build &lt;Dockerfileのあるディレクトリ&gt;
# ex. docker build .

読み込ませ方 (2)
docker build -
# ex. docker build - &lt; /foo/bar/Dockerfile
</code></pre>

<p>Dockerfile の例</p>

<pre><code>FROM base
RUN /bin/echo hi
</code></pre>

<p>これで、<code>docker build</code> すれば <code>docker run base /bin/echo hi</code> と同じ効果が得られる。</p>

<p>指定できるはコマンドは以下の通り。大文字小文字は区別しないけど、引数と見分けやすいように大文字が使われる。</p>

<ul>
<li><code>FROM &lt;image&gt;</code> ベースとなるイメージを指定</li>
<li><code>MAINTAINER &lt;name&gt;</code> メンテナの名前を指定</li>
<li><code>RUN &lt;command&gt;</code> ビルド中に実行したいコマンドを指定</li>
<li><code>CMD &lt;command&gt;</code> 起動後のコンテナで実行したいコマンドを指定</li>
<li><code>EXPOSE &lt;port&gt; [&lt;port&gt; ...]</code> 外部に晒すポートの指定</li>
<li><code>ENV &lt;key&gt; &lt;value&gt;</code> 環境変数の設定</li>
<li><code>INSERT &lt;file url&gt; &lt;path&gt;</code> deprecated なので ADD を利用すること</li>
<li><code>ADD &lt;src&gt; &lt;dest&gt;</code> ファイルを配置</li>
</ul>


<p><code>RUN</code> と <code>CMD</code> の違いがわかりにくいかもしれない。例を出す。</p>

<pre><code># RUN, CMD で指定したコマンドが実行されたとき、
# 標準出力と /tmp/*.log に記録を残す

$ cat &lt;&lt;SCRIPT &gt;Dockerfile
&gt; FROM base
&gt; RUN /bin/echo run | tee /tmp/run.log
&gt; CMD /bin/echo cmd | tee /tmp/cmd.log
&gt; SCRIPT

# ビルドの実行

$ docker build .
Caching Context 10240/? (n/a)
FROM base ()
===&gt; b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc
RUN /bin/echo run | tee /tmp/run.log (b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc)
===&gt; d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d
CMD /bin/echo cmd | tee /tmp/cmd.log (d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d)
===&gt; 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229
Build successful.
===&gt; 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229

# run, cmd の実行結果を確認
# =&gt; run だけが実行されている

$ docker run 60671e99691 /bin/ls /tmp/
run.log

# イメージを inspect する
# =&gt; どうやらコンテナは記憶していることがわかる

$ docker inspect 60671e99691
{
    "id": "60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229",
    "parent": "d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d",
    "created": "2013-06-16T16:29:14.602237Z",
    "container": "4c54683cec90500f329dfaad2e0856cc408483be0ae3166018121d4d4b9b3282",
    "container_config": {
        "Hostname": "78c72f8ba6ad",
        "User": "",
        "Memory": 0,
        "MemorySwap": 0,
        "CpuShares": 0,
        "AttachStdin": false,
        "AttachStdout": false,
        "AttachStderr": false,
        "PortSpecs": null,
        "Tty": false,
        "OpenStdin": false,
        "StdinOnce": false,
        "Env": null,
        "Cmd": [
            "/bin/sh",
            "-c",
            "#(nop) CMD [/bin/sh -c /bin/echo cmd | tee /tmp/cmd.log]"
        ],
        "Dns": null,
        "Image": "d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d",
        "Volumes": null,

# 引数でコマンドを指定せずに run を実行
# =&gt; cmd で登録した内容が実行される

$ docker run 60671e99691
cmd
</code></pre>

<p>つまり、<code>RUN</code> は <code>Dockerfile</code> を元にビルドしているときに参照され、<code>CMD</code> はコンテナを実行する際に参照されるということがわかる。パッケージをインストールしたりといった用途では通常 <code>RUN</code> を使う。</p>

<h1>まとめ</h1>

<p>仮想環境の発達でプログラマブルなインフラストラクチャーは実現できてきているけど、マシンを上げたり下げたりするのにどうしても時間がかかるし、それは仕方が無いものと我慢していた。<code>Docker</code> を使ってみると、今までのそういった不満から解放されることができそう。一応開発中というステータスなのでプロダクション環境では使いづらいけど、開発やテスト、とくに構成管理ツールを設定するときなどは、この俊敏性、柔軟性は有効になると思う。</p>

<h1>参考</h1>

<ul>
<li><a href="http://docs.docker.io/en/latest/">Documentation</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす]]></title>
    <link href="http://apatheia.info/blog/2012/05/13/vps-lxc-xtradb-cluster/"/>
    <updated>2012-05-13T00:00:00+09:00</updated>
    <id>http://apatheia.info/blog/2012/05/13/vps-lxc-xtradb-cluster</id>
    <content type="html"><![CDATA[<p>ほんの数年前までVPSといえばLinode、Slicehostなどの海外のサーバーしか選択肢がなかった。ls を実行しても一呼吸おくほどのレイテンシーがあるような環境で、128MBくらいのメモリを何とかやりくりしてRailsを動かしていたが、現在では月1000円程度で用途によっては手に余るようなスペックが手に入るようになっている。そんなVPSの余ったリソースを使って、仮想環境をたてようというのが今回の目的だ。</p>

<!-- more -->


<p><a href="http://lxc.sourceforge.net/">LXC</a>は、他の仮想化方式と比べてオーバーヘッドが少なくきびきび動くし、必要であればCPUやメモリの制限をつけることもできる。RPMやDebのパッケージ作成をしたり、Chefのcookbook作成などで独立した環境を手軽に構築したい人には特に有用に思う。また、簡単にネットワークが作れるので、複数台構成のソフトウェアを1台のマシンのなかで動かすことが出来る。今回は動作確認として <a href="http://www.percona.com/software/percona-xtradb-cluster/">PerconaXtraDB Cluster</a>を動かしてみることにする。</p>

<h2>前提について</h2>

<p>作業環境は以下を想定している。</p>

<ul>
<li>さくらのVPS(v3) 1G

<ul>
<li>CentOS 6.2 x86_64</li>
</ul>
</li>
<li>LXC 0.7.5</li>
</ul>


<p>CentOSを使っているのはデフォルトのOSイメージだからというのが理由。</p>

<p>今回の内容をEC2上で実行する場合、Amazon Linux でもほとんど同様の設定で作業を行えることを確認しているけど、もっ と新しいOS、たとえば Ubuntu 12.04 LTS を使えば後述する cgroupの設定、bridgeの設定が不要となるためより簡単に行える。CentOS 6で実施したときだけ遭遇するような問題に何度もぶつかったので、出来るだけ新しいOSを使った方がいい。</p>

<p>仮想環境としては、lxcに同梱されているテンプレートを利用してUbuntuを、またOSイメージの作成からCentOSを構築する。</p>

<h2>構築方法</h2>

<p>以降の作業はすべて root で行うものとする。</p>

<h3>ネットワークの設定</h3>

<p>仮想環境とのやりとりで使うブリッジを作る。</p>

<pre><code># yum install bridge-utils
# vim /etc/sysconfig/network-scripts/ifcfg-lxcbr0

    DEVICE=lxcbr0
    TYPE=Bridge
    BOOTPROTO=none
    IPADDR=10.0.3.1
    NETMASK=255.255.255.0
    ONBOOT=yes

# ifup lxcbr0 # 起動
</code></pre>

<h3>cgroup</h3>

<pre><code># mount | grep cgroup # cgroup がないこと確認
# mkdir -p /cgroup
# printf "none          /cgroup     cgroup  defaults        0 0
" &gt;&gt; /etc/fstab
# mount -a
# mount | grep cgroup # cgroup があること確認
</code></pre>

<h3>lxc セットアップ</h3>

<pre><code># yum install libcap-devel docbook-utils
# yum groupinstall "Development Tools"

# wget [http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz](http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz)
# tar xf lxc-0.7.5.tar.gz
# cd lxc-0.7.5
# ./configure
# make rpm # この途中で /usr/lib64/lxc/{template,rootfs} がインストールされるのかなり狂ってる
# rpm -ivh ~/rpmbuild/RPMS/x86_64/lxc-0.7.5-1.x86_64.rpm
   (~/rpmbuild になければ、/usr/src/rpm から探す)
# mkdir -p /var/lib/lxc
</code></pre>

<h3>dnsmasq (DHCP, DNS サーバー) セットアップ</h3>

<p>環境を増やすごとに毎回NICの設定を編集するのは手間なので、ホスト側で dncp, dns の設定をする。</p>

<pre><code># yum install dnsmasq
# vim /etc/dnsmasq.conf

    コメントを外して有効化する、編集するなどで以下の設定を行う
    domain は自分の使いたい名前にすればいい

    domain-needed
    bogus-priv
    interface = lxcbr0
    listen-address = 127.0.0.1
    listen-address = 10.0.3.1
    expand-hosts
    domain = lxc
    dhcp-range = 10.0.3.50,10.0.3.200,1h

# service dnsmasq reload
</code></pre>

<h3>ネットワークセットアップ</h3>

<p>仮想環境から外部へのやりとりが出来るようにネットワークの設定を変更する。</p>

<pre><code># sysctl -w net.ipv4.ip_forward=1
# sed -i -re s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/ /etc/sysctl.conf
# iptables -A POSTROUTING -s 10.0.3.0/24 -t nat -j MASQUERADE
# service iptables save # 設定を /etc/sysconfig/iptables に保存
</code></pre>

<h3>仮想環境構築 (1) 同梱のスクリプトを使った Ubuntu のインストール</h3>

<p>lxcに同梱のスクリプト /usr/lib64/lxc/templates/lxc-ubuntu を使ってUbuntuをインストールする。</p>

<p>基本的な設定ファイルを作る。</p>

<pre><code># cd
# vim lxc.conf

    lxc.network.type=veth
    lxc.network.link=lxcbr0
    lxc.network.flags=up
</code></pre>

<p>今回は Ubuntu を導入するので、そのために必要なプログラムをインストールする。</p>

<pre><code># yum install --enablerepo=epel debootstrap dpkg
</code></pre>

<p>これで準備が出来たので、実際に仮想環境を動かしてみる。</p>

<pre><code># lxc-create -t ubuntu -f lxc.conf -n vm0
   -t がテンプレートの名前。 -t ubuntu なら /usr/lib64/lxc/templates/lxc-ubuntu が読み込まれる
      オプションでバージョンが指定可能だが、lxc 0.7.5 に同梱されているテンプレートのデフォルトだと Ubuntu 10.04 が選ばれる。
   -f がさっき作った設定ファイルの場所
   -n が環境の名前。今回は vm0 とした。 /var/lib/lxc/vm0 にファイルがおかれる
# lxc-start -n vm0 -l debug -o debug.out -d
   -l はデバッグレベル、-o はデバッグの場所を指定。安定して起動するようになったらつけなくていい
# lxc-console -n vm0
  一回エンター押した後、ユーザー root パスワード root でログイン
  抜けるときは Ctrl-a q

  lxc-console をしても何も表示されない状態になったら、以下を施して再起動

# vim /var/lib/lxc/vm0/rootfs/etc/init/lxc.conf

  telinit を差し込む

    --- /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf.orig 2012-02-07 10:28:25.000000000 +0900
    +++ /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf      2012-05-06 22:43:21.606098530 +0900
    @@ -12,5 +12,6 @@
        touch /var/run/utmp
        chown root:utmp /var/run/utmp
        initctl emit --no-wait net-device-added INTERFACE=lo || true
    +   telinit 3
        exit 0
     end script
</code></pre>

<p>lxc-console だとCtrl-aが使えなくて不便なので、今後はsshでログインしたい。テンプレートが自動的にOpenSSHをインストールしてくれるが、ちゃんと起動しない。仕方が無いので、update-rc.d で起動するように設定</p>

<pre><code>  仮想環境内で実行
# update-rc.d ssh enable
</code></pre>

<p>固定IPアドレスを振りたい場合は、設定を変更する。</p>

<pre><code>  ホスト側からの変更
# vim /var/lib/lxc/vm0/config

  lxc.network.ipv4 = 10.0.3.2/24

  仮想環境の中で変更
# vim /etc/network/interfaces

    変更前
    auto lo
    iface lo inet loopback

    auto eth0
    iface eth0 inet dhcp

    変更後
    auto lo
    iface lo inet loopback

    iface eth0 inet static
        address 10.0.3.2
        netmask 255.255.255.0
        gateway 10.0.3.1
</code></pre>

<p>仮想環境の破棄は lxc-destroy で行う</p>

<pre><code># lxc-destroy -n vm0
</code></pre>

<h3>仮想環境構築 (2) 独自に構築した CentOS 6 のインストール</h3>

<p>lxc-console の標準テンプレートでは CentOS が用意されていないので、自力でセットアップする。</p>

<h4>イメージ作成</h4>

<p>基本的に <a href="http://wiki.1tux.org/wiki/Centos6/Installation/Minimal_installation_using_yum">Centos6/Installation/Minimal installation using yum</a> の通り。ただし 64 bit 版をインストールする</p>

<pre><code># mkdir /t
# cd /t
# wget [http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm](http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm)
# rpm2cpio centos-release-6-2.el6.centos.7.x86_64.rpm  | cpio -idm
# sed -i s/$releasever/6/g ./etc/yum.repos.d/*
# yum --installroot=/t groupinstall base
# yum --installroot=/t install dhclient
# rm centos-release*.rpm
# chroot /t

  // ここから後はchroot内

# passwd # パスワード変更

# rm -f /dev/null
# mknod -m 666 /dev/null c 1 3
# mknod -m 666 /dev/zero c 1 5
# mknod -m 666 /dev/urandom c 1 9
# ln -s /dev/urandom /dev/random
# mknod -m 600 /dev/console c 5 1
# mknod -m 660 /dev/tty1 c 4 1
# chown root:tty /dev/tty1

# mkdir -p /dev/shm
# chmod 1777 /dev/shm
# mkdir -p /dev/pts
# chmod 755 /dev/pts

# cp -a /etc/skel/. /root/.

# cat &gt; /etc/resolv.conf &lt;&lt; END
# Google public DNS
nameserver 8.8.8.8
nameserver 8.8.4.4
END

# cat &gt; /etc/hosts &lt;&lt; END
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
END

# cat &gt; /etc/sysconfig/network &lt;&lt; END
NETWORKING=yes
HOSTNAME=localhost
END

# cat &gt; /etc/sysconfig/network-scripts/ifcfg-eth0  &lt;&lt; END
DEVICE=eth0
ONBOOT=yes
BOOTPROTO=dhcp
END

# cat &gt; /etc/fstab &lt;&lt; END
/dev/root               /                       rootfs   defaults        0 0
none                    /dev/shm                tmpfs    nosuid,nodev    0 0
END

# cat &gt; /etc/init/lxc-sysinit.conf &lt;&lt; END
start on startup
env container

pre-start script
        if [ "x$container" != "xlxc" -a "x$container" != "xlibvirt" ]; then
                stop;
        fi
        telinit 3
        initctl start tty TTY=console
        exit 0;
end script
END

# exit

// ここから後はchroot外

# cd /t
# tar cvfz /centos6-lxc-root.tgz .
</code></pre>

<h4>設定</h4>

<pre><code># mkdir /var/lib/lxc/vm0
# cd /var/lib/lxc/vm0
# mkdir rootfs
# cd rootfs
# tar xfz /centos6-lxc-root.tgz --numeric-owner
# cd /var/lib/lxc/vm0

# cat &gt;/var/lib/lxc/vm0/config &lt;&lt; END
lxc.network.type=veth
lxc.network.link=lxcbr0
lxc.network.flags=up
lxc.network.veth.pair=veth-vm0
lxc.utsname = vm0

lxc.tty = 1
lxc.pts = 1024
lxc.rootfs = /var/lib/lxc/vm0/rootfs
lxc.mount  = /var/lib/lxc/vm0/fstab
lxc.arch = x86_64
lxc.cap.drop = sys_module mac_admin

lxc.cgroup.devices.deny = a
# Allow any mknod (but not using the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
# /dev/null and zero
lxc.cgroup.devices.allow = c 1:3 rwm
lxc.cgroup.devices.allow = c 1:5 rwm
# consoles
lxc.cgroup.devices.allow = c 5:1 rwm
lxc.cgroup.devices.allow = c 5:0 rwm
# /dev/{,u}random
lxc.cgroup.devices.allow = c 1:9 rwm
lxc.cgroup.devices.allow = c 1:8 rwm
lxc.cgroup.devices.allow = c 136:* rwm
lxc.cgroup.devices.allow = c 5:2 rwm
# rtc
lxc.cgroup.devices.allow = c 254:0 rwm
#fuse
lxc.cgroup.devices.allow = c 10:229 rwm
#tun
lxc.cgroup.devices.allow = c 10:200 rwm
#full
lxc.cgroup.devices.allow = c 1:7 rwm
#hpet
lxc.cgroup.devices.allow = c 10:228 rwm
#kvm
lxc.cgroup.devices.allow = c 10:232 rwm
END

# cat &gt; fstab  &lt;&lt; END
proc            /var/lib/lxc/vm0/rootfs/proc         proc    nodev,noexec,nosuid 0 0
sysfs           /var/lib/lxc/vm0/rootfs/sys          sysfs defaults  0 0
END
</code></pre>

<h4>起動</h4>

<pre><code># lxc-start -n vm0 -l debug -o debug.out -d
# lxc-console -n vm0

OpenSSH がなければ入れておく
# yum install openssh-server
# service sshd start
</code></pre>

<h2>動作確認 (Percona XtraDB Cluster の稼働確認)</h2>

<p>動作確認として Percona XtraDB Cluster を動かしてみる。</p>

<p>すでにこれまでの作業を通して vm0 としてCentOS 6がインストール済みとする。</p>

<h3>ホスト側設定</h3>

<ul>
<li>構成

<ul>
<li>ホスト, IPアドレス 10.0.3.1</li>
<li>仮想0 vm0, IPアドレス 10.0.3.2</li>
<li>仮想1 vm1, IPアドレス 10.0.3.3</li>
<li>仮想2 vm2, IPアドレス 10.0.3.4</li>
</ul>
</li>
</ul>


<p>各仮想環境に簡単にアクセスできるように hosts を設定しておく。ホスト側に設定しておけば、dnsmasq のおかげで仮想側でも名前が引けるようになる。</p>

<pre><code># vim /etc/hosts
    以下を追記
    10.0.3.2 vm0
    10.0.3.3 vm1
    10.0.3.4 vm2
</code></pre>

<h3>コピー元(vm0) 設定</h3>

<pre><code># ssh vm0
  ここからはvm0の中

  固定IPアドレスを設定
# vim /var/lib/lxc/vm1/rootfs/etc/sysconfig/network-scripts/ifcfg-eth0
    DEVICE=eth0
    ONBOOT=yes
    BOOTPROTO=static
    IPADDR=10.0.3.3
    NETMASK=255.255.255.0
    GATEWAY=10.0.3.1

  XtraDB Cluster インストール
# rpm -Uhv [http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm](http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm)
# rpm -Uhv [http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm](http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm)
# yum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client
# cat &gt; /etc/my.cnf &lt;&lt;END
[mysqld]
binlog_format=ROW
wsrep_provider=/usr/lib64/libgalera_smm.so
wsrep_cluster_address=gcomm://
wsrep_slave_threads=2
wsrep_cluster_name=lxccluster
wsrep_sst_method=rsync
wsrep_node_name=node0
innodb_locks_unsafe_for_binlog=1
innodb_autoinc_lock_mode=2
END

# poweroff
</code></pre>

<h3>コピー、起動</h3>

<pre><code># lxc-clone -n vm1 -o vm0
  -n はこれから作る仮想環境の名前
  -o はコピー元の仮想環境の名前
# lxc-clone -n vm1 -o vm0
# vim /var/lib/lxc/vm1/config
  vm0をvm1に置換 (vm2ではvm2に置換)
  IPアドレスを10.0.3.2 -&gt; 10.0.3.3 に変更 (vm2では 10.0.3.4に変更)
# vim /var/lib/lxc/vm1/rootfs/etc/my.cnf
    wsrep_cluster_address=gcomm:// をwsrep_cluster_address=gcomm://10.0.3.2 に変更
    wsrep_node_name=node0 を wsrep_node_name=node1 に変更 (vm2ではnode2に変更)

  同様にvm0からvm2のコピーを実施
</code></pre>

<p>3つの環境が完成したら起動</p>

<pre><code># lxc-start -n vm0 -l debug -o debug.0.out -d
# lxc-start -n vm1 -l debug -o debug.1.out -d
# lxc-start -n vm2 -l debug -o debug.2.out -d
</code></pre>

<h3>動作確認</h3>

<p>vm0 にログインして実行</p>

<pre><code># mysql -u root
  データベース、テーブル作成
mysql&gt; create database t;
mysql&gt; use t;
mysql&gt; create table sample (
id int not null primary key auto_increment,
value int
);

データ投入
mysql&gt; insert into sample set value = 1;
mysql&gt; insert into sample set value = 1;
mysql&gt; insert into sample set value = 1;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
+----+-------+
</code></pre>

<p>IDがスキップしながらインサートされることがわかる。引き続き、他の環境でもデータを入れてみる。</p>

<p>vm1 にログインして実行</p>

<pre><code>mysql&gt; use t;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
+----+-------+
mysql&gt; insert into sample set value =  1;
mysql&gt; insert into sample set value =  1;
mysql&gt; insert into sample set value =  1;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
|  9 |     1 |
| 12 |     1 |
| 15 |     1 |
+----+-------+
</code></pre>

<p>同様のことがvm2でも起きる。</p>

<p>これにより、XtraDB Cluster の以下の動作が確認出来た。</p>

<ul>
<li>すべてのサーバーで書き込みと参照がおこなえること</li>
<li>オートインクリメントがバッティングしないように、値が自動的にオフセットをつけて挿入されること</li>
</ul>


<h1>メモ</h1>

<h2>外部から仮想環境へ直接アクセスしたい場合</h2>

<p>たとえば、外部からポート10080でアクセスされたとき、仮想環境の 10.0.3.51 のポート 80 へ転送させたい場合は iptables
で以下のような設定をする。</p>

<pre><code># vim /etc/syscofig/iptables
    -A POSTROUTING -s 10.0.3.0/24 -j MASQUERADE の下に以下を追加
    -A PREROUTING -i eth0 -p tcp --dport 10080 -j DNAT --to-destination 10.0.3.51:80
# service iptables condrestart
# iptables -L -t nat # NATテーブルから設定追加を確認
</code></pre>

<h2>新しい Ubuntu を入れたい場合</h2>

<p>元の手順だとlucid (10.04) がインストールされるが、たとえば oneiric (11.10) であれば以下でインストール可能。</p>

<pre><code># cp -a /usr/share/debootstrap/scripts/lucid  /usr/share/debootstrap/scripts/oneiric
    lucid は /usr/share/debootstrap/scripts/gutsy のシンボリックリンクで、他のリリースも同様。とにかくファ イル名が参照できるようにシンボリックリンクをコピーしておけばいい。
# lxc-create -t ubuntu -f lxc.conf -n vm0 -- --trim -r oneiric
    lxc-create ではなく -r はテンプレートへの引数
</code></pre>

<h2>他の OS もインストールしてみたい場合</h2>

<p>/usr/lib64/lxc/templates/ には lxc-busybox,lxc-debian,lxc-fedora,lxc-lenny,lxc-
opensuse,lxc-sshd,lxc-ubuntu の テンプレートがある。これ以外の環境が必要であれば、「lxc guset
OS名」とかで検索してみる。</p>

<h1>参考</h1>

<ul>
<li><a href="http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc">http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc</a></li>
<li><a href="http://wiki.debian.org/LXC">http://wiki.debian.org/LXC</a></li>
<li><a href="https://help.ubuntu.com/12.04/serverguide/lxc.html">https://help.ubuntu.com/12.04/serverguide/lxc.html</a></li>
<li><a href="http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104">http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104</a></li>
<li><a href="http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6">http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6</a></li>
<li><a href="http://www.percona.com/doc/percona-xtradb-cluster/index.html">http://www.percona.com/doc/percona-xtradb-cluster/index.html</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
