<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mysql | apatheia.info]]></title>
  <link href="http://apatheia.info/blog/categories/mysql/atom.xml" rel="self"/>
  <link href="http://apatheia.info/"/>
  <updated>2013-03-17T22:54:14+09:00</updated>
  <id>http://apatheia.info/</id>
  <author>
    <name><![CDATA[f440]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす]]></title>
    <link href="http://apatheia.info/blog/2012/05/13/vps-lxc-xtradb-cluster/"/>
    <updated>2012-05-13T00:00:00+09:00</updated>
    <id>http://apatheia.info/blog/2012/05/13/vps-lxc-xtradb-cluster</id>
    <content type="html"><![CDATA[<p>ほんの数年前までVPSといえばLinode、Slicehostなどの海外のサーバーしか選択肢がなかった。ls を実行しても一呼吸おくほどのレイテンシーがあるような環境で、128MBくらいのメモリを何とかやりくりしてRailsを動かしていたが、現在では月1000円程度で用途によっては手に余るようなスペックが手に入るようになっている。そんなVPSの余ったリソースを使って、仮想環境をたてようというのが今回の目的だ。</p>

<!-- more -->


<p><a href="http://lxc.sourceforge.net/">LXC</a>は、他の仮想化方式と比べてオーバーヘッドが少なくきびきび動くし、必要であればCPUやメモリの制限をつけることもできる。RPMやDebのパッケージ作成をしたり、Chefのcookbook作成などで独立した環境を手軽に構築したい人には特に有用に思う。また、簡単にネットワークが作れるので、複数台構成のソフトウェアを1台のマシンのなかで動かすことが出来る。今回は動作確認として <a href="http://www.percona.com/software/percona-xtradb-cluster/">PerconaXtraDB Cluster</a>を動かしてみることにする。</p>

<h2>前提について</h2>

<p>作業環境は以下を想定している。</p>

<ul>
<li>さくらのVPS(v3) 1G

<ul>
<li>CentOS 6.2 x86_64</li>
</ul>
</li>
<li>LXC 0.7.5</li>
</ul>


<p>CentOSを使っているのはデフォルトのOSイメージだからというのが理由。</p>

<p>今回の内容をEC2上で実行する場合、Amazon Linux でもほとんど同様の設定で作業を行えることを確認しているけど、もっ と新しいOS、たとえば Ubuntu 12.04 LTS を使えば後述する cgroupの設定、bridgeの設定が不要となるためより簡単に行える。CentOS 6で実施したときだけ遭遇するような問題に何度もぶつかったので、出来るだけ新しいOSを使った方がいい。</p>

<p>仮想環境としては、lxcに同梱されているテンプレートを利用してUbuntuを、またOSイメージの作成からCentOSを構築する。</p>

<h2>構築方法</h2>

<p>以降の作業はすべて root で行うものとする。</p>

<h3>ネットワークの設定</h3>

<p>仮想環境とのやりとりで使うブリッジを作る。</p>

<pre><code># yum install bridge-utils
# vim /etc/sysconfig/network-scripts/ifcfg-lxcbr0

    DEVICE=lxcbr0
    TYPE=Bridge
    BOOTPROTO=none
    IPADDR=10.0.3.1
    NETMASK=255.255.255.0
    ONBOOT=yes

# ifup lxcbr0 # 起動
</code></pre>

<h3>cgroup</h3>

<pre><code># mount | grep cgroup # cgroup がないこと確認
# mkdir -p /cgroup
# printf "none          /cgroup     cgroup  defaults        0 0
" &gt;&gt; /etc/fstab
# mount -a
# mount | grep cgroup # cgroup があること確認
</code></pre>

<h3>lxc セットアップ</h3>

<pre><code># yum install libcap-devel docbook-utils
# yum groupinstall "Development Tools"

# wget [http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz](http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz)
# tar xf lxc-0.7.5.tar.gz
# cd lxc-0.7.5
# ./configure
# make rpm # この途中で /usr/lib64/lxc/{template,rootfs} がインストールされるのかなり狂ってる
# rpm -ivh ~/rpmbuild/RPMS/x86_64/lxc-0.7.5-1.x86_64.rpm
   (~/rpmbuild になければ、/usr/src/rpm から探す)
# mkdir -p /var/lib/lxc
</code></pre>

<h3>dnsmasq (DHCP, DNS サーバー) セットアップ</h3>

<p>環境を増やすごとに毎回NICの設定を編集するのは手間なので、ホスト側で dncp, dns の設定をする。</p>

<pre><code># yum install dnsmasq
# vim /etc/dnsmasq.conf

    コメントを外して有効化する、編集するなどで以下の設定を行う
    domain は自分の使いたい名前にすればいい

    domain-needed
    bogus-priv
    interface = lxcbr0
    listen-address = 127.0.0.1
    listen-address = 10.0.3.1
    expand-hosts
    domain = lxc
    dhcp-range = 10.0.3.50,10.0.3.200,1h

# service dnsmasq reload
</code></pre>

<h3>ネットワークセットアップ</h3>

<p>仮想環境から外部へのやりとりが出来るようにネットワークの設定を変更する。</p>

<pre><code># sysctl -w net.ipv4.ip_forward=1
# sed -i -re s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/ /etc/sysctl.conf
# iptables -A POSTROUTING -s 10.0.3.0/24 -t nat -j MASQUERADE
# service iptables save # 設定を /etc/sysconfig/iptables に保存
</code></pre>

<h3>仮想環境構築 (1) 同梱のスクリプトを使った Ubuntu のインストール</h3>

<p>lxcに同梱のスクリプト /usr/lib64/lxc/templates/lxc-ubuntu を使ってUbuntuをインストールする。</p>

<p>基本的な設定ファイルを作る。</p>

<pre><code># cd
# vim lxc.conf

    lxc.network.type=veth
    lxc.network.link=lxcbr0
    lxc.network.flags=up
</code></pre>

<p>今回は Ubuntu を導入するので、そのために必要なプログラムをインストールする。</p>

<pre><code># yum install --enablerepo=epel debootstrap dpkg
</code></pre>

<p>これで準備が出来たので、実際に仮想環境を動かしてみる。</p>

<pre><code># lxc-create -t ubuntu -f lxc.conf -n vm0
   -t がテンプレートの名前。 -t ubuntu なら /usr/lib64/lxc/templates/lxc-ubuntu が読み込まれる
      オプションでバージョンが指定可能だが、lxc 0.7.5 に同梱されているテンプレートのデフォルトだと Ubuntu 10.04 が選ばれる。
   -f がさっき作った設定ファイルの場所
   -n が環境の名前。今回は vm0 とした。 /var/lib/lxc/vm0 にファイルがおかれる
# lxc-start -n vm0 -l debug -o debug.out -d
   -l はデバッグレベル、-o はデバッグの場所を指定。安定して起動するようになったらつけなくていい
# lxc-console -n vm0
  一回エンター押した後、ユーザー root パスワード root でログイン
  抜けるときは Ctrl-a q

  lxc-console をしても何も表示されない状態になったら、以下を施して再起動

# vim /var/lib/lxc/vm0/rootfs/etc/init/lxc.conf

  telinit を差し込む

    --- /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf.orig 2012-02-07 10:28:25.000000000 +0900
    +++ /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf      2012-05-06 22:43:21.606098530 +0900
    @@ -12,5 +12,6 @@
        touch /var/run/utmp
        chown root:utmp /var/run/utmp
        initctl emit --no-wait net-device-added INTERFACE=lo || true
    +   telinit 3
        exit 0
     end script
</code></pre>

<p>lxc-console だとCtrl-aが使えなくて不便なので、今後はsshでログインしたい。テンプレートが自動的にOpenSSHをインストールしてくれるが、ちゃんと起動しない。仕方が無いので、update-rc.d で起動するように設定</p>

<pre><code>  仮想環境内で実行
# update-rc.d ssh enable
</code></pre>

<p>固定IPアドレスを振りたい場合は、設定を変更する。</p>

<pre><code>  ホスト側からの変更
# vim /var/lib/lxc/vm0/config

  lxc.network.ipv4 = 10.0.3.2/24

  仮想環境の中で変更
# vim /etc/network/interfaces

    変更前
    auto lo
    iface lo inet loopback

    auto eth0
    iface eth0 inet dhcp

    変更後
    auto lo
    iface lo inet loopback

    iface eth0 inet static
        address 10.0.3.2
        netmask 255.255.255.0
        gateway 10.0.3.1
</code></pre>

<p>仮想環境の破棄は lxc-destroy で行う</p>

<pre><code># lxc-destroy -n vm0
</code></pre>

<h3>仮想環境構築 (2) 独自に構築した CentOS 6 のインストール</h3>

<p>lxc-console の標準テンプレートでは CentOS が用意されていないので、自力でセットアップする。</p>

<h4>イメージ作成</h4>

<p>基本的に <a href="http://wiki.1tux.org/wiki/Centos6/Installation/Minimal_installation_using_yum">Centos6/Installation/Minimal installation using yum</a> の通り。ただし 64 bit 版をインストールする</p>

<pre><code># mkdir /t
# cd /t
# wget [http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm](http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm)
# rpm2cpio centos-release-6-2.el6.centos.7.x86_64.rpm  | cpio -idm
# sed -i s/$releasever/6/g ./etc/yum.repos.d/*
# yum --installroot=/t groupinstall base
# yum --installroot=/t install dhclient
# rm centos-release*.rpm
# chroot /t

  // ここから後はchroot内

# passwd # パスワード変更

# rm -f /dev/null
# mknod -m 666 /dev/null c 1 3
# mknod -m 666 /dev/zero c 1 5
# mknod -m 666 /dev/urandom c 1 9
# ln -s /dev/urandom /dev/random
# mknod -m 600 /dev/console c 5 1
# mknod -m 660 /dev/tty1 c 4 1
# chown root:tty /dev/tty1

# mkdir -p /dev/shm
# chmod 1777 /dev/shm
# mkdir -p /dev/pts
# chmod 755 /dev/pts

# cp -a /etc/skel/. /root/.

# cat &gt; /etc/resolv.conf &lt;&lt; END
# Google public DNS
nameserver 8.8.8.8
nameserver 8.8.4.4
END

# cat &gt; /etc/hosts &lt;&lt; END
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
END

# cat &gt; /etc/sysconfig/network &lt;&lt; END
NETWORKING=yes
HOSTNAME=localhost
END

# cat &gt; /etc/sysconfig/network-scripts/ifcfg-eth0  &lt;&lt; END
DEVICE=eth0
ONBOOT=yes
BOOTPROTO=dhcp
END

# cat &gt; /etc/fstab &lt;&lt; END
/dev/root               /                       rootfs   defaults        0 0
none                    /dev/shm                tmpfs    nosuid,nodev    0 0
END

# cat &gt; /etc/init/lxc-sysinit.conf &lt;&lt; END
start on startup
env container

pre-start script
        if [ "x$container" != "xlxc" -a "x$container" != "xlibvirt" ]; then
                stop;
        fi
        telinit 3
        initctl start tty TTY=console
        exit 0;
end script
END

# exit

// ここから後はchroot外

# cd /t
# tar cvfz /centos6-lxc-root.tgz .
</code></pre>

<h4>設定</h4>

<pre><code># mkdir /var/lib/lxc/vm0
# cd /var/lib/lxc/vm0
# mkdir rootfs
# cd rootfs
# tar xfz /centos6-lxc-root.tgz --numeric-owner
# cd /var/lib/lxc/vm0

# cat &gt;/var/lib/lxc/vm0/config &lt;&lt; END
lxc.network.type=veth
lxc.network.link=lxcbr0
lxc.network.flags=up
lxc.network.veth.pair=veth-vm0
lxc.utsname = vm0

lxc.tty = 1
lxc.pts = 1024
lxc.rootfs = /var/lib/lxc/vm0/rootfs
lxc.mount  = /var/lib/lxc/vm0/fstab
lxc.arch = x86_64
lxc.cap.drop = sys_module mac_admin

lxc.cgroup.devices.deny = a
# Allow any mknod (but not using the node)
lxc.cgroup.devices.allow = c *:* m
lxc.cgroup.devices.allow = b *:* m
# /dev/null and zero
lxc.cgroup.devices.allow = c 1:3 rwm
lxc.cgroup.devices.allow = c 1:5 rwm
# consoles
lxc.cgroup.devices.allow = c 5:1 rwm
lxc.cgroup.devices.allow = c 5:0 rwm
# /dev/{,u}random
lxc.cgroup.devices.allow = c 1:9 rwm
lxc.cgroup.devices.allow = c 1:8 rwm
lxc.cgroup.devices.allow = c 136:* rwm
lxc.cgroup.devices.allow = c 5:2 rwm
# rtc
lxc.cgroup.devices.allow = c 254:0 rwm
#fuse
lxc.cgroup.devices.allow = c 10:229 rwm
#tun
lxc.cgroup.devices.allow = c 10:200 rwm
#full
lxc.cgroup.devices.allow = c 1:7 rwm
#hpet
lxc.cgroup.devices.allow = c 10:228 rwm
#kvm
lxc.cgroup.devices.allow = c 10:232 rwm
END

# cat &gt; fstab  &lt;&lt; END
proc            /var/lib/lxc/vm0/rootfs/proc         proc    nodev,noexec,nosuid 0 0
sysfs           /var/lib/lxc/vm0/rootfs/sys          sysfs defaults  0 0
END
</code></pre>

<h4>起動</h4>

<pre><code># lxc-start -n vm0 -l debug -o debug.out -d
# lxc-console -n vm0

OpenSSH がなければ入れておく
# yum install openssh-server
# service sshd start
</code></pre>

<h2>動作確認 (Percona XtraDB Cluster の稼働確認)</h2>

<p>動作確認として Percona XtraDB Cluster を動かしてみる。</p>

<p>すでにこれまでの作業を通して vm0 としてCentOS 6がインストール済みとする。</p>

<h3>ホスト側設定</h3>

<ul>
<li>構成

<ul>
<li>ホスト, IPアドレス 10.0.3.1</li>
<li>仮想0 vm0, IPアドレス 10.0.3.2</li>
<li>仮想1 vm1, IPアドレス 10.0.3.3</li>
<li>仮想2 vm2, IPアドレス 10.0.3.4</li>
</ul>
</li>
</ul>


<p>各仮想環境に簡単にアクセスできるように hosts を設定しておく。ホスト側に設定しておけば、dnsmasq のおかげで仮想側でも名前が引けるようになる。</p>

<pre><code># vim /etc/hosts
    以下を追記
    10.0.3.2 vm0
    10.0.3.3 vm1
    10.0.3.4 vm2
</code></pre>

<h3>コピー元(vm0) 設定</h3>

<pre><code># ssh vm0
  ここからはvm0の中

  固定IPアドレスを設定
# vim /var/lib/lxc/vm1/rootfs/etc/sysconfig/network-scripts/ifcfg-eth0
    DEVICE=eth0
    ONBOOT=yes
    BOOTPROTO=static
    IPADDR=10.0.3.3
    NETMASK=255.255.255.0
    GATEWAY=10.0.3.1

  XtraDB Cluster インストール
# rpm -Uhv [http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm](http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm)
# rpm -Uhv [http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm](http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm)
# yum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client
# cat &gt; /etc/my.cnf &lt;&lt;END
[mysqld]
binlog_format=ROW
wsrep_provider=/usr/lib64/libgalera_smm.so
wsrep_cluster_address=gcomm://
wsrep_slave_threads=2
wsrep_cluster_name=lxccluster
wsrep_sst_method=rsync
wsrep_node_name=node0
innodb_locks_unsafe_for_binlog=1
innodb_autoinc_lock_mode=2
END

# poweroff
</code></pre>

<h3>コピー、起動</h3>

<pre><code># lxc-clone -n vm1 -o vm0
  -n はこれから作る仮想環境の名前
  -o はコピー元の仮想環境の名前
# lxc-clone -n vm1 -o vm0
# vim /var/lib/lxc/vm1/config
  vm0をvm1に置換 (vm2ではvm2に置換)
  IPアドレスを10.0.3.2 -&gt; 10.0.3.3 に変更 (vm2では 10.0.3.4に変更)
# vim /var/lib/lxc/vm1/rootfs/etc/my.cnf
    wsrep_cluster_address=gcomm:// をwsrep_cluster_address=gcomm://10.0.3.2 に変更
    wsrep_node_name=node0 を wsrep_node_name=node1 に変更 (vm2ではnode2に変更)

  同様にvm0からvm2のコピーを実施
</code></pre>

<p>3つの環境が完成したら起動</p>

<pre><code># lxc-start -n vm0 -l debug -o debug.0.out -d
# lxc-start -n vm1 -l debug -o debug.1.out -d
# lxc-start -n vm2 -l debug -o debug.2.out -d
</code></pre>

<h3>動作確認</h3>

<p>vm0 にログインして実行</p>

<pre><code># mysql -u root
  データベース、テーブル作成
mysql&gt; create database t;
mysql&gt; use t;
mysql&gt; create table sample (
id int not null primary key auto_increment,
value int
);

データ投入
mysql&gt; insert into sample set value = 1;
mysql&gt; insert into sample set value = 1;
mysql&gt; insert into sample set value = 1;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
+----+-------+
</code></pre>

<p>IDがスキップしながらインサートされることがわかる。引き続き、他の環境でもデータを入れてみる。</p>

<p>vm1 にログインして実行</p>

<pre><code>mysql&gt; use t;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
+----+-------+
mysql&gt; insert into sample set value =  1;
mysql&gt; insert into sample set value =  1;
mysql&gt; insert into sample set value =  1;
mysql&gt; select * from sample;
+----+-------+
| id | value |
+----+-------+
|  2 |     1 |
|  5 |     1 |
|  8 |     1 |
|  9 |     1 |
| 12 |     1 |
| 15 |     1 |
+----+-------+
</code></pre>

<p>同様のことがvm2でも起きる。</p>

<p>これにより、XtraDB Cluster の以下の動作が確認出来た。</p>

<ul>
<li>すべてのサーバーで書き込みと参照がおこなえること</li>
<li>オートインクリメントがバッティングしないように、値が自動的にオフセットをつけて挿入されること</li>
</ul>


<h1>メモ</h1>

<h2>外部から仮想環境へ直接アクセスしたい場合</h2>

<p>たとえば、外部からポート10080でアクセスされたとき、仮想環境の 10.0.3.51 のポート 80 へ転送させたい場合は iptables
で以下のような設定をする。</p>

<pre><code># vim /etc/syscofig/iptables
    -A POSTROUTING -s 10.0.3.0/24 -j MASQUERADE の下に以下を追加
    -A PREROUTING -i eth0 -p tcp --dport 10080 -j DNAT --to-destination 10.0.3.51:80
# service iptables condrestart
# iptables -L -t nat # NATテーブルから設定追加を確認
</code></pre>

<h2>新しい Ubuntu を入れたい場合</h2>

<p>元の手順だとlucid (10.04) がインストールされるが、たとえば oneiric (11.10) であれば以下でインストール可能。</p>

<pre><code># cp -a /usr/share/debootstrap/scripts/lucid  /usr/share/debootstrap/scripts/oneiric
    lucid は /usr/share/debootstrap/scripts/gutsy のシンボリックリンクで、他のリリースも同様。とにかくファ イル名が参照できるようにシンボリックリンクをコピーしておけばいい。
# lxc-create -t ubuntu -f lxc.conf -n vm0 -- --trim -r oneiric
    lxc-create ではなく -r はテンプレートへの引数
</code></pre>

<h2>他の OS もインストールしてみたい場合</h2>

<p>/usr/lib64/lxc/templates/ には lxc-busybox,lxc-debian,lxc-fedora,lxc-lenny,lxc-
opensuse,lxc-sshd,lxc-ubuntu の テンプレートがある。これ以外の環境が必要であれば、「lxc guset
OS名」とかで検索してみる。</p>

<h1>参考</h1>

<ul>
<li><a href="http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc">http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc</a></li>
<li><a href="http://wiki.debian.org/LXC">http://wiki.debian.org/LXC</a></li>
<li><a href="https://help.ubuntu.com/12.04/serverguide/lxc.html">https://help.ubuntu.com/12.04/serverguide/lxc.html</a></li>
<li><a href="http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104">http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104</a></li>
<li><a href="http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6">http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6</a></li>
<li><a href="http://www.percona.com/doc/percona-xtradb-cluster/index.html">http://www.percona.com/doc/percona-xtradb-cluster/index.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySQL::Sandboxで環境を作ってBLACKHOLEエンジンを試す]]></title>
    <link href="http://apatheia.info/blog/2011/12/31/mysql-sandbox-blackhole/"/>
    <updated>2011-12-31T00:00:00+09:00</updated>
    <id>http://apatheia.info/blog/2011/12/31/mysql-sandbox-blackhole</id>
    <content type="html"><![CDATA[<p>ちょっとMySQLのBLACKHOLEエンジン使って調べたいことがあったんだけど、
<a href="http://mysqlsandbox.net/">MySQL::Sandbox</a> 使うとレプリケーション環境が簡単に構築できて便利。</p>

<!-- more -->


<p>以下は、MySQL::SandboxのセットアップからBLACKHOLEエンジン使うところまでの記録。</p>

<h3>前提</h3>

<ul>
<li>確認した環境は Debian 6.0.3, perl v5.12.3</li>
<li>cpanm &amp; perlbrew インストール済み（<code>cpan</code>コマンドでも問題無いはず）</li>
<li>MySQL 5.5 はサーバーを起動するためにlibaio1が必要なので、あらかじめパッケージをインストールしておく</li>
</ul>


<h3>手順</h3>

<h4>インストール</h4>

<p><code>cpanm</code> でMySQL::Sandbox をインストール。</p>

<pre><code>$ cpanm MySQL::Sandbox
</code></pre>

<p>これで make_sandbox などのコマンド群がインストールされる。</p>

<p>データファイルは以下のようなファイル構成を取る</p>

<ul>
<li>$HOME/opt/mysql 以下のサブディレクトリにサーバーを設置 (環境変数 SANDBOX_BINARY で変更可能)</li>
<li>$HOME/sandboxes 以下のサブディレクトリにMySQLのデータや起動スクリプトを設置 (環境変数 SANDBOX_HOME で変更可能)</li>
</ul>


<h4>サーバーセットアップ</h4>

<p>サーバーを起動してみる。</p>

<pre><code># 使用するディレクトリ設定
$ export SANDBOX_BINARY=$HOME/opt/mysql
$ export SANDBOX_HOME=$HOME/opt/sandboxes
# ソース取得
$ cd SANDBOX_HOME
$ curl -L -o mysql-5.5.19-linux2.6-x86_64.tar.gz http://www-jp.mysql.com/get/Downloads/MySQL-5.5/mysql-5.5.19-linux2.6-x86_64.tar.gz/from/http://ftp.jaist.ac.jp/pub/mysql/
# Sandbox 作成
$ make_sandbox $SANDBOX_BINARY/mysql-5.5.19-linux2.6-x86_64.tar.gz
</code></pre>

<p>これで$SANDBOX_BINARY/5.5.19 にサーバーが、また $SANDBOX_HOME/msb_5_5_19 以下にインスタンスが作成される。</p>

<p>この時点で起動出来ているはずなので、接続してみる。以下で mysql クライアントが起動するはず。</p>

<pre><code>$ $SANDBOX_HOME/msb_5_5_19/use
</code></pre>

<p><code>use</code>以外には、<code>start</code>, <code>stop</code>, <code>status</code> などの名前から動作が推測できそうなコマンド群がある。$SANDBOX_HOME
には複数のサンドボックスを操作するコマンド <code>use_all</code>, <code>start_all</code>, <code>stop_all</code> などがある。</p>

<pre><code>$ make_replication_sandbox 5.5.19 
# 第二引数はtar.gzまでのパスでもいいが、一度展開されたらバージョン番号指定でもいい。make_sandboxも同様
</code></pre>

<p>以上で、 master, node1, node2 というサーバーが起動する。node1, node2 は master
を参照したレプリケーション構成となる。デフォルトでノードは2台だが、<code>--how_many_nodes</code> オプションで台数は変更可能。</p>

<p>同様に<a href="http://dev.mysql.com/doc/refman/5.1/ja/replication-%0Atopology-circular.html">Circular recplication</a>(日本語訳だとなんになるんだろう)も簡単に作れる。マスター/マスターレプリケーションはCircular
replication が2台のみの構成だった場合に同じ。</p>

<pre><code>$ make_replication_sandbox --circular=4 5.5.19
</code></pre>

<p>これで node1 -> node2, node2 -> nod3, node3 -> node4, node4 -> node1
という循環関係のレプリケーションが作れる。</p>

<p>使い終わったら止めておこう。</p>

<pre><code>$ $SANDBOX_HOME/stop_all
</code></pre>

<h3>BLACKHOLEエンジンを試す</h3>

<p>準備ができたので、<a href="http://dev.mysql.com/doc/refman/5.1/ja/blackhole-%0Astorage-engine.html">BLACKHOLEエンジン</a>を使ってみる。BLACKHOLEエンジンはバイナリログは記録するが、データは残さないストレージエンジンのこと。</p>

<p>まずは、サンドボックス作成</p>

<p>$ make_replication_sandbox --circular=3 5.5.19</p>

<p>デフォルトストレージエンジンをnode1, node3はInnoDB、node2 はBLACKHOLEに変更</p>

<pre><code>$ echo default_storage_engine=InnoDB &gt;&gt; $SANDBOX_HOME/rcsandbox_5_5_19/node1/my.sandbox.cnf
$ echo default_storage_engine=BLACKHOLE &gt;&gt; $SANDBOX_HOME/rcsandbox_5_5_19/node2/my.sandbox.cnf
$ echo default_storage_engine=InnoDB &gt;&gt; $SANDBOX_HOME/rcsandbox_5_5_19/node3/my.sandbox.cnf
$ $SANDBOX_HOME/rcsandbox_5_5_19/restart_all
</code></pre>

<p>node2, node3 のレプリケーションだけ再開して、node1 -> node2 -> node3 の2階層スレーブにする。</p>

<pre><code>$ $SANDBOX_HOME/rcsandbox_5_5_19/node2/use -e start slave
$ $SANDBOX_HOME/rcsandbox_5_5_19/node3/use -e start slave
</code></pre>

<p>実験のための構成が完成した。ここで、node1 にデータを流し込んだとき、node2 にはデータが残らなくて、node3 にデータが出来たら成功。</p>

<p>サンプルデータには、<a href="https://launchpad.net/test-%0Adb/">Sample database with test suite</a>を利用する。</p>

<pre><code>$ curl -LO [http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2](http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2)
$ tar xf employees_db-full-1.0.6.tar.bz2
$ cd employees_db
# InnoDB が決めうちで設定されているので、コメントアウト
$ sed -i -re s/^\s+(set storage_engine = InnoDB;)/-- \1/ *.sql
# 取り込み
$ $SANDBOX_HOME/rcsandbox_5_5_19/node1/use  -t &lt; ./employees.sql
</code></pre>

<p>結果</p>

<pre><code>$ cd $SANDBOX_HOME
$ du -sh rcsandbox_5_5_19/node?/data/ibdata1
219M    rcsandbox_5_5_19/node1/data/ibdata1
18M     rcsandbox_5_5_19/node2/data/ibdata1
219M    rcsandbox_5_5_19/node3/data/ibdata1
</code></pre>

<p>node2 だけデータファイルがふくらまない。</p>

<pre><code>$ ls -l rcsandbox_5_5_19/node2/data/
合計 357652
drwx------ 2 f440 f440      4096 2011-12-31 17:15 employees/
-rw-rw---- 1 f440 f440   5242880 2011-12-31 17:14 ib_logfile0
-rw-rw---- 1 f440 f440   5242880 2011-12-31 17:11 ib_logfile1
-rw-rw---- 1 f440 f440  18874368 2011-12-31 17:14 ibdata1
-rw-rw---- 1 f440 f440        88 2011-12-31 17:16 master.info
-rw-rw---- 1 f440 f440      5925 2011-12-31 17:14 msandbox.err
drwx------ 2 f440 f440      4096 2011-12-31 17:11 mysql/
-rw-rw---- 1 f440 f440      6273 2011-12-31 17:14 mysql-bin.000001
-rw-rw---- 1 f440 f440 168403385 2011-12-31 17:16 mysql-bin.000002
-rw-rw---- 1 f440 f440        38 2011-12-31 17:14 mysql-bin.index
-rw-rw---- 1 f440 f440       315 2011-12-31 17:14 mysql_sandbox15902-relay-bin.000005
-rw-rw---- 1 f440 f440 168399499 2011-12-31 17:16 mysql_sandbox15902-relay-bin.000006
-rw-rw---- 1 f440 f440        76 2011-12-31 17:14 mysql_sandbox15902-relay-bin.index
-rw-rw---- 1 f440 f440         5 2011-12-31 17:14 mysql_sandbox15902.pid
drwx------ 2 f440 f440      4096 2011-12-31 17:11 performance_schema/
-rw-rw---- 1 f440 f440        75 2011-12-31 17:16 relay-log.info
drwx------ 2 f440 f440      4096 2011-12-31 17:11 test/
</code></pre>

<p>node2 のバイナリログ、リレーログはちゃんと出来ているので、BLACKHOLEエンジンの適用を確認できた。</p>

<h3>メモ</h3>

<ul>
<li><code>$SANDBOX_HOME/clear_all</code> でデータ消せるの便利。</li>
<li>MySQL::Sandbox 3.0.19 からは、<a href="http://mysqlsandbox.net/news.html">Percona や Maria DB などの派生DBも扱える</a>みたい。いろいろなバージョンで試すことが多いのでうれしい。</li>
</ul>

]]></content>
  </entry>
  
</feed>
