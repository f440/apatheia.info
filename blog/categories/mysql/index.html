<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>tagged mysql - aptheia.info</title><link rel="alternate" type="application/rss+xml" title="apatheia.info" href="/atom.xml"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/490828b4c4131e135f17.css" as="style"/><link rel="stylesheet" href="/_next/static/css/490828b4c4131e135f17.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-1a8a258926ecde76681b.js" defer=""></script><script src="/_next/static/chunks/framework-895f067827ebe11ffe45.js" defer=""></script><script src="/_next/static/chunks/main-a9acf05574b3448968f1.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c8f1e010c17d4a8ac8e7.js" defer=""></script><script src="/_next/static/chunks/915-c287d7adb8a31cf4da35.js" defer=""></script><script src="/_next/static/chunks/pages/blog/categories/%5Btag%5D-c38c66d576a52b73b44b.js" defer=""></script><script src="/_next/static/jhtzWSA_9WoQI8qF29M1i/_buildManifest.js" defer=""></script><script src="/_next/static/jhtzWSA_9WoQI8qF29M1i/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header><div id="site-title"><h1><a href="/">apatheia.info</a></h1></div><nav><ul><li><a href="/">Home</a></li><li><a href="/atom.xml">RSS</a></li></ul></nav></header><main><article><h1>Articles tagged &#x27;<!-- -->mysql<!-- -->&#x27;</h1><section style="margin-bottom:1em"><h2><a href="/blog/2012/05/13/vps-lxc-xtradb-cluster/">さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす</a></h2><span>2012.05.13</span> <span><a href="/blog/categories/mysql/">mysql</a> </span><span><a href="/blog/categories/lxc/">lxc</a> </span><span><a href="/blog/categories/linux/">linux</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2011/12/31/mysql-sandbox-blackhole/">MySQL::Sandboxで環境を作ってBLACKHOLEエンジンを試す</a></h2><span>2011.12.31</span> <span><a href="/blog/categories/mysql/">mysql</a> </span></section></article></main><footer><ul><li>Link:</li><li><a href="https://twitter.com/f440">Twitter</a></li><li><a href="https://github.com/f440">Github</a></li><li><a href="https://pinbaord.in/u:f440">Pinbaord</a></li></ul></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":"mysql","posts":[{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-05-13-vps-lxc-xtradb-cluster.markdown","path":"blog/2012/05/13/vps-lxc-xtradb-cluster","layout":"post","title":"さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす","createdAt":"2012-05-12T15:00:00.000Z","kind":"article","comments":true,"tags":["mysql","lxc","linux"],"content":"ほんの数年前までVPSといえばLinode、Slicehostなどの海外のサーバーしか選択肢がなかった。ls を実行しても一呼吸おくほどのレイテンシーがあるような環境で、128MBくらいのメモリを何とかやりくりしてRailsを動かしていたが、現在では月1000円程度で用途によっては手に余るようなスペックが手に入るようになっている。そんなVPSの余ったリソースを使って、仮想環境をたてようというのが今回の目的だ。\n\n\u003c!-- more --\u003e\n\n[LXC](http://lxc.sourceforge.net/)は、他の仮想化方式と比べてオーバーヘッドが少なくきびきび動くし、必要であればCPUやメモリの制限をつけることもできる。RPMやDebのパッケージ作成をしたり、Chefのcookbook作成などで独立した環境を手軽に構築したい人には特に有用に思う。また、簡単にネットワークが作れるので、複数台構成のソフトウェアを1台のマシンのなかで動かすことが出来る。今回は動作確認として [PerconaXtraDB Cluster](http://www.percona.com/software/percona-xtradb-cluster/)を動かしてみることにする。\n\n## 前提について\n\n作業環境は以下を想定している。\n\n  * さくらのVPS(v3) 1G \n    * CentOS 6.2 x86_64\n  * LXC 0.7.5\n\nCentOSを使っているのはデフォルトのOSイメージだからというのが理由。\n\n今回の内容をEC2上で実行する場合、Amazon Linux でもほとんど同様の設定で作業を行えることを確認しているけど、もっ と新しいOS、たとえば Ubuntu 12.04 LTS を使えば後述する cgroupの設定、bridgeの設定が不要となるためより簡単に行える。CentOS 6で実施したときだけ遭遇するような問題に何度もぶつかったので、出来るだけ新しいOSを使った方がいい。\n\n仮想環境としては、lxcに同梱されているテンプレートを利用してUbuntuを、またOSイメージの作成からCentOSを構築する。\n\n## 構築方法\n\n以降の作業はすべて root で行うものとする。\n\n### ネットワークの設定\n\n仮想環境とのやりとりで使うブリッジを作る。\n\n    \n    # yum install bridge-utils\n    # vim /etc/sysconfig/network-scripts/ifcfg-lxcbr0\n    \n        DEVICE=lxcbr0\n        TYPE=Bridge\n        BOOTPROTO=none\n        IPADDR=10.0.3.1\n        NETMASK=255.255.255.0\n        ONBOOT=yes\n    \n    # ifup lxcbr0 # 起動\n    \n\n### cgroup\n\n    \n    # mount | grep cgroup # cgroup がないこと確認\n    # mkdir -p /cgroup\n    # printf \"none\t\t\t/cgroup\t\tcgroup\tdefaults\t\t0 0\n    \" \u003e\u003e /etc/fstab\n    # mount -a\n    # mount | grep cgroup # cgroup があること確認\n    \n\n### lxc セットアップ\n\n    \n    # yum install libcap-devel docbook-utils\n    # yum groupinstall \"Development Tools\"\n    \n    # wget [http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz](http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz)\n    # tar xf lxc-0.7.5.tar.gz\n    # cd lxc-0.7.5\n    # ./configure\n    # make rpm # この途中で /usr/lib64/lxc/{template,rootfs} がインストールされるのかなり狂ってる\n    # rpm -ivh ~/rpmbuild/RPMS/x86_64/lxc-0.7.5-1.x86_64.rpm\n       (~/rpmbuild になければ、/usr/src/rpm から探す)\n    # mkdir -p /var/lib/lxc\n    \n\n### dnsmasq (DHCP, DNS サーバー) セットアップ\n\n環境を増やすごとに毎回NICの設定を編集するのは手間なので、ホスト側で dncp, dns の設定をする。\n\n    \n    # yum install dnsmasq\n    # vim /etc/dnsmasq.conf\n    \n        コメントを外して有効化する、編集するなどで以下の設定を行う\n        domain は自分の使いたい名前にすればいい\n    \n        domain-needed\n        bogus-priv\n        interface = lxcbr0\n        listen-address = 127.0.0.1\n        listen-address = 10.0.3.1\n        expand-hosts\n        domain = lxc\n        dhcp-range = 10.0.3.50,10.0.3.200,1h\n    \n    # service dnsmasq reload\n    \n\n### ネットワークセットアップ\n\n仮想環境から外部へのやりとりが出来るようにネットワークの設定を変更する。\n\n    \n    # sysctl -w net.ipv4.ip_forward=1\n    # sed -i -re s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/ /etc/sysctl.conf\n    # iptables -A POSTROUTING -s 10.0.3.0/24 -t nat -j MASQUERADE\n    # service iptables save # 設定を /etc/sysconfig/iptables に保存\n    \n\n### 仮想環境構築 (1) 同梱のスクリプトを使った Ubuntu のインストール\n\nlxcに同梱のスクリプト /usr/lib64/lxc/templates/lxc-ubuntu を使ってUbuntuをインストールする。\n\n基本的な設定ファイルを作る。\n\n    \n    # cd\n    # vim lxc.conf\n    \n        lxc.network.type=veth\n        lxc.network.link=lxcbr0\n        lxc.network.flags=up\n    \n\n今回は Ubuntu を導入するので、そのために必要なプログラムをインストールする。\n\n    \n    # yum install --enablerepo=epel debootstrap dpkg\n    \n\nこれで準備が出来たので、実際に仮想環境を動かしてみる。\n\n    \n    # lxc-create -t ubuntu -f lxc.conf -n vm0\n       -t がテンプレートの名前。 -t ubuntu なら /usr/lib64/lxc/templates/lxc-ubuntu が読み込まれる\n          オプションでバージョンが指定可能だが、lxc 0.7.5 に同梱されているテンプレートのデフォルトだと Ubuntu 10.04 が選ばれる。\n       -f がさっき作った設定ファイルの場所\n       -n が環境の名前。今回は vm0 とした。 /var/lib/lxc/vm0 にファイルがおかれる\n    # lxc-start -n vm0 -l debug -o debug.out -d\n       -l はデバッグレベル、-o はデバッグの場所を指定。安定して起動するようになったらつけなくていい\n    # lxc-console -n vm0\n      一回エンター押した後、ユーザー root パスワード root でログイン\n      抜けるときは Ctrl-a q\n    \n      lxc-console をしても何も表示されない状態になったら、以下を施して再起動\n    \n    # vim /var/lib/lxc/vm0/rootfs/etc/init/lxc.conf\n    \n      telinit を差し込む\n    \n        --- /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf.orig 2012-02-07 10:28:25.000000000 +0900\n        +++ /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf      2012-05-06 22:43:21.606098530 +0900\n        @@ -12,5 +12,6 @@\n            touch /var/run/utmp\n            chown root:utmp /var/run/utmp\n            initctl emit --no-wait net-device-added INTERFACE=lo || true\n        +   telinit 3\n            exit 0\n         end script\n    \n\nlxc-console だとCtrl-aが使えなくて不便なので、今後はsshでログインしたい。テンプレートが自動的にOpenSSHをインストールしてくれるが、ちゃんと起動しない。仕方が無いので、update-rc.d で起動するように設定\n\n    \n      仮想環境内で実行\n    # update-rc.d ssh enable\n    \n\n固定IPアドレスを振りたい場合は、設定を変更する。\n\n    \n      ホスト側からの変更\n    # vim /var/lib/lxc/vm0/config\n    \n      lxc.network.ipv4 = 10.0.3.2/24\n    \n      仮想環境の中で変更\n    # vim /etc/network/interfaces\n    \n        変更前\n        auto lo\n        iface lo inet loopback\n    \n        auto eth0\n        iface eth0 inet dhcp\n    \n        変更後\n        auto lo\n        iface lo inet loopback\n    \n        iface eth0 inet static\n            address 10.0.3.2\n            netmask 255.255.255.0\n            gateway 10.0.3.1\n    \n\n仮想環境の破棄は lxc-destroy で行う\n\n    \n    # lxc-destroy -n vm0\n    \n\n### 仮想環境構築 (2) 独自に構築した CentOS 6 のインストール\n\nlxc-console の標準テンプレートでは CentOS が用意されていないので、自力でセットアップする。\n\n#### イメージ作成\n\n基本的に [Centos6/Installation/Minimal installation using yum](http://wiki.1tux.org/wiki/Centos6/Installation/Minimal_installation_using_yum) の通り。ただし 64 bit 版をインストールする\n\n    \n    # mkdir /t\n    # cd /t\n    # wget [http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm](http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm)\n    # rpm2cpio centos-release-6-2.el6.centos.7.x86_64.rpm  | cpio -idm\n    # sed -i s/$releasever/6/g ./etc/yum.repos.d/*\n    # yum --installroot=/t groupinstall base\n    # yum --installroot=/t install dhclient\n    # rm centos-release*.rpm\n    # chroot /t\n    \n      // ここから後はchroot内\n    \n    # passwd # パスワード変更\n    \n    # rm -f /dev/null\n    # mknod -m 666 /dev/null c 1 3\n    # mknod -m 666 /dev/zero c 1 5\n    # mknod -m 666 /dev/urandom c 1 9\n    # ln -s /dev/urandom /dev/random\n    # mknod -m 600 /dev/console c 5 1\n    # mknod -m 660 /dev/tty1 c 4 1\n    # chown root:tty /dev/tty1\n    \n    # mkdir -p /dev/shm\n    # chmod 1777 /dev/shm\n    # mkdir -p /dev/pts\n    # chmod 755 /dev/pts\n    \n    # cp -a /etc/skel/. /root/.\n    \n    # cat \u003e /etc/resolv.conf \u003c\u003c END\n    # Google public DNS\n    nameserver 8.8.8.8\n    nameserver 8.8.4.4\n    END\n    \n    # cat \u003e /etc/hosts \u003c\u003c END\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n    END\n    \n    # cat \u003e /etc/sysconfig/network \u003c\u003c END\n    NETWORKING=yes\n    HOSTNAME=localhost\n    END\n    \n    # cat \u003e /etc/sysconfig/network-scripts/ifcfg-eth0  \u003c\u003c END\n    DEVICE=eth0\n    ONBOOT=yes\n    BOOTPROTO=dhcp\n    END\n    \n    # cat \u003e /etc/fstab \u003c\u003c END\n    /dev/root               /                       rootfs   defaults        0 0\n    none                    /dev/shm                tmpfs    nosuid,nodev    0 0\n    END\n    \n    # cat \u003e /etc/init/lxc-sysinit.conf \u003c\u003c END\n    start on startup\n    env container\n    \n    pre-start script\n            if [ \"x$container\" != \"xlxc\" -a \"x$container\" != \"xlibvirt\" ]; then\n                    stop;\n            fi\n            telinit 3\n            initctl start tty TTY=console\n            exit 0;\n    end script\n    END\n    \n    # exit\n    \n    // ここから後はchroot外\n    \n    # cd /t\n    # tar cvfz /centos6-lxc-root.tgz .\n    \n\n#### 設定\n\n    \n    # mkdir /var/lib/lxc/vm0\n    # cd /var/lib/lxc/vm0\n    # mkdir rootfs\n    # cd rootfs\n    # tar xfz /centos6-lxc-root.tgz --numeric-owner\n    # cd /var/lib/lxc/vm0\n    \n    # cat \u003e/var/lib/lxc/vm0/config \u003c\u003c END\n    lxc.network.type=veth\n    lxc.network.link=lxcbr0\n    lxc.network.flags=up\n    lxc.network.veth.pair=veth-vm0\n    lxc.utsname = vm0\n    \n    lxc.tty = 1\n    lxc.pts = 1024\n    lxc.rootfs = /var/lib/lxc/vm0/rootfs\n    lxc.mount  = /var/lib/lxc/vm0/fstab\n    lxc.arch = x86_64\n    lxc.cap.drop = sys_module mac_admin\n    \n    lxc.cgroup.devices.deny = a\n    # Allow any mknod (but not using the node)\n    lxc.cgroup.devices.allow = c *:* m\n    lxc.cgroup.devices.allow = b *:* m\n    # /dev/null and zero\n    lxc.cgroup.devices.allow = c 1:3 rwm\n    lxc.cgroup.devices.allow = c 1:5 rwm\n    # consoles\n    lxc.cgroup.devices.allow = c 5:1 rwm\n    lxc.cgroup.devices.allow = c 5:0 rwm\n    # /dev/{,u}random\n    lxc.cgroup.devices.allow = c 1:9 rwm\n    lxc.cgroup.devices.allow = c 1:8 rwm\n    lxc.cgroup.devices.allow = c 136:* rwm\n    lxc.cgroup.devices.allow = c 5:2 rwm\n    # rtc\n    lxc.cgroup.devices.allow = c 254:0 rwm\n    #fuse\n    lxc.cgroup.devices.allow = c 10:229 rwm\n    #tun\n    lxc.cgroup.devices.allow = c 10:200 rwm\n    #full\n    lxc.cgroup.devices.allow = c 1:7 rwm\n    #hpet\n    lxc.cgroup.devices.allow = c 10:228 rwm\n    #kvm\n    lxc.cgroup.devices.allow = c 10:232 rwm\n    END\n    \n    # cat \u003e fstab  \u003c\u003c END\n    proc            /var/lib/lxc/vm0/rootfs/proc         proc    nodev,noexec,nosuid 0 0\n    sysfs           /var/lib/lxc/vm0/rootfs/sys          sysfs defaults  0 0\n    END\n    \n\n#### 起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.out -d\n    # lxc-console -n vm0\n    \n    OpenSSH がなければ入れておく\n    # yum install openssh-server\n    # service sshd start\n    \n\n## 動作確認 (Percona XtraDB Cluster の稼働確認)\n\n動作確認として Percona XtraDB Cluster を動かしてみる。\n\nすでにこれまでの作業を通して vm0 としてCentOS 6がインストール済みとする。\n\n### ホスト側設定\n\n  * 構成 \n    * ホスト, IPアドレス 10.0.3.1\n    * 仮想0 vm0, IPアドレス 10.0.3.2\n    * 仮想1 vm1, IPアドレス 10.0.3.3\n    * 仮想2 vm2, IPアドレス 10.0.3.4\n\n各仮想環境に簡単にアクセスできるように hosts を設定しておく。ホスト側に設定しておけば、dnsmasq のおかげで仮想側でも名前が引けるようになる。\n\n    \n    # vim /etc/hosts\n        以下を追記\n        10.0.3.2 vm0\n        10.0.3.3 vm1\n        10.0.3.4 vm2\n    \n\n### コピー元(vm0) 設定\n\n    \n    # ssh vm0\n      ここからはvm0の中\n    \n      固定IPアドレスを設定\n    # vim /var/lib/lxc/vm1/rootfs/etc/sysconfig/network-scripts/ifcfg-eth0\n        DEVICE=eth0\n        ONBOOT=yes\n        BOOTPROTO=static\n        IPADDR=10.0.3.3\n        NETMASK=255.255.255.0\n        GATEWAY=10.0.3.1\n    \n      XtraDB Cluster インストール\n    # rpm -Uhv [http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm](http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm)\n    # rpm -Uhv [http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm](http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm)\n    # yum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client\n    # cat \u003e /etc/my.cnf \u003c\u003cEND\n    [mysqld]\n    binlog_format=ROW\n    wsrep_provider=/usr/lib64/libgalera_smm.so\n    wsrep_cluster_address=gcomm://\n    wsrep_slave_threads=2\n    wsrep_cluster_name=lxccluster\n    wsrep_sst_method=rsync\n    wsrep_node_name=node0\n    innodb_locks_unsafe_for_binlog=1\n    innodb_autoinc_lock_mode=2\n    END\n    \n    # poweroff\n    \n\n### コピー、起動\n\n    \n    # lxc-clone -n vm1 -o vm0\n      -n はこれから作る仮想環境の名前\n      -o はコピー元の仮想環境の名前\n    # lxc-clone -n vm1 -o vm0\n    # vim /var/lib/lxc/vm1/config\n      vm0をvm1に置換 (vm2ではvm2に置換)\n      IPアドレスを10.0.3.2 -\u003e 10.0.3.3 に変更 (vm2では 10.0.3.4に変更)\n    # vim /var/lib/lxc/vm1/rootfs/etc/my.cnf\n        wsrep_cluster_address=gcomm:// をwsrep_cluster_address=gcomm://10.0.3.2 に変更\n        wsrep_node_name=node0 を wsrep_node_name=node1 に変更 (vm2ではnode2に変更)\n    \n      同様にvm0からvm2のコピーを実施\n    \n\n3つの環境が完成したら起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.0.out -d\n    # lxc-start -n vm1 -l debug -o debug.1.out -d\n    # lxc-start -n vm2 -l debug -o debug.2.out -d\n    \n\n### 動作確認\n\nvm0 にログインして実行\n\n    \n    # mysql -u root\n      データベース、テーブル作成\n    mysql\u003e create database t;\n    mysql\u003e use t;\n    mysql\u003e create table sample (\n    id int not null primary key auto_increment,\n    value int\n    );\n    \n    データ投入\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    \n\nIDがスキップしながらインサートされることがわかる。引き続き、他の環境でもデータを入れてみる。\n\nvm1 にログインして実行\n\n    \n    mysql\u003e use t;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    |  9 |     1 |\n    | 12 |     1 |\n    | 15 |     1 |\n    +----+-------+\n    \n\n同様のことがvm2でも起きる。\n\nこれにより、XtraDB Cluster の以下の動作が確認出来た。\n\n  * すべてのサーバーで書き込みと参照がおこなえること\n  * オートインクリメントがバッティングしないように、値が自動的にオフセットをつけて挿入されること\n\n## メモ\n\n### 外部から仮想環境へ直接アクセスしたい場合\n\nたとえば、外部からポート10080でアクセスされたとき、仮想環境の 10.0.3.51 のポート 80 へ転送させたい場合は iptables\nで以下のような設定をする。\n\n    \n    # vim /etc/syscofig/iptables\n        -A POSTROUTING -s 10.0.3.0/24 -j MASQUERADE の下に以下を追加\n        -A PREROUTING -i eth0 -p tcp --dport 10080 -j DNAT --to-destination 10.0.3.51:80\n    # service iptables condrestart\n    # iptables -L -t nat # NATテーブルから設定追加を確認\n    \n\n### 新しい Ubuntu を入れたい場合\n\n元の手順だとlucid (10.04) がインストールされるが、たとえば oneiric (11.10) であれば以下でインストール可能。\n\n    \n    # cp -a /usr/share/debootstrap/scripts/lucid  /usr/share/debootstrap/scripts/oneiric\n        lucid は /usr/share/debootstrap/scripts/gutsy のシンボリックリンクで、他のリリースも同様。とにかくファ イル名が参照できるようにシンボリックリンクをコピーしておけばいい。\n    # lxc-create -t ubuntu -f lxc.conf -n vm0 -- --trim -r oneiric\n        lxc-create ではなく -r はテンプレートへの引数\n    \n\n### 他の OS もインストールしてみたい場合\n\n/usr/lib64/lxc/templates/ には lxc-busybox,lxc-debian,lxc-fedora,lxc-lenny,lxc-\nopensuse,lxc-sshd,lxc-ubuntu の テンプレートがある。これ以外の環境が必要であれば、「lxc guset\nOS名」とかで検索してみる。\n\n## 参考\n\n  * [http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc](http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc)\n  * [http://wiki.debian.org/LXC](http://wiki.debian.org/LXC)\n  * [https://help.ubuntu.com/12.04/serverguide/lxc.html](https://help.ubuntu.com/12.04/serverguide/lxc.html)\n  * [http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104](http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104)\n  * [http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6](http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6)\n  * [http://www.percona.com/doc/percona-xtradb-cluster/index.html](http://www.percona.com/doc/percona-xtradb-cluster/index.html)\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2011-12-31-mysql-sandbox-blackhole.markdown","path":"blog/2011/12/31/mysql-sandbox-blackhole","layout":"post","title":"MySQL::Sandboxで環境を作ってBLACKHOLEエンジンを試す","createdAt":"2011-12-30T15:00:00.000Z","kind":"article","comments":true,"tags":["mysql"],"content":"ちょっとMySQLのBLACKHOLEエンジン使って調べたいことがあったんだけど、\n[MySQL::Sandbox](http://mysqlsandbox.net/) 使うとレプリケーション環境が簡単に構築できて便利。\n\n\u003c!-- more --\u003e\n\n以下は、MySQL::SandboxのセットアップからBLACKHOLEエンジン使うところまでの記録。\n\n### 前提\n\n  * 確認した環境は Debian 6.0.3, perl v5.12.3\n  * cpanm \u0026 perlbrew インストール済み（`cpan`コマンドでも問題無いはず）\n  * MySQL 5.5 はサーバーを起動するためにlibaio1が必要なので、あらかじめパッケージをインストールしておく\n\n### 手順\n\n#### インストール\n\n`cpanm` でMySQL::Sandbox をインストール。\n\n    \n    $ cpanm MySQL::Sandbox\n    \n\nこれで make_sandbox などのコマンド群がインストールされる。\n\nデータファイルは以下のようなファイル構成を取る\n\n  * $HOME/opt/mysql 以下のサブディレクトリにサーバーを設置 (環境変数 SANDBOX_BINARY で変更可能)\n  * $HOME/sandboxes 以下のサブディレクトリにMySQLのデータや起動スクリプトを設置 (環境変数 SANDBOX_HOME で変更可能)\n\n#### サーバーセットアップ\n\nサーバーを起動してみる。\n\n    \n    # 使用するディレクトリ設定\n    $ export SANDBOX_BINARY=$HOME/opt/mysql\n    $ export SANDBOX_HOME=$HOME/opt/sandboxes\n    # ソース取得\n    $ cd SANDBOX_HOME\n    $ curl -L -o mysql-5.5.19-linux2.6-x86_64.tar.gz http://www-jp.mysql.com/get/Downloads/MySQL-5.5/mysql-5.5.19-linux2.6-x86_64.tar.gz/from/http://ftp.jaist.ac.jp/pub/mysql/\n    # Sandbox 作成\n    $ make_sandbox $SANDBOX_BINARY/mysql-5.5.19-linux2.6-x86_64.tar.gz\n    \n\nこれで$SANDBOX_BINARY/5.5.19 にサーバーが、また $SANDBOX_HOME/msb_5_5_19 以下にインスタンスが作成される。\n\nこの時点で起動出来ているはずなので、接続してみる。以下で mysql クライアントが起動するはず。\n\n    \n    $ $SANDBOX_HOME/msb_5_5_19/use\n    \n\n`use`以外には、`start`, `stop`, `status` などの名前から動作が推測できそうなコマンド群がある。$SANDBOX_HOMEには複数のサンドボックスを操作するコマンド `use_all`, `start_all`, `stop_all` などがある。\n\n    \n    $ make_replication_sandbox 5.5.19 \n    # 第二引数はtar.gzまでのパスでもいいが、一度展開されたらバージョン番号指定でもいい。make_sandboxも同様\n    \n\n以上で、 master, node1, node2 というサーバーが起動する。node1, node2 は masterを参照したレプリケーション構成となる。デフォルトでノードは2台だが、`--how_many_nodes` オプションで台数は変更可能。\n\n同様に[Circular recplication](http://dev.mysql.com/doc/refman/5.1/ja/replication-\ntopology-circular.html)(日本語訳だとなんになるんだろう)も簡単に作れる。マスター/マスターレプリケーションはCircular replication が2台のみの構成だった場合に同じ。\n\n    \n    $ make_replication_sandbox --circular=4 5.5.19\n    \n\nこれで node1 -\u003e node2, node2 -\u003e nod3, node3 -\u003e node4, node4 -\u003e node1 という循環関係のレプリケーションが作れる。\n\n使い終わったら止めておこう。\n\n    \n    $ $SANDBOX_HOME/stop_all\n    \n\n### BLACKHOLEエンジンを試す\n\n準備ができたので、[BLACKHOLEエンジン](http://dev.mysql.com/doc/refman/5.1/ja/blackhole-storage-engine.html)を使ってみる。BLACKHOLEエンジンはバイナリログは記録するが、データは残さないストレージエンジンのこと。\n\nまずは、サンドボックス作成\n\n$ make_replication_sandbox --circular=3 5.5.19\n\nデフォルトストレージエンジンをnode1, node3はInnoDB、node2 はBLACKHOLEに変更\n\n    \n    $ echo default_storage_engine=InnoDB \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node1/my.sandbox.cnf\n    $ echo default_storage_engine=BLACKHOLE \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node2/my.sandbox.cnf\n    $ echo default_storage_engine=InnoDB \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node3/my.sandbox.cnf\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/restart_all\n    \n\nnode2, node3 のレプリケーションだけ再開して、node1 -\u003e node2 -\u003e node3 の2階層スレーブにする。\n\n    \n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node2/use -e start slave\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node3/use -e start slave\n    \n\n実験のための構成が完成した。ここで、node1 にデータを流し込んだとき、node2 にはデータが残らなくて、node3 にデータが出来たら成功。\n\nサンプルデータには、[Sample database with test suite](https://launchpad.net/test-\ndb/)を利用する。\n\n    \n    $ curl -LO [http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2](http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2)\n    $ tar xf employees_db-full-1.0.6.tar.bz2\n    $ cd employees_db\n    # InnoDB が決めうちで設定されているので、コメントアウト\n    $ sed -i -re s/^\\s+(set storage_engine = InnoDB;)/-- \\1/ *.sql\n    # 取り込み\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node1/use  -t \u003c ./employees.sql\n    \n\n結果\n\n    \n    $ cd $SANDBOX_HOME\n    $ du -sh rcsandbox_5_5_19/node?/data/ibdata1\n    219M    rcsandbox_5_5_19/node1/data/ibdata1\n    18M     rcsandbox_5_5_19/node2/data/ibdata1\n    219M    rcsandbox_5_5_19/node3/data/ibdata1\n    \n\nnode2 だけデータファイルがふくらまない。\n\n    \n    $ ls -l rcsandbox_5_5_19/node2/data/\n    合計 357652\n    drwx------ 2 f440 f440      4096 2011-12-31 17:15 employees/\n    -rw-rw---- 1 f440 f440   5242880 2011-12-31 17:14 ib_logfile0\n    -rw-rw---- 1 f440 f440   5242880 2011-12-31 17:11 ib_logfile1\n    -rw-rw---- 1 f440 f440  18874368 2011-12-31 17:14 ibdata1\n    -rw-rw---- 1 f440 f440        88 2011-12-31 17:16 master.info\n    -rw-rw---- 1 f440 f440      5925 2011-12-31 17:14 msandbox.err\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 mysql/\n    -rw-rw---- 1 f440 f440      6273 2011-12-31 17:14 mysql-bin.000001\n    -rw-rw---- 1 f440 f440 168403385 2011-12-31 17:16 mysql-bin.000002\n    -rw-rw---- 1 f440 f440        38 2011-12-31 17:14 mysql-bin.index\n    -rw-rw---- 1 f440 f440       315 2011-12-31 17:14 mysql_sandbox15902-relay-bin.000005\n    -rw-rw---- 1 f440 f440 168399499 2011-12-31 17:16 mysql_sandbox15902-relay-bin.000006\n    -rw-rw---- 1 f440 f440        76 2011-12-31 17:14 mysql_sandbox15902-relay-bin.index\n    -rw-rw---- 1 f440 f440         5 2011-12-31 17:14 mysql_sandbox15902.pid\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 performance_schema/\n    -rw-rw---- 1 f440 f440        75 2011-12-31 17:16 relay-log.info\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 test/\n    \n\nnode2 のバイナリログ、リレーログはちゃんと出来ているので、BLACKHOLEエンジンの適用を確認できた。\n\n### メモ\n\n  * `$SANDBOX_HOME/clear_all` でデータ消せるの便利。\n  * MySQL::Sandbox 3.0.19 からは、[Percona や Maria DB などの派生DBも扱える](http://mysqlsandbox.net/news.html)みたい。いろいろなバージョンで試すことが多いのでうれしい。\n\n"}]},"__N_SSG":true},"page":"/blog/categories/[tag]","query":{"tag":"mysql"},"buildId":"jhtzWSA_9WoQI8qF29M1i","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>