<?xml version="1.0" encoding="UTF-8"?>
  <feed xmlns="http://www.w3.org/2005/Atom" xml:base="https://apatheia.info/">
    <id>https://apatheia.info/</id>
    <title>apatheia.info</title>
    <updated>2016-10-29T15:51:00.000Z</updated>
    <link rel="self" href="https://apatheia.info/atom.xml"/>
    <author>
      <name>f440</name>
      <uri>https://apatheia.info/</uri>
    </author>
    
<entry>
  <id>tag:apatheia.info,2016-10-30:/blog/2016/10/30/isucon6/</id>
  <title type="text">ISUCON 6</title>
  <published>2016-10-29T15:51:00.000Z</published>
  <updated>2016-10-29T15:51:00.000Z</updated>
  <link href="https://apatheia.info/blog/2016/10/30/isucon6/"/>
  <content type="html">
  <![CDATA[<p>今年も<a href="http://isucon.net/">ISUCON</a>に参加した。</p>
<!-- more -->
<p>例年に比べて十分な予習・復習をすることができず、メンバー内の得手・不得手を十分に理解できない状態だったので、不安を残したままコンテストを迎えることとなった。</p>
<h2>予選</h2>
<p>最初にAzure Resource Managerテンプレートで環境を作るのだが、プロビジョニングに失敗したり、作業ミス（AppArmorを削除したら一緒にMySQLが消された）などで、結局3回くらい環境の構築をしなおしてだいぶ焦ったりした。</p>
<p>問題の内容としてはいわばはてなキーワードで、記事の投稿により別の記事内のリンクが増えるといったものだった。</p>
<p>参考実装ではオンデマンドにリンクを計算してページをレンダリングするのだが、これでは遅いので事前に計算結果をキャッシュする方針とした。ただ、ページが投稿されるごとに過去の記事も再生成する必要があり、これについては影響を受けるページだけを絞り込んでキャッシュを破棄することで対応しようとした。</p>
<p>方針が決まり、試しに雑なキャッシュをアプリに組み込んで見るとベンチマークを流すと毎回同じ箇所でベンチマークが失敗判定される。どうやらベンチマークは特定のページでリンクを確認しており、それ以外については問題があっても完走できる模様である。このことに気づいてからはベンチマークの挙動に合わせてキャッシュの再生成箇所を絞り込むなどを行った。</p>
<p>これだけでかなりスコアは伸びるようになったのだが、あとは地道にDBからRedisへの切り替えなどをしていった。</p>
<p>運営側と多少の行き違いがあり、スコアが計算されない事態が発生して肝を冷やしたが、最終的には対応してもらいなんとか本戦にたどり着くことができた。</p>
<h2>本戦</h2>
<p>問題としてはリアルタイム性のあるお絵かきアプリという非常にこったものであった。利用されている技術としては Docker / React / SSE (Server-sent events) といったところで、慣れていないこともあり初動が遅くなってしまった。</p>
<p>画像の表示部分で詰まっているような状態だったので、このあたりをなんとかしようという話ができはじめたのが昼から夕方にかけてで、いろいろ手を出しては失敗しを繰り返しているうちにタイムアップになってしまった。</p>
<h2>コンテストを終えて</h2>
<p>今年も非常に楽しく行えた。やり応えのある問題、快適な会場の提供、その他もろもろ運営の方々には頭が下がるばかりである。</p>
<p>本戦の結果についてはとにかく悔しい。この記事を書いている時点で一週間が経ったが、未だに後悔が残るばかりである。ただ、自分の未熟さを痛感させてくれるという意味でとても有意義であったと思う。今後も参加したいので、末永く続いてくれることを願う。</p>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2016-02-22:/blog/2016/02/22/install-dlite/</id>
  <title type="text">DLiteでOS X上にDockerの環境を構築する</title>
  <published>2016-02-22T11:54:00.000Z</published>
  <updated>2016-05-08T10:12:00.000Z</updated>
  <link href="https://apatheia.info/blog/2016/02/22/install-dlite/"/>
  <content type="html">
  <![CDATA[<p><a href="https://github.com/nlf/dlite">DLite</a> をインストールしたので、そのときのメモ。</p>
<!-- more -->
<h2>DLiteとは</h2>
<p>OS XでDockerを使えるようにやつ。内部では<a href="https://github.com/mist64/xhyve">xhyve</a>を使っていて非常にコンパクト。</p>
<h2>作業環境</h2>
<ul>
<li>DLite 1.1.3</li>
<li>OS X El capitan 10.11.3</li>
</ul>
<h2>インストール手順</h2>
<pre><code>brew install dlite
sudo dlite install # CPUやディスクサイズなどのオプションは`-h`で確認可能
</code></pre>
<p>おしまい。</p>
<p>内部では以下が行われる:</p>
<ul>
<li><code>/etc/sudoers</code>に<code>dlite,nfs</code>コマンドをパスワードなしで<code>sudo</code>できるようにする設定を追加</li>
<li><code>~/Library/LaunchAgents/local.dlite.plist</code>に起動設定を配置</li>
<li><code>~/.dlite</code>に起動イメージをダウンロード</li>
</ul>
<h2>起動</h2>
<p>Tmux内で起動しようとするとエラーになるので、必ずTmux外でやること。</p>
<pre><code>dlite start
</code></pre>
<p>問題がなければ以下が加えられる:</p>
<ul>
<li><code>/var/run/docker.sock</code> にソケットファイルを作成</li>
<li><code>/etc/hosts</code> にdliteへの参照を追加 (デフォルトは <code>local.docker</code>。インストール時のオプションで変更可能)</li>
<li><code>/etc/exports</code> にDLite側のホストへのNFSマウントする設定を追加</li>
<li><code>~/Library/LaunchAgents/local.dlite.plist</code> をロードし、自動起動するように設定</li>
</ul>
<p>うまくいっていれば、<code>docker -H unix://var/run/docker.sock</code>(<code>export DOCKER_HOST=unix:///var/run/docker.sock</code>で指定も可)でdockerが使えるようになる。</p>
<p>もしうまくいかないようなら、<code>sudo dlite daemon</code>でコマンドラインから実行して原因を突き止める。とくに、NFS周りのコンフリクトが起きていないかを確認。</p>
<h2>まとめ</h2>
<p>ホストと仮想環境の間がシームレスにつながってcoLinuxっぽさがある。こういうすっきりしたツールは楽しい。</p>
<h2>追記</h2>
<p>2016-05-08: 現在はDocker for Macを利用している。osxfsでファイルシステムのイベントが連携できて便利。</p>
<h2>参考</h2>
<ul>
<li><a href="https://github.com/nlf/dlite">GitHub - nlf/dlite: The simplest way to use Docker on OS X</a></li>
<li><a href="https://blog.andyet.com/2016/01/25/easy-docker-on-osx/">Simplifying Docker on OS X</a></li>
</ul>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-11-11:/blog/2013/11/11/isucon3/</id>
  <title type="text">ISUCON 3 の参加記録</title>
  <published>2013-11-11T00:20:00.000Z</published>
  <updated>2013-11-11T00:20:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/11/11/isucon3/"/>
  <content type="html">
  <![CDATA[<p>Web アプリケーションのパフォーマンスコンテスト <a href="http://isucon.net/">ISUCON 3</a> に参加し、2 位の成績となった。どのような状態で当日を迎え、どのような作業を行ったのかをまとめる。</p>
<!-- more -->
<p>私自身はこれで三度目の ISUCON 参加となるが、今回チームを組むメンバーはみんな初めての参加ということもあり、事前の打ち合わせでは以下のようなことを話していた:</p>
<ul>
<li>これまでの大会の説明と典型的なアプローチ
<ul>
<li>同時に、過去にとられた戦法は参考程度であること、あくまで現物のアプリケーションを元に戦略を立てるべきでアプリケーションやミドルウェアを事前に決めることは危険であるということは強調</li>
</ul>
</li>
<li>よく使われるミドルウェアの概要、メリット/デメリット
<ul>
<li>主要機能のほか、キャッシュ部分の永続性の有無（単純に考えればメモリだけで処理してファイルに書き出さない方が早いが、ベンチマークをまたいでキャッシュを引き継げれば切り札になり得る）、キャッシュの時間単位(ベンチマークがフェイルしない限り極力キャッシュしたいという要望を考えると、ミリ秒単位で制御したくなることが多い) などのコンテストで重要となりそうな部分について解説</li>
</ul>
</li>
<li>今後問題として取り上げられそうなWebアプリケーションの構成要素
<ul>
<li>毎回趣向を凝らした題材が提供されるので完全に予想しきることは不可能だが、Web プログラミングで頻出する要素（予選で言えば「セッション」「ページング」のような単位）を挙げ、他にもどのようなものが考えられるか、どうやって実装できてどのような高速化が行えるかを話し合い</li>
</ul>
</li>
</ul>
<p>予選までに数回休日に集まっては過去の問題を解いたり、自分たちで考えた予想アプリに対してチューニングをしてみたり、もくもく自分なりのトレーニングをしたりする会を開いていた。</p>
<p>また、個人的には以下のようなことを行ってきた:</p>
<ul>
<li>よく目にしているWebアプリケーションの解析
<ul>
<li>普段 Web を閲覧しているときでも、どのようなデータ構造が想定されそれをどうやって KVS に乗せるか、画面の構成要素のうちリアルタイムで作る必要がある場所はどこでキャッシュ可能な場所はどこか、といったことを考えるようにする</li>
</ul>
</li>
<li>速度に対する感覚を養うため、アプリケーションやミドルウェアのチューニングとその際の効果測定
<ul>
<li>自分の日頃扱っているWebアプリケーションではネットワークが飽和するようなアクセスはこないため、限界性能を引き出すような使い方はしていないことが多い。「コンテンツキャッシュでさばいてアプリに届かせないのが定石っス！Nginx とか Varnish っていうのがApacheよりいいらしいっス！」みたいなレベルではなく、アプリケーションと Nginx ではどれくらいの性能差が生まれるのか、Nginx を使うのであれば <a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html">proxy_cache</a> と比べて Unix Domain Socket でMemcached に接続した <a href="https://github.com/agentzh/srcache-nginx-module">SRCache</a> ではどれくらい単位時間あたりの処理数が変わるのか、別サーバーにある Memcached と TCP 通信した場合や Redis に変えたときではどのように変わるのか、というようなことを考えられるようにしておく<br>
ベンチマーク上の最速を求めるということではなく、どういった選択肢が考えられ、それぞれにどのような性能と機能のトレードオフが発生するのかを体にしみつけておく</li>
</ul>
</li>
</ul>
<p>どんなアプリケーションがくるかわからない以上、不安がぬぐいさることはできない。この時点でできることは「当日できるだけ早くウィークポイントを見つけること」、「見つけたウィークポイントに対しての効果的な対応をとれるための引き出しを増やしておくこと」であり、そのための準備をひたすら積み重ねてきた。</p>
<h2>予選</h2>
<p>予選問題はチームメンバーの勤め先の会議室を借りて取り組んだ。何度も事前打ち合わせで利用していたので、ストレス無く作業に打ち込めた。ただ、興奮しすぎて当日 2 時間くらいしか眠れていない状態だったため、テンションが高いのに頭があんまり働かなくてだいぶつらかった。</p>
<p>題材はコードスニペット投稿アプリケーション、いわゆる Nopaste や Pastebin と呼ばれるたぐいで <a href="https://gist.github.com/">Gist</a> を想像してもらうとわかりやすいと思う。ログインがありセッションが発生すること、公開/プライベートのフラグがあることがこれまでの題材との大きな違いであったが、ある程度予想の範囲内であったことと後述するとおりベンチマークの抜け道に気づけたので、すぐにスコアを伸ばすことができた。</p>
<p>主な施策は以下の通り:</p>
<ul>
<li>インフラ面
<ul>
<li>Varnish の配置
<ul>
<li>ページキャッシュ。Cookie の値を参照し、ユーザー別にキャッシュを管理することでログイン中のユーザーもキャッシュ対象となるよう設定</li>
</ul>
</li>
</ul>
</li>
<li>アプリ面
<ul>
<li>markdown -> html の事前変換
<ul>
<li>全件 markdown -> html 化したテーブルの dump データを用意しておき、DBクリア後にリストアしようとした<br>
リストア時間が時間が40秒くらいかかり初期化制限時間60秒をだいぶ圧迫すること(*)、途中から初期データはほぼページキャッシュでさばけるようになっていたので、ここに力をかけるのは無駄だと判定して採用しなかった
<ul>
<li>(*) init の間に /var/lib/mysql をまるごと差し替えるなり別のDBにアプリを接続するなりで回避できるので、これ自体はぬるい判断だったと思う</li>
</ul>
</li>
</ul>
</li>
<li>更新のないテーブルのインメモリ化や Cookie 内へユーザー情報を登録などで DB 参照回数の低減</li>
<li>（チームメンバー担当）クエリの組み方やインデックスの追加などのRDB最適化
<ul>
<li>今回のアプリの肝となる部分であり、ひたすら調整を行ってもらっていた</li>
</ul>
</li>
<li>（チームメンバー担当）Memcached の参照方法を TCP から Unix Domain Socket に変更
<ul>
<li>初期状態では MySQL Memcached Plugin を参照しているので成績が伸びなかったが、こによりトラップを回避してくれた</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>事後の講評などでも抜け穴があったという話があったが、後日公開された<a href="http://isucon.net/archives/32971265.html">予選のAMI</a>を利用しても以下のような方法で簡単に確認できる。</p>
<ol>
<li>アプリを  Perl から Ruby に変更 (※ これは単純に自分がRuby に慣れているからで、他の言語では確認していない)</li>
<li>Memcached への接続先を MySQL Memcached plugin (ポート 11211) から本当の Memcached (ポート 11212)に変更しアプリケーション起動</li>
<li>リバースプロキシの Apache を停止</li>
<li>Varnish をインストール</li>
<li>以下の設定をして Varnish を起動</li>
</ol>
<p>/etc/varnish/default.vcl</p>
<pre><code>backend default { .host = "127.0.0.1"; .port = "5000"; }

sub vcl_recv {
  # POST リクエストやCookieがある(ログイン中)の場合は直接アプリを参照
  if (req.request != "GET" || req.http.Cookie) {
      return (pass);
  }
  return (lookup); # それ以外はキャッシュを探す
}

sub vcl_hash {
    hash_data(req.url);
    hash_data(req.http.host);
    return (hash);
}

sub vcl_fetch {
    set beresp.ttl = 1d; # キャッシュ期間は1日に設定。数字は適当で、とにかく長くしておけばいい
    return (deliver);
}
</code></pre>
<p>上記設定で初回 workload=1 で 9000 くらい、2 回目に workload=2 で24000くらいでベンチマークが Fail せずに完走することがわかる。Varnish じゃなくて他のコンテンツキャッシュを利用しても同様。</p>
<p>予選開始直後、Sinatra のアクション単位でリクエスト回数、平均実行時間、ワーストケースの実行時間などの情報がそろえ、いざ Varnish のチューニングを行いはじめたところ、どんなにキャッシュの TTL を伸ばしてもベンチマークが通ってしまうことに気づき思わず吹き出してしまった。</p>
<p>キャッシュで簡単にスコアが伸ばせることに気づいたのがたしかお昼前で、このあと夕方くらいまで 1 位だったと思う。その後トップは手放すこととなったが、最終的に<a href="http://isucon.net/archives/32951235.html" title="ISUCON 本戦出場者決定のお知らせ : ISUCON公式Blog">予選5位</a>で決勝進出を決めることができた。</p>
<h3>予選後</h3>
<p>どうにも簡単に勝てすぎたので本当の自分の実力がどの程度かの不安が残るかたちとなったが、穴を見つけられたのも実力のうちと考えて本戦へ向けての準備を進める。</p>
<p>予選が割とシンプルで簡単にキャッシュで返せるような作りだったので、本戦はよりアプリケーションよりの対応が求められるはず、そのためにはいままで以上にアプリの状況を把握する能力を高めておく必要がある。*stat 系のパフォーマンス調査ソフトに慣れておいたり、ログ解析のやり方探してみたり（<a href="http://goaccess.prosoftcorp.com/">GoAccess</a> 使ったりだとか）、プロファイリング用のソフトウェアを把握しておいたり、調査用<a href="https://gist.github.com/f440/7395268">アクセスカウンタ</a> を準備したり、とにかくすぐに分析できるようにしておいた。</p>
<p>また、予選の際は自分もアプリを改修し、アプリメインのメンバーもインフラ面を考慮してくれていたので、本戦で複数台構成になることにより作業がバッティングすることを懸念された。これについては事前にチーム内でそれぞれにメインとなる作業をお願いして競合しないよう取りはからった。</p>
<h2>本戦</h2>
<p>予選での経験を踏まえ、前日早めに就寝したお陰で当日は体調的には万全、先着で利用可能なミーティングスペースも確保でき順調な滑り出しだった。</p>
<p>題材は画像投稿版の Twitter で、アップロードした画像ファイルがパブリック/フォロワーのみ/プライベートといった公開範囲に応じて閲覧可能となり、関係するメンバーの投稿が画面に自動反映される SPA (Single Page Application) だった。「ファイルアップロード」「フォロワーのアクティビティがタイムラインに表示」などで、チーム内の感想としては「やっぱりきましたね」、といった感じだったのだが、なによりアプリケーションの出来がよくてびっくりした。</p>
<p>作業用サーバーとしては、アプリケーションが稼働している 1 台と自由に使える 4 台の計 5 台が与えられた。これまでの ISUCON では CentOS 5 系が使われており、データホテルの Web サイトでもホスティングしている VPS の OS CentOS 5.8 となっているので CentOS 5 系が来る可能性も考慮していたのだが、今回は CentOS 6.4 であった。</p>
<p>サーバー受け取り後、バックアップをとりつつアプリケーションのプロファイリングを進めるが、どう見ても画像変換のコストだけが突出していることがわかる。データベースへのアクセスはきわめて短時間で終わっており、ボトルネックではない以上手をかけるのは無駄だと判断した。</p>
<p>状況確認を踏まえ、以下のような作業を行っていった:</p>
<ul>
<li>事前に初期データのアイコンおよび投稿画像はリサイズをかけておく
<ul>
<li>バックアップとしてローカルに全ファイルを転送していたので、そのファイルを変換して本番環境に書き戻し
<ul>
<li>うっかりリサイズのサイズ間違えたり、そんなに早くない自分のマシン(Macbook Air)で実行していたので、かなりの時間がかかってしまった</li>
</ul>
</li>
</ul>
</li>
<li>画像のうち、誰でも閲覧可能なファイルは Web サーバーから直接配信できるよう、公開領域にシンボリックリンクを作成
<ul>
<li>全ユーザーと全ファイルのアクセス権限を組み合わせてリンクを作成し、アクセス権限をDBに問い合わせずにレスポンスを返すというのもアイデアとしてはあったのだが、時間がかかるのでこのようにした</li>
</ul>
</li>
<li>（チームメンバー担当）画像変換時にファイルシステムへファイルを保存。また画像表示時に変換済みファイルがないかのチェックを追加</li>
<li>（チームメンバー担当）ファイルシステムに保存したタイミングで、リサイズを非同期処理で実施
<ul>
<li>負荷が高くなってくるとこの部分が詰まってしまい、変換待ちが大量にたまるのであまり効率的ではなく、不採用</li>
</ul>
</li>
</ul>
<p>最終的に以下の構成で計測を迎えた。フロント 4 台に Nginx を配置のうえ <code>try_files</code> でローカルにファイルがあるかどうかをチェックし、なければバックエンドサーバーに処理を委譲するようになっている。</p>
<pre><code>                +----------+        +----------------+
  Benchmark ->  | Web x 4  |  ----  | Web + App + DB |
                +----------+        +----------------+
</code></pre>
<p>時間がなくてこのようなかたちとなったが、今考えてもこれがよかったかどうかでいえばまったくもってよくなかった。完全にバックエンドのアプリサーバーが負荷でつぶれていたが、全サーバーでアプリを動かせばきれいに台数分スケールしていたはずだったのに、本当に悔やまれる。</p>
<p>タイムアップ時点ではそれほどの成績ではなかったが、本戦のベンチマークを無事乗り越え、気づいたら 2 位という成績で ISUCON 3 の幕を閉じた。</p>
<h2>ふりかえって</h2>
<p>正攻法ではない方法もいろいろ考えてはいたんだけど、結局のところ他のチームと比べても過激な改修は行わずに済ませた。直しやすそうなところ、目のつきやすいところではなく、手早く確実にウィークポイントを直すというのが目標だったので、それは実現できたんだと思う。</p>
<p>けどやっぱり優勝できなかったのは心残りで、試合終了後や帰宅後のチャット上でもチームメンバーと「非同期のワーカー作る時間の無駄だった」とか「どう考えてもアプリサーバーネックだったし、他の4台でもアプリ動かしてベンチマークのワーカー増やせばトップとれたんじゃないの」とか「Macbook Air でちまちま画像変換するの失敗だった」とか、いろいろ話をしていた。1位があまりに鮮烈で、それ以外の順位は空気みたいな存在だし、やはりこの世はトップ総取りなのだなぁ、と痛感した。</p>
<p>毎回このような場を用意していただいている LINE 社、データホテル社の方々には感謝の限りです。お弁当おいしゅうございました。出題のカヤックの皆さんもすばらしい問題をありがとうございました。</p>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-08-10:/blog/2013/08/10/immutable-infrastructure/</id>
  <title type="text">Immutable Infrastracture について</title>
  <published>2013-08-10T12:00:00.000Z</published>
  <updated>2013-08-10T12:00:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/08/10/immutable-infrastructure/"/>
  <content type="html">
  <![CDATA[<p>ここ最近話題に上がることが多い Immutable Infrastracture と、その他仮想環境周りについての雑感。</p>
<!-- more -->
<p>Immutable Server や Immutable Infrastracture っていう単語がいろんなところで目に入るようになった。とくに Chad Fowler が<a href="http://chadfowler.com/blog/2013/06/23/immutable-deployments/">ブログで取り上げたり</a>、<a href="http://foodfightshow.org/2013/07/immutable-infrastructure.html">Food Fight に出たり</a> して、世間でも関心が高まった感じがある。</p>
<p>プログラムを書く人にはご存じの通り、この Immutable っていうのは状態が変更出来ないことを指している。Immutable な Infrastracture っていうのは、ざっくり言うと「運用中のサーバーに変更を加えない」っていうアプローチでサーバーを管理しているスタイルのこと。</p>
<p>(ファイルシステムを読み取り専用にする、とかそういう話じゃなくて、あくまでそういう方針でやろうっていう話)</p>
<p>サーバーの設定を変更したくなったら、その変更を加えた新しいサーバーを用意する。アプリケーションをデプロイしたくなったら、新しくサーバーを立ち上げてそちらにデプロイを行う。稼働中のサーバーに SSH でログインして設定を変更するようなことはせず、なにかしらの変更のためにはつねにサーバーを追加していく。サービスを新サーバー群にで行うように DNS を切り替えたあと、参照されなくなったサーバーは破棄する(まるでガベージコレクションみたいだね)。</p>
<h2>どうやって Immutable Infrastracture を実現するのか</h2>
<p>Immutable Infrastracture には、いわゆる <a href="http://martinfowler.com/bliki/BlueGreenDeployment.html">Blue Green Deployment</a> で知られているテクニックを用いる。</p>
<p>現在、プラットフォームとして上手に Immutable Infrastracture を実現できているのは <a href="http://aws.amazon.com/jp/elasticbeanstalk/">AWS Elastic Beanstalk</a> だと思っているので、これを例に説明する。Elastic Beanstalk を「あー、AWS がやってる heroku 的なアレだろ」くらいの認識しかなければ、一度ちゃんと調べてみたほうがいい。</p>
<p>Elastic Beanstalk では、ロードバランサーとそこにぶら下がるサーバー群 (オートスケールするので、台数は伸び縮みする) が「環境(Envirnment)」というくくりで管理される。</p>
<p>サーバーには Amazon が用意したマシンイメージを使うこともできるし、カスタマイズしたイメージを利用することも可能になっている。アプリケーションのデプロイは git push だったり、Java の war ファイルアップロードでできるので、サーバーにログインする必要はない。</p>
<p>「環境」にはそれぞれ URL が割り振られるのだが、これは環境間ですげ替えることができる。つまり、検証環境でアプリをデプロイしたりミドルウェアの設定変更をして、確認がとれたら本番環境と入れ替えたり、問題が起きたらすぐに元に戻したりといったことがダウンタイムなく行える。</p>
<p>Netflix のデプロイツール <a href="https://github.com/Netflix/asgard">asgard</a> も同じようなことをしているし、Heroku の <a href="https://devcenter.heroku.com/articles/labs-preboot/">preboot</a> も内部では同じようなことやってるんじゃないかな。(追記: Heroku の Preboot だけど、説明ページへのリンクがなくなっている。今は <a href="https://devcenter.heroku.com/articles/labs-pipelines">Pipelines</a> 使えってことかも)</p>
<p>もちろん、オンプレミスに自前の環境でこういったことを行うことも可能だろうけど、アプリケーションの切り替えに DNS の設定変更なり浮動 IP アドレスの付け替えなりが必要となってくるので、かなり面倒くさい。すでにシステムとして提供されているものを利用できるのであれば、それを使うのが現実的だとは思う。</p>
<h2>構成管理ツールの役割</h2>
<p>設定を変更するためだけに新しいマシンを作るだなんて、なんでそんなことをするのだろうと Chef や Puppet などのツールを使って変更管理している人たちは不思議に思うかもしれない。発想を逆にしてみると、仮想マシンが状態を持っているから冪等性だとか自己修復性を考慮したセットアップツールが必要になる。仮想マシンが不変だという前提にたてば、こういった処理が省けるようになるのかもしれない。</p>
<p>とはいえ、Immutable Infrastracture を実践したとしても構成管理ツールは以下のような局面で今後も使われていくことになるだろうと思う。</p>
<ul>
<li>ベースとなる仮想マシンのセットアップのため</li>
<li>初回起動後の仮想マシンに対して、(仮想マシンに組み込めない or 組み込みたくない) マシンごとの変更を設定するため</li>
</ul>
<p>個人的には、今の Chef や Puppet みたいなサーバー/クライアント構成は必要なくて、もっとライトウェイトなもので十分な気がしている。</p>
<h2>仮想マシンの役割</h2>
<p>昨今の構成管理ツールブームで、サーバーセットアップの技術が成熟してきた。こういったツールをソフトウェアでいうところの autotools や ant といったビルドツールにたとえるなら、次の興味は apt や rpm といったパッケージにあたるもの、つまりセットアップ済み仮想マシンになるかと思う。</p>
<ul>
<li>Packer は仮想マシンの作成手順や作成先を抽象化しようとしている</li>
<li>Docker は仮想マシンをまるで Github から clone するかのように共有する方法を提供している</li>
<li>AWS MarketPlace は個人/企業に仮想マシンを売り買いできる仕組みを提供している</li>
</ul>
<p>最近のプロダクトやサービスを考えてみても、仮想マシン自体の取り扱いにだんだん関心がむかっているのは確かっぽい。Immutable Infrastracture もまた、仮想マシンを仮想マシンらしく扱った運用形態といえるんじゃないかな。</p>
<h2>参考</h2>
<ul>
<li><a href="http://martinfowler.com/bliki/ImmutableServer.html">ImmutableServer</a></li>
<li><a href="http://www.thoughtworks-studios.com/blog/rethinking-building-cloud-part-4-immutable-servers">Rethinking building on the cloud: Part 4: Immutable Servers</a></li>
<li><a href="http://chadfowler.com/blog/2013/06/23/immutable-deployments/">Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components - Chad Fowler</a></li>
</ul>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-08-10:/blog/2013/08/10/octopress-to-middleman/</id>
  <title type="text">ブログエンジンを Octopress から Middleman に変えた</title>
  <published>2013-08-10T11:30:00.000Z</published>
  <updated>2013-08-10T11:30:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/08/10/octopress-to-middleman/"/>
  <content type="html">
  <![CDATA[<p>このサイトは今まで <a href="http://octopress.org/">Octopress</a> を使って生成していたんだけど、<a href="http://middlemanapp.com/">Middleman</a> に変えてみた。</p>
<!-- more -->
<p>元々 Octopress の設定ファイルの書き方とかがモヤモヤするものがあって(Rakefile に設定項目埋め込んであるところとか) Jekyll にしようかなと思ったんだけど、なんとなく Middleman にしてみた。その後の感想など。</p>
<p>(もう2ヶ月くらい前の話なので、移行当時の記憶はおぼろげ)</p>
<ul>
<li>ビルド時間が短くなった
<ul>
<li>素の jekyll はビルド早いんだけど、Octopress は結構遅いんで気になっていた。middleman は Octopress よりかは早い</li>
</ul>
</li>
<li>ブログの内容はほぼそのまま使い回せた
<ul>
<li>メタデータ部分を s/categories/tags/ で置換したくらいだったと思う</li>
</ul>
</li>
<li>公開されているテンプレートが少ない
<ul>
<li>自分でちまちま書いてる</li>
</ul>
</li>
<li>middleman-livereload が便利
<ul>
<li><a href="https://github.com/johnbintz/rack-livereload">rack-livereload</a> を使っている。ブラウザに拡張を入れなくても、Web Socket でリロードしてくれる。</li>
</ul>
</li>
<li><a href="https://github.com/rtomayko/tilt">tilt</a> を使っているので、テンプレートエンジンは自由に選択できる
<ul>
<li>楽しくなって、<a href="http://slim-lang.com/">Slim</a> を使って <a href="https://github.com/shower/shower">shower</a> を <a href="https://github.com/f440/middleman-miwer">移植してみたりした</a>。</li>
</ul>
</li>
</ul>
<p>おおむね満足です。</p>
<h2>参考</h2>
<ul>
<li><a href="http://octopress.org/">Octopress</a></li>
<li><a href="http://middlemanapp.com/">Middleman</a></li>
<li><a href="https://github.com/rtomayko/tilt">tilt</a></li>
</ul>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-06-17:/blog/2013/06/17/docker/</id>
  <title type="text">仮想環境構築に docker を使う</title>
  <published>2013-06-17T00:13:00.000Z</published>
  <updated>2013-06-17T00:13:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/06/17/docker/"/>
  <content type="html">
  <![CDATA[<p>ちょっと前から <a href="http://www.docker.io/">Docker</a> を使っているので、その話。</p>
<!-- more -->
<h2>Dockr について</h2>
<p><a href="http://www.docker.io/">Docker</a> は <a href="https://www.dotcloud.com/">dotcloud</a> がオープンソースで公開している、コンテナ技術による仮想化ソフトウェア。</p>
<p>以下のテクノロジーベースにしている:</p>
<ul>
<li><a href="http://lxc.sourceforge.net/">LXC</a>
<ul>
<li><a href="/blog/2012/05/13/vps-lxc-xtradb-cluster/">前にも書いた</a>。Xen とか VirtualBOX みたいにホスト内に仮想マシンを立ち上げるんじゃなくて、ホスト内の隔離された環境で仮想マシンを動かす技術。物理マシンをシミュレーションしているんじゃないってことは、VPS とか EC2 とかの仮想マシン上でも問題なく動くし、マシンを起動するプロセスが不要となるので、一瞬で使い始められるというメリットにつながっている。</li>
</ul>
</li>
<li><a href="http://aufs.sourceforge.net/">AUFS</a>
<ul>
<li>UnionFS(ディレクトリを重ね合わせることができる)の実装の一つ。元の仮想マシンイメージを書き換えないで、更新が発生した部分は別の場所に書き込んでいくようになっている。これにより、仮想マシンの立ち上げ時にイメージのコピーが発生しないので、すぐに使い始められる。</li>
</ul>
</li>
</ul>
<p>Docker を使う前は LXC のラッパーとして取っつきにくさを緩和してくれる、とかそういうレベルだと思ったんだけど、予想はよい方向に裏切られた。</p>
<p><a href="http://docs.docker.io/en/latest/commandline/command/images/">仮想マシンのイメージを可視化したもの</a>を見ると、まるで Git のコミットログみたいに見えると思う。実際、情報は差分で管理され、履歴を残したり分岐させたりといった操作が非常に軽量にできていて、Git を操作するかのように仮想マシンを操作できるようになっている。</p>
<h2>動かし方</h2>
<p>Arch Linux や Debian で動かしている人がいるみたいだけど、公式サポートは今のところ Ubuntu のみ。Ubuntu 12.04 LTS を使っているのであれば、<code>curl get.docker.io | sh -x</code> で動くようになる。</p>
<p>ちゃんとしたやり方は <a href="http://docs.docker.io/en/latest/installation/">ドキュメント</a>を見れば、特にはまることもないと思う。できるだけ新しい Ubuntu を使っておけばいい。</p>
<p>すぐに試してみたいんなら、Vagrant 経由で簡単に使い始められる。</p>
<pre><code>git clone https://github.com/dotcloud/docker.git
cd docker
vagrant up --provider virtualbox # or vagrant up --provider aws
</code></pre>
<h2>基礎的な操作方法</h2>
<p>インストールがうまくいって Docker が起動しているものとして、早速使ってみる。</p>
<pre><code>$ docker
Usage: docker [OPTIONS] COMMAND [arg...]
  -H="127.0.0.1:4243": Host:port to bind/connect to

  A self-sufficient runtime for linux containers.

  Commands:
  attach    Attach to a running container
  build     Build a container from a Dockerfile
  commit    Create a new image from a container's changes
(以下省略)
</code></pre>
<p>コマンドがずらっと表示されるかと思う。まずは単発のコマンドをコンテナ内で実行してみる。</p>
<pre><code>$ docker run base /bin/echo hi
Pulling repository base from https://index.docker.io/v1
Pulling image b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc (latest) from base
Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc metadata
Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc fs layer
Downloading 10240/? (n/a)
Pulling 27cf784147099545 metadata
Pulling 27cf784147099545 fs layer
Downloading 94863360/? (n/a)
Pulling image 27cf784147099545 () from base
hi
</code></pre>
<p>「<code>docker</code> コマンドに run サブコマンドを指定して、<code>base</code> という仮想マシンで <code>/bin/echo hi</code> コマンドを実行する」という意味になる。仮想マシンがダウンロードされるが、これは初回実行時のみ。最後に表示された「hi」というのが今回の実行結果で、このコンテナの役割はこれで終わり。</p>
<p>今度は作ったマシンの中に入ってみるために、<code>-i</code> と <code>-t</code> オプションで入出力できるようにして <code>/bin/bash</code> を起動してみる。</p>
<pre><code>$ docker run -i -t base /bin/bash
root@bc43a290f0ce:/#
</code></pre>
<p>端末から抜けるとホスト側に制御が戻る。</p>
<pre><code>root@bc43a290f0ce:/# exit
exit
$
</code></pre>
<p>今度は <code>-d</code> オプションでコマンドを実行しっぱなしにする。</p>
<pre><code>$ docker run -i -t -d base /bin/ping -i 5 www.aikatsu.net
79365b2985c4
$
</code></pre>
<p>ID が返されて、すぐに端末が利用可能になる。稼働中のプロセスを確認してみる。</p>
<pre><code>$ docker ps
ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS
79365b2985c4        base:latest         /bin/ping -i 5 www.a   22 seconds ago      Up 21 seconds
</code></pre>
<p>次に実行中の出力をのぞいてみよう。</p>
<pre><code>$ docker logs 79365b2985c4
PING www.aikatsu.net (60.32.7.37) 56(84) bytes of data.
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=1 ttl=49 time=282 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=3 ttl=49 time=278 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=4 ttl=49 time=283 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=5 ttl=49 time=266 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=6 ttl=49 time=268 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=8 ttl=49 time=264 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=9 ttl=49 time=270 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=10 ttl=49 time=290 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=11 ttl=49 time=284 ms
</code></pre>
<p>順調に動き続けているようなので、このジョブにアタッチしてみる。</p>
<pre><code>$ docker attach 79365b2985c4
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=18 ttl=49 time=239 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=19 ttl=49 time=291 ms
64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=20 ttl=49 time=275 ms
(出力が続く)
</code></pre>
<p>アタッチ中の端末は <code>Ctrl-p Ctrl-q</code> でデタッチできる。(このとき use of closed network connection っていうエラーが出る場合 Ctrl-c で抜けるしかないっぽい。バグレポートは上がっているので、じきに直ると思う。)</p>
<p>最後に<code>kill</code>でこのプロセスを消してみる。</p>
<pre><code>$ docker kill 79365b2985c4
$ docker ps
$
</code></pre>
<p><code>ps</code>からプロセスが消えた。基礎的なコンテナの操作の説明は以上。</p>
<h2>詳細</h2>
<h3>コンテナ</h3>
<p>これまでコマンドを実行したり、<code>kill</code> されたコンテナはどうなっているのか。実は全部残っている。停止したコンテナを表示するために<code>-a</code>をつける。ついでに、情報を省略しないで表示するために<code>-notrunc</code> もつける。</p>
<pre><code>$ docker ps -a -notrunc
ID                                                                 IMAGE               COMMAND                          CREATED             STATUS              PORTS
79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2   base:latest         /bin/ping -i 5 www.aikatsu.net   14 minutes ago      Exit 0
bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503   base:latest         /bin/bash                        16 minutes ago      Exit 0
7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971   base:latest         /bin/echo hi                     16 minutes ago      Exit 0
8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb   base:latest         /bin/bash                        21 minutes ago      Exit 0
4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a   base:latest         /bin/echo hi                     21 minutes ago      Exit 0
</code></pre>
<p>正常終了しているので、すべて<code>Exit 0</code>になっている。また、ID は省略表記されていたこともわかる。コンテナの実体は <code>/var/lib/docker/containers/&#x3C;ID></code> 以下に格納されている。</p>
<pre><code>$ sudo ls /var/lib/docker/containers/
4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a
79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2
7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971
8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb
bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503
</code></pre>
<p>どんどんたまっていくから心配かもしれないけど、各コンテナはベースイメージからの差分しかもたないので、問題にならない。もし、消したくなったら <code>docker rm &#x3C;コンテナのID></code> で消せる。</p>
<p>作業領域であったコンテナを <code>commit</code> するとイメージとして使い回せるようになる。<code>ユーザー名/名称</code>にするのが作法っぽい。</p>
<pre><code>$ docker commit -m "My first container" 4637bc634170 f440/first_container
02036952e5dc
$ docker images
REPOSITORY             TAG                 ID                  CREATED
base                   latest              b750fe79269d        12 weeks ago
base                   ubuntu-quantl       b750fe79269d        12 weeks ago
base                   ubuntu-quantal      b750fe79269d        12 weeks ago
base                   ubuntu-12.10        b750fe79269d        12 weeks ago
f440/first_container   latest              02036952e5dc        3 seconds ago
</code></pre>
<p>これで今後は <code>docker run f440/first_container</code> をベースにしたコンテナを作れるようになる。</p>
<h3>イメージ</h3>
<p>もう一回イメージの一覧を内容を確認してみよう。</p>
<pre><code>$ docker images
REPOSITORY             TAG                 ID                  CREATED
f440/first-container   latest              141fef9a2f57        14 seconds ago
base                   latest              b750fe79269d        12 weeks ago
base                   ubuntu-12.10        b750fe79269d        12 weeks ago
base                   ubuntu-quantl       b750fe79269d        12 weeks ago
base                   ubuntu-quantal      b750fe79269d        12 weeks ago
</code></pre>
<p>base イメージは latest, ubuntu-quantl, ubuntu-quantal, ubuntu-12.10 といった複数のタグがついていることがわかる。イメージは複数の名称をタグ付けできるようになっており、<code>base:latest</code>, <code>base:ubuntu-12.10</code> といった形で異なるイメージを呼び出せるようになっている。省略時は <code>base:latest</code> と同じ。</p>
<p>pull してくるイメージは <a href="https://index.docker.io/">https://index.docker.io/</a> から情報を持ってくる。コマンドラインで検索したい場合は <code>search</code> コマンドを利用する。</p>
<pre><code>$ docker search centos
Found 4 results matching your query ("centos")
NAME                          DESCRIPTION
centos
backjlack/centos-6.4-x86_64
creack/centos
mbkan/lamp                    centos with ssh, LAMP, PHPMyAdmin(root pas...
</code></pre>
<p>ローカルにキャッシュされたイメージを消すには <code>docker rmi &#x3C;イメージのID></code>でいい。</p>
<p>自前で作ったイメージを <a href="https://index.docker.io/">https://index.docker.io/</a>  に登録するには、あらかじめサイト上でアカウントを作っておき、 <code>docker login</code> した後に <code>docker push</code> する。イメージ名にアンダーバー使っていると <code>push</code> で失敗するのと、アップロードしたイメージを消す機能がまだなかったりするので注意。</p>
<p>イメージの実体は <code>/var/lib/docker/graph/</code> にある。</p>
<pre><code>$ docker images -a -notrunc
REPOSITORY          TAG                 ID                                                                 CREATED
base                latest              b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-12.10        b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-quantl       b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
base                ubuntu-quantal      b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago
&#x3C;none>              &#x3C;none>              27cf784147099545                                                   12 weeks ago

$ sudo ls -1 /var/lib/docker/graph
141fef9a2f57e86dd6d9aa58fe9318b0d9d71d91053079842051d9738bad6e45
27cf784147099545
b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc
checksums
:tmp:
</code></pre>
<p>ここで images に ID: 27cf784147099545 というのが現れた。これは何か。<code>inspect</code> を使うとイメージの詳細を表示できる。</p>
<pre><code>$ docker inspect base
{
    "id": "b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc",
    "parent": "27cf784147099545",
    "created": "2013-03-23T22:24:18.818426-07:00",
    "container": "3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0",
    "container_config": {
        "Hostname": "",
        "User": "",
        "Memory": 0,
        "MemorySwap": 0,
        "CpuShares": 0,
        "AttachStdin": false,
        "AttachStdout": false,
        "AttachStderr": false,
        "PortSpecs": null,
        "Tty": true,
        "OpenStdin": true,
        "StdinOnce": false,
        "Env": null,
        "Cmd": [
            "/bin/bash"
        ],
        "Dns": null,
        "Image": "base",
        "Volumes": null,
        "VolumesFrom": ""
    }
}
</code></pre>
<p>ID: 27cf784147099545 は base イメージの親イメージの ID であることがわかる。イメージは差分になっているので、親のイメージが必要ということで初回実行のタイミングで base と一緒に 27cf784147099545 もダウンロードされていたのだった。</p>
<h3>ネットワーク</h3>
<p><code>docker run</code> 時に <code>-p</code> をつけることで、コンテナから外部にさらすポートを決められる。コンテナ側のポートはホスト側のポートに変換される際、ポート番号が変更される(49153以降になる)ので、<code>docker port &#x3C;ジョブのID> &#x3C;ポート番号></code> あるいは <code>docker ps </code> でポートの対応状況を確認する必要がある。</p>
<p>ドキュメントの <a href="https://github.com/dotcloud/docker#expose-a-service-on-a-tcp-port">Expose a service on a TCP port</a> がわかりやすい。</p>
<pre><code># 以下、コメントは書き換えてある
# また、途中経過がわかりやすいように set -x しておく
set -x

# 4444 を晒すよう -p オプションをつけて docker run しつつ、
# コンテナは netcat で4444を待ち受ける
JOB=$(docker run -d -p 4444 base /bin/nc -l -p 4444)
++ docker run -d -p 4444 base /bin/nc -l -p 4444
+ JOB=c86c892574f7

# 4444 がローカルのどのポートに対応するのか確認
# docker ps でも調べることはできる
PORT=$(docker port $JOB 4444)
++ docker port c86c892574f7 4444
+ PORT=49166

# ルーティングによっては localhost とか 127.0.0.1 だと
# うまくいかないことがあるので、eth0 のIPアドレスを使おう、
# ってことらしい
IP=$(ifconfig eth0 | perl -n -e 'if (m/inet addr:([\d\.]+)/g) { print $1 }')
++ perl -n -e 'if (m/inet addr:([\d\.]+)/g) { print $1 }'
++ ifconfig eth0
+ IP=10.156.137.111
echo hello world | nc $IP $PORT
+ nc 10.156.137.111 49166
+ echo hello world

# コンテナが受信したメッセージを logs で表示
echo "Daemon received: $(docker logs $JOB)"
++ docker logs c86c892574f7
+ echo 'Daemon received: hello world'
Daemon received: hello world
</code></pre>
<h3>Dockerfile</h3>
<p>DSLで書かれた設定(通常ファイル名は<code>Dockerfile</code>とする)をあらかじめ用意することで、手順に従ってイメージを作ることができる。</p>
<pre><code>読み込ませ方 (1)
docker build &#x3C;Dockerfileのあるディレクトリ>
# ex. docker build .

読み込ませ方 (2)
docker build -
# ex. docker build - &#x3C; /foo/bar/Dockerfile
</code></pre>
<p>Dockerfile の例</p>
<pre><code>FROM base
RUN /bin/echo hi
</code></pre>
<p>これで、<code>docker build</code> すれば <code>docker run base /bin/echo hi</code> と同じ効果が得られる。</p>
<p>指定できるはコマンドは以下の通り。大文字小文字は区別しないけど、引数と見分けやすいように大文字が使われる。</p>
<ul>
<li><code>FROM &#x3C;image></code> ベースとなるイメージを指定</li>
<li><code>MAINTAINER &#x3C;name></code> メンテナの名前を指定</li>
<li><code>RUN &#x3C;command></code> ビルド中に実行したいコマンドを指定</li>
<li><code>CMD &#x3C;command></code> 起動後のコンテナで実行したいコマンドを指定</li>
<li><code>EXPOSE &#x3C;port> [&#x3C;port> ...]</code> 外部に晒すポートの指定</li>
<li><code>ENV &#x3C;key> &#x3C;value></code> 環境変数の設定</li>
<li><code>INSERT &#x3C;file url> &#x3C;path></code> deprecated なので ADD を利用すること</li>
<li><code>ADD &#x3C;src> &#x3C;dest></code> ファイルを配置</li>
</ul>
<p><code>RUN</code> と <code>CMD</code> の違いがわかりにくいかもしれない。例を出す。</p>
<pre><code># RUN, CMD で指定したコマンドが実行されたとき、
# 標準出力と /tmp/*.log に記録を残す

$ cat &#x3C;&#x3C;SCRIPT >Dockerfile
> FROM base
> RUN /bin/echo run | tee /tmp/run.log
> CMD /bin/echo cmd | tee /tmp/cmd.log
> SCRIPT

# ビルドの実行

$ docker build .
Caching Context 10240/? (n/a)
FROM base ()
===> b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc
RUN /bin/echo run | tee /tmp/run.log (b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc)
===> d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d
CMD /bin/echo cmd | tee /tmp/cmd.log (d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d)
===> 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229
Build successful.
===> 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229

# run, cmd の実行結果を確認
# => run だけが実行されている

$ docker run 60671e99691 /bin/ls /tmp/
run.log

# イメージを inspect する
# => どうやらコンテナは記憶していることがわかる

$ docker inspect 60671e99691
{
    "id": "60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229",
    "parent": "d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d",
    "created": "2013-06-16T16:29:14.602237Z",
    "container": "4c54683cec90500f329dfaad2e0856cc408483be0ae3166018121d4d4b9b3282",
    "container_config": {
        "Hostname": "78c72f8ba6ad",
        "User": "",
        "Memory": 0,
        "MemorySwap": 0,
        "CpuShares": 0,
        "AttachStdin": false,
        "AttachStdout": false,
        "AttachStderr": false,
        "PortSpecs": null,
        "Tty": false,
        "OpenStdin": false,
        "StdinOnce": false,
        "Env": null,
        "Cmd": [
            "/bin/sh",
            "-c",
            "#(nop) CMD [/bin/sh -c /bin/echo cmd | tee /tmp/cmd.log]"
        ],
        "Dns": null,
        "Image": "d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d",
        "Volumes": null,

# 引数でコマンドを指定せずに run を実行
# => cmd で登録した内容が実行される

$ docker run 60671e99691
cmd
</code></pre>
<p>つまり、<code>RUN</code> は <code>Dockerfile</code> を元にビルドしているときに参照され、<code>CMD</code> はコンテナを実行する際に参照されるということがわかる。パッケージをインストールしたりといった用途では通常 <code>RUN</code> を使う。</p>
<h2>まとめ</h2>
<p>仮想環境の発達でプログラマブルなインフラストラクチャーは実現できてきているけど、マシンを上げたり下げたりするのにどうしても時間がかかるし、それは仕方が無いものと我慢していた。<code>Docker</code> を使ってみると、今までのそういった不満から解放されることができそう。一応開発中というステータスなのでプロダクション環境では使いづらいけど、開発やテスト、とくに構成管理ツールを設定するときなどは、この俊敏性、柔軟性は有効になると思う。</p>
<h2>参考</h2>
<ul>
<li><a href="http://docs.docker.io/en/latest/">Documentation</a></li>
</ul>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-06-16:/blog/2013/06/16/yum-cache/</id>
  <title type="text">yum のパッケージキャッシュについて</title>
  <published>2013-06-15T16:26:00.000Z</published>
  <updated>2013-06-15T16:26:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/06/16/yum-cache/"/>
  <content type="html">
  <![CDATA[<p><code>/etc/yum.conf</code>で<code>keepcache=1</code>にしておくと、インストールしたパッケージがキャッシュされるようになる。これが無効化された状態だと、パッケージアップグレード時に問題が起きても元に戻せなくなるので有効化しておいた方がいい。</p>
<!-- more -->
<p>あるパッケージについて、どのバージョンが利用可能な状態かは以下で確認できる。</p>
<pre><code>$ sudo yum --showduplicates list パッケージ名
</code></pre>
<p>RHEL なら過去のバージョンまですべて手に入るけど、CentOS だとOSリリース時のバージョンと最新版しか手に入らない模様。リポジトリ上なりキャッシュなりで過去のバージョンが手に入るのであれば、<code>yum install</code> や <code>yum update</code> は以下の手順でロールバックが行える。</p>
<pre><code># yum の利用履歴を確認
$ sudo yum history

# 履歴から詳細を確認
# 未引数なら直近、引数ありなら該当する ID を表示
$ sudo yum history info 4

# 仮に ID 4 で問題のバージョンアップが行われたようだということが確認できたら、その ID を指定して操作をアンドゥ
$ sudo yum history undo 4
</code></pre>
<p>アンドゥ(リドゥもある)では、対象パッケージおよび依存パッケージがまとめて一度に入れ替えられる。これはパッケージの操作がちゃんとトランザクションになっているため。</p>
<p>話がそれるけど、パッケージの操作にトランザクションがかかるというのはかなり重要だ。たとえば syslog-ng から rsyslog に入れ替えるとき、単純にアンインストール、インストールの順番でやろうとするとアンインストールのタイミングで大量の Syslog 依存なパッケージが道連れになるけど、以下のようにすればひとつのトランザクションでパッケージを入れ替えることができる。(情報源: <a href="http://wiki.rsyslog.com/index.php/Install_rsyslog_with_yum">Rsyslog Wiki</a>)</p>
<pre><code>$ sudo yum shell
> remove syslog-ng
> install rsyslog
> run
</code></pre>
<p>話がそれたついでにふれておくと、vagrant を使っているのであれば <a href="https://github.com/fgrehm/vagrant-cachier">vagrant-cachier</a> を使うとパッケージのキャッシュ保存先を仮想マシン外の領域(ホストOSとの共有ディスク部分など)に変更してくれる。こうすることで、仮想マシンを破棄してもパッケージのキャッシュが永続化されるため、2回目以降はダウンロードがスキップされて高速化する。</p>
<p>話を戻すと、世の中何が起きるかわからないので古いパッケージもとっておいたほうがいいかと。ディスク容量が気になりだしたら、<code>yum clean packages</code> を実行すればキャッシュは消せる。</p>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-05-07:/blog/2013/05/07/create-ominibus-installer/</id>
  <title type="text">omnibus を使って オムニバスインストーラーを作成する</title>
  <published>2013-05-06T16:41:00.000Z</published>
  <updated>2013-05-06T16:41:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/05/07/create-ominibus-installer/"/>
  <content type="html">
  <![CDATA[<p>Chef のインストールは結構面倒くさかったんだけど、<a href="http://www.opscode.com/chef/install/">オムニバスインストーラー</a>が出たことで状況はがらっと変わって、簡単に導入できるようになった。このオムニバスインストーラーの仕組みは汎用的に作られているので、他のツールでも適用できるという話。</p>
<!-- more -->
<h2>オムニバスインストーラーについて</h2>
<p>Chef のオムニバスインストーラーを実行すると以下のようなディレクトリ構成でファイルが置かれる:</p>
<ul>
<li>/opt/chef/bin/ ... Chef 関連のスクリプト</li>
<li>/opt/chef/embedded/ ... ruby インタプリタ、Chef とその他依存パッケージ</li>
<li>(/usr/bin/ ... /opt/chef/bin/ 以下のものがシンボリックリンクが配置される)</li>
</ul>
<p>以上の通り、<code>/opt/chef</code> の中に動作に必要なものがごっそり置かれる。アプリケーションレベルでプログラミングの処理系を持っちゃうというのはこれに限らずよく見る光景で、理由としてはパッケージ提供されていない最新版が使いたかったり、バージョンアップやライブラリインストールの影響範囲を限定させたかったりだと思う。</p>
<p>ここしばらくは手軽なパッケージ作成ツールとして<a href="https://github.com/jordansissel/fpm">fpm</a>がよく使われているけど、オムニバスインストーラーは<a href="https://github.com/opscode/omnibus-ruby">omnibus</a>という「ビルドツール」＋「fpm ラッパー」といった感じのもので作られている。以下は実際に <a href="https://github.com/opscode/omnibus-ruby">omnibus</a> を使ったインストーラー作成の手順についてまとめる。</p>
<h2>パッケージ作成</h2>
<p><a href="https://github.com/etsy/statsd">statsd</a> および <a href="https://github.com/etsy/statsd">statsd</a> を動かすために必要な Node.js を /opt/statsd にインストールする RPM, Deb パッケージの作成を行ってみる。</p>
<h3>環境</h3>
<ul>
<li>Macbook Air Mountain Lion</li>
<li>Ruby 2.0.0-p0</li>
<li>Vagrant 1.2.2</li>
</ul>
<h3>手順</h3>
<pre><code># omnibus のインストール
gem install omnibus

# 必要となる vagrant 用の plugin をインストール
vagrant plugin install vagrant-omnibus
vagrant plugin install vagrant-berkshelf

# プロジェクトディレクトリの作成(ディレクトリ名は `omnibus-プロジェクト` となる)
omnibus project statsd
cd omnibus-statsd

# プロジェクトディレクトリ内のファイルを適宜修正:
    Berksfile
      Berkshelf 用の設定。変更する必要無い。
    Vagrantfile
      Vagrant 用の設定。2013-06-07 現在だと CentOS 5, 6 Ubuntu 10.04, 11.04, 12.04 の設定が導入済み。
    README.md
    omnibus.rb.example
      成果物を S3 上にキャッシュする場合などに利用。使わないなら気にしなくていい。
    config/projects/statsd.rb
      後述
    config/software/*
      後述
    package-scripts/statsd/*
      インストール時、アンインストール時などに実行したいスクリプトなど。
</code></pre>
<p>この中で、実際のビルドプロセスを定義するのは、config/projects/ 以下と config/software 以下になる。</p>
<p><code>config/projects/</code> はプロジェクトの設定を格納するディレクトリで、初期状態では statsd 用のプロジェクトファイル <code>config/projects/statsd.rb</code> が作られている。このファイルを修正していくことになる。</p>
<pre><code>name "statsd"
maintainer "f440"
homepage "https://github.com/f440/omnibus-statsd"

install_path    "/opt/statsd"
build_version   "0.6.0"
build_iteration 1

dependency "preparation"
dependency "node"
dependency "statsd"

exclude "\.git*"
</code></pre>
<p>おおむね想像がつく名前だけど、dependency だけはよく分からないと思う。dependency で指定したものはプロジェクトを構成する software という扱いで、<code>config/software/</code> 以下でその設定を行っていく。</p>
<p>software の例を示す。典型的な例だと、指定した URL からダウンロードしてきたものを一時ディレクトリで展開して、<code>configure &#x26;&#x26; make &#x26;&#x26; make install</code> を実行、などだが今回の作業では Node.js のバイナリを展開して <code>/opt/embedded</code> 以下にコピーしているだけである。</p>
<pre><code>name "node"
version "0.10.5"

source :url => "http://nodejs.org/dist/v0.10.5/node-v0.10.5-linux-x64.tar.gz",
       :md5 => "fb65723d395c559393201dd41e0eb275"

relative_path "node-v0.10.5-linux-x64"

build do
  command "rsync -av . #{install_dir}/embedded/"
end
</code></pre>
<p>必要となる software の設定を全部そろえたらビルドを実行する。マシンの起動、Chef のインストール、omnibus の Cookbook 実行、ビルド環境構築、ビルド実行、パッケージ作成 といったことが行われることになるため、初回はかなり待つことになる。</p>
<pre><code>vagrant up
(vagrant up centos-6 など、直接マシンを指定してもいい)
(もし Linux 上で作業しているのであれば、omnibus build project statsd で直接パッケージ作成を開始出来る)
</code></pre>
<p>問題なければ、pkg/ 以下に statsd-0.6.0-1.el6.x86_64.rpm, statsd_0.6.0-1.ubuntu.12.04_amd64.deb といったファイルが出来る。</p>
<h2>まとめ</h2>
<p>やっていることは <a href="https://github.com/jordansissel/fpm">fpm</a> でパッケージを作っているだけなんだけど、<a href="http://www.vagrantup.com/">Vagrant</a> x <a href="http://berkshelf.com/">Berkshelf</a> x <a href="http://www.opscode.com/chef/">Chef</a> のコンビネーションのおかげで、パッケージとそのパッケージを作るための環境が簡単に手に入るのはとてもいい。複数環境のパッケージを作る予定がなくっても、最初から<a href="https://github.com/opscode/omnibus-ruby">omnibus</a>上でパッケージを作れるようにしておくと運用が楽そう。</p>
<h2>備考</h2>
<p>似たようなツールとして、<a href="https://github.com/joemiller/bunchr">bunchr</a> が存在する。</p>
<h2>参考</h2>
<ul>
<li><a href="https://github.com/etsy/statsd">Statsd</a></li>
<li><a href="http://www.opscode.com/chef/install/">Install Chef</a></li>
<li><a href="https://github.com/opscode/omnibus-ruby">omnibus</a></li>
<li><a href="https://github.com/joemiller/bunchr">bunchr</a></li>
<li><a href="https://github.com/jordansissel/fpm">fpm</a></li>
<li><a href="http://www.vagrantup.com/">Vagrant</a></li>
<li><a href="http://berkshelf.com/">Berkshelf</a></li>
</ul>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-05-03:/blog/2013/05/03/create-mesos-rpm-using-fpm/</id>
  <title type="text">fpm で Mesos の RPM を作るまで</title>
  <published>2013-05-03T08:24:00.000Z</published>
  <updated>2013-05-03T08:24:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/05/03/create-mesos-rpm-using-fpm/"/>
  <content type="html">
  <![CDATA[<p><a href="http://incubator.apache.org/mesos/">Mesos</a> をインストールするとき各マシンでビルドはしんどいので、<a href="https://github.com/jordansissel/fpm">fpm</a> で Mesos の RPM を作ってインストールしている。ビルドからパッケージ作成までの作業ログを残しておく。</p>
<!-- more -->
<ul>
<li><a href="https://github.com/jordansissel/fpm">fpm</a> は Ruby の gem や Node.js の npm などのプログラミング言語のライブラリ、あるいは直接ディレクトリから RPM やら Deb やらのパッケージを作成するソフトウェア。</li>
<li><a href="http://incubator.apache.org/mesos/">Mesos</a> はクラスタ構成のリソースをよしなに管理するソフトウェア。
<ul>
<li>今回の話では具体的な使い方までは触れない</li>
</ul>
</li>
</ul>
<h2>手順</h2>
<p>作業環境は CentOS 6.4 x86_64。</p>
<p>Ruby をインストール。</p>
<pre><code>sudo yum install ruby.x86_64 rubygems ruby-devel.x86_64 rpm-build.x86_64
</code></pre>
<p>fpm をインストール。</p>
<pre><code>sudo gem install fpm --no-rdoc --no-ri
</code></pre>
<p>Mesos のソースをダウンロード、展開。</p>
<pre><code>curl -LO http://ftp.meisei-u.ac.jp/mirror/apache/dist/incubator/mesos/mesos-0.10.0-incubating/mesos-0.10.0-incubating.tar.gz
tar xf mesos-0.10.0-incubating.tar.gz
cd mesos-0.10.0
</code></pre>
<p>Mesos のビルドに必要なパッケージをインストール。</p>
<pre><code>sudo yum install gcc-c++.x86_64 patch.x86_64 python-devel.x86_64 \
  cppunit-devel.x86_64 java-1.6.0-openjdk-devel.x86_64
</code></pre>
<p>ビルド。今回は、configure のオプションで Redhat っぽい配置を指定している。<code>/opt/mesos</code> とか <code>/usr/local/mesos</code> に全部まとめたければ --prefix を使うなど、このあたりはお好みで。
<code>make install</code> 時には書き込み可能な場所を DESTDIR で指定。説明中では、<code>/tmp/mesos</code> を利用している。</p>
<pre><code>JAVA_HOME=/etc/alternatives/java_sdk ./configure \
  --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/libexec \
  --localstatedir=/var --libdir=/usr/lib64 --includedir=/usr/include \
  --datarootdir=/usr/share
make
make install DESTDIR=/tmp/mesos
</code></pre>
<p>fpm でパッケージを作成。詳細は fpm --help を参照。注意点としては、<code>--description</code> は RPM のメタ情報 <code>description</code>, <code>summary</code> で兼用されるので、あまり長い情報を入れると <code>yum search</code> とかがごちゃごちゃすることになる。適度に切り詰めた方がいい。</p>
<pre><code>fpm -s dir -t rpm \
  -v 0.10.0 \
  -n mesos \
  -C /tmp/mesos \
  -a x86_64 \
  --license "ASL 2.0" \
  --url "http://incubator.apache.org/mesos/" \
  --description "Dynamic resource sharing for clusters" \
  -d python-devel \
  -d java-1.6.0-openjdk-devel \
  .
</code></pre>
<p>RPM ファイルのメタ情報やファイル一覧をチェック。</p>
<pre><code>rpm -qpi mesos-0.10.0-1.x86_64.rpm
rpm -qpl mesos-0.10.0-1.x86_64.rpm
</code></pre>
<p>あとは、できあがった RPM ファイルを他のマシンに持っていってインストール。</p>
<pre><code>sudo yum install ./mesos-0.10.0-1.x86_64.rpm
</code></pre>
]]>
  </content>
</entry>
<entry>
  <id>tag:apatheia.info,2013-04-06:/blog/2013/04/06/about-ansible/</id>
  <title type="text">構成管理ツール Ansible について</title>
  <published>2013-04-06T05:50:00.000Z</published>
  <updated>2013-04-06T05:50:00.000Z</updated>
  <link href="https://apatheia.info/blog/2013/04/06/about-ansible/"/>
  <content type="html">
  <![CDATA[<p><a href="http://ansible.cc/">Ansible</a> というサーバーの設定を管理するツールの説明。いわゆる構成管理 (CM: Configuration Management) にカテゴライズされるもので、Puppet や Chef の親戚みたいなものと考えてもらえればだいたいあってる。</p>
<!-- more -->
<h2>概要</h2>
<p>リード開発者は Michael DeHaan で、現職の AnsibleWorks の前は Redhat で <a href="http://cobbler.github.io/">Cobbler</a> や <a href="https://fedorahosted.org/func/">Func</a> に携わっていたり、Puppet labs でプロダクトマネージャーしたりしているという経歴の持ち主。</p>
<p>Ansible は Python で書かれている。同じジャンルで Python 製というと <a href="http://saltstack.com/">Salt</a> が有名。Chef の場合、レシピを書くためには Ruby の知識が必要となってくるけど、Ansible はどんな言語でもモジュールが書けるようになっているので、運用にあたって Python の知識は必要無い。</p>
<p>動作の点でも Puppet や Chef などのツールとまったく異なるアプローチをしている。Puppet や Chef は、サーバーとクライアントで構成され、クライアントとなるマシンはサーバーに設定を問い合わせながら、自分自身を「あるべき状態」に収束するよう変更を加えていく。Ansible の場合、サーバー側からクライアントとなるサーバー(群)に対して直接命令を送り込み結果を得る。これは <a href="https://fedorahosted.org/func/">Func</a>、<a href="http://capistranorb.com/">Capistrano</a>、<a href="http://fabfile.org/">Fabric</a> などに似ているが、これらのデプロイを目的としたツールにはない「何回やっても結果が同じ」(idempotence) という CM ツールらしさはちゃんと備えている。</p>
<p>ドキュメントは12ページしかなく(ちなみに、さっき数えてみたらChefのドキュメントは2834ファイルあった) 非常に習得は簡単。サーバーを立てる必要もなく、クライアントマシンもエージェントレス、加えて短期間で学習できるので手軽感は非常に高いが、モジュール機構が強力なのできわめて実用的になっている。</p>
<h2>基本的な概念</h2>
<p>Ansible を理解する上で重要となる、モジュールとプレーブックについて説明する。</p>
<h3>モジュール</h3>
<p>クライアント内での動きはモジュールという形で定義する。</p>
<p>パッケージのインストール、サービスの起動、ユーザーやグループの作成などの基本的なモジュールはあるが、実際には環境に合わせて不足分は自分でモジュールを作っていくことになる。</p>
<p>モジュールは簡単に作れる。モジュールが役割を端的に言うと、以下を行うだけである。</p>
<ul>
<li>
<p>標準入力でオプションを受け取る</p>
</li>
<li>
<p>標準出力で実行結果を返す</p>
<ul>
<li>出力形式は key=value を空白でつなげたものか JSON</li>
</ul>
</li>
</ul>
<p>これができる言語であれば、シェルスクリプトでも Perl でも問題ない。</p>
<h3>プレーブック</h3>
<p>実際の処理では単発のモジュールでサーバーの設定が終わることはないので、モジュールの使い方をまとめたものが必要になる。Ansible では、YAML で処理をまとめたものを プレーブック (Playbook)と呼んでいる。</p>
<p>例: Apache と PHP をインストールする (webapp.yml)</p>
<pre><code>- hosts: webserver
  user: vagrant
  sudo: yes
  tasks:
    - name: install apache
      action: yum pkg=httpd state=installed
    - name: install php
      action: yum pkg=php state=installed
</code></pre>
<p>例: 実行</p>
<pre><code># ansible-playbook プレーブック名
$ ansible-playbook webapp.yml
</code></pre>
<p>以上は簡単な例だが、設定ファイルを配置したり、それに併せてサービスを再起動させたりといったことも記述可能。</p>
<p>プレーブックには以下のような内容が含まれる:</p>
<ul>
<li>hosts: 対象のホスト</li>
<li>user: 実行ユーザー</li>
<li>vars: 変数</li>
<li>tasks: タスク</li>
</ul>
<p><code>vars</code> の変数は、テンプレート内で展開される。設定ファイル配置時にパラメータを変更、といった場合に利用する。</p>
<h3>インストール</h3>
<p>以下では、インストールから簡単なコマンドの実行までの例を挙げる。サーバー、クライアント双方で CentOS 6.4 を利用した。</p>
<p>Ansible を動かすためには、Python 2.6 以上と Ansible のソースコードとごくわずかな Python パッケージだけあればよい。CentOS 6 であれば Python の条件は満たせているし、EPEL で Ansible のパッケージが提供されているので、<code>yum</code> でインストール可能。</p>
<pre><code># EPEL 有効化
$ sudo rpm -ivh http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm

# Ansible インストール
$ sudo yum install ansible
</code></pre>
<p>他の Unix 系OSであれば、<code>pip install ansible</code> でいい。</p>
<pre><code>$ sudo pip install ansible
</code></pre>
<p>次に、サーバーからクライアントに SSH でログインできるように調整しておく。</p>
<pre><code># 以下のマシンを用意した。
# それぞれホスト名でアクセスできる
#    Ansible 実行側 ... server
#    変更対象 ... client1, client2

# server側で公開鍵認証用の鍵を作成
$ $ ssh-keygen -t rsa

# client に公開鍵を配置する
$ ssh-copy-id client1
$ ssh-copy-id client2

# 試しにログインしてみる
# 頻繁に実行することになるので、公開鍵にパスフレーズを
# 設定している場合は、ssh-agent を使ってパスフレーズの
# 入力を省略できるようにしておく。
$ ssh client1
$ ssh client2
</code></pre>
<p>今度は、対象のサーバーを設定してみよう。環境変数 <code>ANSIBLE_HOSTS</code> にあるファイルでサーバーの指定が可能。</p>
<pre><code>$ cat &#x3C;EOD >~/target
> [webserver]
> client1
> 
> [dbserver]
> client2
> EOD
$ export ANSIBLE_HOSTS=~/target
</code></pre>
<p>設定の中で、<code>[ ]</code> によりグループを作っている。つまり「webserver グループに client1、dbserver グループに client2 が所属している」ということを表している。グループはオプションなので、単純にホスト名を羅列するだけでもいい。試しに、対象のホストを調べてみよう。</p>
<pre><code># ansible ホストパターン --list-hosts

# ホスト名を直接指定
$ ansible client1 --list-hosts
client1

# グループ名を指定
$ ansible webserver --list-hosts
client1
$ ansible dbserver --list-hosts
client2

# all を指定した場合、全サーバーを列挙
$ ansible all --list-hosts
client1
client2
</code></pre>
<p>これだけで準備は完了。実行してみる。</p>
<pre><code># コマンドの書式
ansible 対象 -m モジュール名 -a オプション

# 例 ping モジュール
$ ansible all -m ping
client2 | success >> {
    "changed": false,
    "ping": "pong"
}

client1 | success >> {
    "changed": false,
    "ping": "pong"
}
</code></pre>
<p><code>-m</code> をつけないで、直接コマンドを実行することも可能。</p>
<pre><code># すべてのマシンでカーネルのバージョンを取得
$ ansible all -a 'uname -r'
client2 | success | rc=0 >>
2.6.32-358.el6.x86_64

client1 | success | rc=0 >>
2.6.32-358.el6.x86_64
</code></pre>
<p>プレーブックを実行したときは以下のようになる。</p>
<pre><code># 対象は webserver というグループ(client1 が所属)に対して、
# Apache と PHP をインストールするプレーブック、webapp.yml を実行
# Apache はすでにインストールされていたので、
# PHP のみインストールされることとなった

$ ansible-playbook webapp.yml

PLAY [webserver] *********************

GATHERING FACTS *********************
ok: [client1]

TASK: [install apache] *********************
ok: [client1]

TASK: [install php] *********************
changed: [client1]

PLAY RECAP *********************
client1                        : ok=3    changed=1    unreachable=0    failed=0
</code></pre>
<h2>その他</h2>
<ul>
<li><a href="https://twitter.com/mitchellh/status/319914935910027264">Vagrant もバージョン 1.2 から Ansible でのプロビジョニングをサポート予定</a></li>
<li>開発は活発</li>
<li>リリース名がヴァンヘイレンの曲名 (1.0 は Eruptionだった)</li>
<li>ロゴがださい (ML でも 90年代のデザインなんて言われている)</li>
</ul>
<h2>まとめ</h2>
<p>ロゴのセンスは悪いけど、アプリケーション自体の仕組みはすごくセンスがいい。</p>
<p>他の構成管理ツールと比べると、DSL を覚えるといった「ツールを使うまでののコスト」、ツールのためのサーバー構築・運用といった「ツールを使ってからのコスト」が軽微なので、よりやりたいことに目が向けられるのもうれしい。</p>
<p>最近日本国内でも Chef の話題を聞くことが多いんだけど、Chef Server の運用とかオートスケールとのコンビネーションとかの情報はあまり聞かないので、たぶん割と小規模な環境でリモートサーバーの Chef-solo をキックみたいなケースが多いのかと思う。そういったところだと、Ansible のほうがふさわしいっていうことが多いんじゃないかな。</p>
]]>
  </content>
</entry>
  </feed>