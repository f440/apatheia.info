<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>aptheia.info</title><link rel="alternate" type="application/rss+xml" title="apatheia.info" href="/atom.xml"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/c513e7236c8bf720ec88.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c513e7236c8bf720ec88.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-1a8a258926ecde76681b.js" defer=""></script><script src="/_next/static/chunks/framework-895f067827ebe11ffe45.js" defer=""></script><script src="/_next/static/chunks/main-a9acf05574b3448968f1.js" defer=""></script><script src="/_next/static/chunks/pages/_app-e9aa21e07402b3aa54f7.js" defer=""></script><script src="/_next/static/chunks/915-c287d7adb8a31cf4da35.js" defer=""></script><script src="/_next/static/chunks/pages/index-d8aba403c190d3f640a2.js" defer=""></script><script src="/_next/static/EL7aLKI83tZi_FFCsIrzy/_buildManifest.js" defer=""></script><script src="/_next/static/EL7aLKI83tZi_FFCsIrzy/_ssgManifest.js" defer=""></script></head><body><div id="__next"><header><div id="site-title"><h1><a href="/">apatheia.info</a></h1></div><nav><ul><li><a href="/">Home</a></li><li><a href="/atom.xml">RSS</a></li></ul></nav></header><main><article><section style="margin-bottom:1em"><h2><a href="/blog/2016/10/30/isucon6/">ISUCON 6</a></h2><span>2016.10.30</span> <span><a href="/blog/categories/isucon/">isucon</a> </span><span><a href="/blog/categories/performance/">performance</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2016/02/22/install-dlite/">DLiteでOS X上にDockerの環境を構築する</a></h2><span>2016.02.22</span> <span><a href="/blog/categories/docker/">docker</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/11/11/isucon3/">ISUCON 3 の参加記録</a></h2><span>2013.11.11</span> <span><a href="/blog/categories/isucon/">isucon</a> </span><span><a href="/blog/categories/performance/">performance</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/08/10/immutable-infrastructure/">Immutable Infrastracture について</a></h2><span>2013.08.10</span> <span><a href="/blog/categories/infrastructure/">infrastructure</a> </span><span><a href="/blog/categories/virtualization/">virtualization</a> </span><span><a href="/blog/categories/aws/">aws</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/08/10/octopress-to-middleman/">ブログエンジンを Octopress から Middleman に変えた</a></h2><span>2013.08.10</span> <span><a href="/blog/categories/octopress/">octopress</a> </span><span><a href="/blog/categories/middleman/">middleman</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/06/17/docker/">仮想環境構築に docker を使う</a></h2><span>2013.06.17</span> <span><a href="/blog/categories/docker/">docker</a> </span><span><a href="/blog/categories/lxc/">lxc</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/06/16/yum-cache/">yum のパッケージキャッシュについて</a></h2><span>2013.06.16</span> <span><a href="/blog/categories/yum/">yum</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/05/07/create-ominibus-installer/">omnibus を使って オムニバスインストーラーを作成する</a></h2><span>2013.05.07</span> <span><a href="/blog/categories/packaging/">packaging</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/05/03/create-mesos-rpm-using-fpm/">fpm で Mesos の RPM を作るまで</a></h2><span>2013.05.03</span> <span><a href="/blog/categories/packaging/">packaging</a> </span><span><a href="/blog/categories/mesos/">mesos</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/04/06/about-ansible/">構成管理ツール Ansible について</a></h2><span>2013.04.06</span> <span><a href="/blog/categories/cm/">cm</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/03/17/logging-with-splunk-storm/">イベント管理にSplunk Stormを使ってみる</a></h2><span>2013.03.17</span> <span><a href="/blog/categories/logging/">logging</a> </span><span><a href="/blog/categories/visualization/">visualization</a> </span><span><a href="/blog/categories/splunk/">splunk</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/03/17/fluentd-and-graphite/">FluentdのデータをGraphiteに出力するときのTips</a></h2><span>2013.03.17</span> <span><a href="/blog/categories/fluentd/">fluentd</a> </span><span><a href="/blog/categories/graphite/">graphite</a> </span><span><a href="/blog/categories/logging/">logging</a> </span><span><a href="/blog/categories/visualization/">visualization</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/03/17/fluentd-and-stathat/">Fluentdの出力結果をStatHatで可視化する</a></h2><span>2013.03.17</span> <span><a href="/blog/categories/fluentd/">fluentd</a> </span><span><a href="/blog/categories/stathat/">stathat</a> </span><span><a href="/blog/categories/logging/">logging</a> </span><span><a href="/blog/categories/visualization/">visualization</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/02/08/create-chocolatey-package/">chocolatey で Haskell Platform 用のパッケージを作る</a></h2><span>2013.02.08</span> <span><a href="/blog/categories/windows/">windows</a> </span><span><a href="/blog/categories/chocolatey/">chocolatey</a> </span><span><a href="/blog/categories/haskell/">haskell</a> </span><span><a href="/blog/categories/chef/">chef</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/02/03/installing-node-dot-js-on-windows-7/">Windows 上に node.js の開発環境を整える</a></h2><span>2013.02.03</span> <span><a href="/blog/categories/node.js">node.js</a> </span><span><a href="/blog/categories/windows/">windows</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/01/03/log-visualization-using-logstalgia/">logstalgia を使ってログを可視化</a></h2><span>2013.01.03</span> <span><a href="/blog/categories/logging/">logging</a> </span><span><a href="/blog/categories/visualization/">visualization</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2013/01/01/network-restriction-using-trickle/">Trickleを使って帯域制限をする</a></h2><span>2013.01.01</span> <span><a href="/blog/categories/unix/">unix</a> </span><span><a href="/blog/categories/linux/">linux</a> </span><span><a href="/blog/categories/network/">network</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2012/10/07/sub-for-subcommands/">サブコマンドを sub で処理する</a></h2><span>2012.10.07</span> <span><a href="/blog/categories/cli/">cli</a> </span><span><a href="/blog/categories/shell/">shell</a> </span><span><a href="/blog/categories/unix/">unix</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2012/09/22/head-tail/">head と tail の行数指定方法</a></h2><span>2012.09.22</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2012/09/22/riemann/">イベント処理ツール riemannを使う</a></h2><span>2012.09.22</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2012/09/22/tumbler-to-octopress/">TumblerからOctopressへの移行</a></h2><span>2012.09.22</span> <span><a href="/blog/categories/octopress/">octopress</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2012/07/30/roundsman-capistrano-chef-solo/">roundsmanを使ってcapistranoからchef-soloを実行する</a></h2><span>2012.07.30</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2012/06/05/glusterfs/">分散ファイルシステム GlusterFS を使う</a></h2><span>2012.06.05</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2012/06/03/sphinx-guard-livereload/">sphinxの更新をguard-livereloadで検知してブラウザを自動リロードする</a></h2><span>2012.06.03</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2012/05/13/vps-lxc-xtradb-cluster/">さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす</a></h2><span>2012.05.13</span> <span><a href="/blog/categories/mysql/">mysql</a> </span><span><a href="/blog/categories/lxc/">lxc</a> </span><span><a href="/blog/categories/linux/">linux</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2012/01/03/rundeck-jenkins-java/">rundeckをセットアップして、jenkins上のjava成果物をデプロイする</a></h2><span>2012.01.03</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2011/12/31/mysql-sandbox-blackhole/">MySQL::Sandboxで環境を作ってBLACKHOLEエンジンを試す</a></h2><span>2011.12.31</span> <span><a href="/blog/categories/mysql/">mysql</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2011/12/24/superviser/">プロセス管理にsuperviserを使う</a></h2><span>2011.12.24</span> </section><section style="margin-bottom:1em"><h2><a href="/blog/2011/12/17/github-yum/">githubを使ってyumリポジトリを公開する</a></h2><span>2011.12.17</span> <span><a href="/blog/categories/git/">git</a> </span><span><a href="/blog/categories/github/">github</a> </span><span><a href="/blog/categories/centos/">centos</a> </span><span><a href="/blog/categories/rpm/">rpm</a> </span><span><a href="/blog/categories/yum/">yum</a> </span></section><section style="margin-bottom:1em"><h2><a href="/blog/2011/12/17/pv/">プログレスバーを簡単に表示できるコマンド pv</a></h2><span>2011.12.17</span> <span><a href="/blog/categories/unix/">unix</a> </span><span><a href="/blog/categories/pv/">pv</a> </span></section></article></main><footer><ul><li>Link:</li><li><a href="https://twitter.com/f440">Twitter</a></li><li><a href="https://github.com/f440">Github</a></li><li><a href="https://pinbaord.in/u:f440">Pinbaord</a></li></ul></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2016-10-30-isucon6.markdown","path":"blog/2016/10/30/isucon6","layout":"post","title":"ISUCON 6","createdAt":"2016-10-29T15:51:00.000Z","kind":"article","comments":true,"tags":["isucon","performance"],"content":"\n今年も[ISUCON](http://isucon.net/)に参加した。\n\n\u003c!-- more --\u003e\n\n\n例年に比べて十分な予習・復習をすることができず、メンバー内の得手・不得手を十分に理解できない状態だったので、不安を残したままコンテストを迎えることとなった。\n\n## 予選\n\n最初にAzure Resource Managerテンプレートで環境を作るのだが、プロビジョニングに失敗したり、作業ミス（AppArmorを削除したら一緒にMySQLが消された）などで、結局3回くらい環境の構築をしなおしてだいぶ焦ったりした。\n\n問題の内容としてはいわばはてなキーワードで、記事の投稿により別の記事内のリンクが増えるといったものだった。\n\n参考実装ではオンデマンドにリンクを計算してページをレンダリングするのだが、これでは遅いので事前に計算結果をキャッシュする方針とした。ただ、ページが投稿されるごとに過去の記事も再生成する必要があり、これについては影響を受けるページだけを絞り込んでキャッシュを破棄することで対応しようとした。\n\n方針が決まり、試しに雑なキャッシュをアプリに組み込んで見るとベンチマークを流すと毎回同じ箇所でベンチマークが失敗判定される。どうやらベンチマークは特定のページでリンクを確認しており、それ以外については問題があっても完走できる模様である。このことに気づいてからはベンチマークの挙動に合わせてキャッシュの再生成箇所を絞り込むなどを行った。\n\nこれだけでかなりスコアは伸びるようになったのだが、あとは地道にDBからRedisへの切り替えなどをしていった。\n\n運営側と多少の行き違いがあり、スコアが計算されない事態が発生して肝を冷やしたが、最終的には対応してもらいなんとか本戦にたどり着くことができた。\n\n## 本戦\n\n問題としてはリアルタイム性のあるお絵かきアプリという非常にこったものであった。利用されている技術としては Docker / React / SSE (Server-sent events) といったところで、慣れていないこともあり初動が遅くなってしまった。\n\n画像の表示部分で詰まっているような状態だったので、このあたりをなんとかしようという話ができはじめたのが昼から夕方にかけてで、いろいろ手を出しては失敗しを繰り返しているうちにタイムアップになってしまった。\n\n## コンテストを終えて\n\n今年も非常に楽しく行えた。やり応えのある問題、快適な会場の提供、その他もろもろ運営の方々には頭が下がるばかりである。\n\n本戦の結果についてはとにかく悔しい。この記事を書いている時点で一週間が経ったが、未だに後悔が残るばかりである。ただ、自分の未熟さを痛感させてくれるという意味でとても有意義であったと思う。今後も参加したいので、末永く続いてくれることを願う。\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2016-02-22-install-dlite.markdown","path":"blog/2016/02/22/install-dlite","layout":"post","title":"DLiteでOS X上にDockerの環境を構築する","createdAt":"2016-02-22T11:54:00.000Z","updatedAt":"2016-05-08T10:12:00.000Z","kind":"article","comments":true,"tags":["docker"],"content":"\n[DLite][dlite] をインストールしたので、そのときのメモ。\n\n\u003c!-- more --\u003e\n\n## DLiteとは\n\nOS XでDockerを使えるようにやつ。内部では[xhyve](https://github.com/mist64/xhyve)を使っていて非常にコンパクト。\n\n## 作業環境\n\n- DLite 1.1.3\n- OS X El capitan 10.11.3\n\n## インストール手順\n\n    brew install dlite\n    sudo dlite install # CPUやディスクサイズなどのオプションは`-h`で確認可能\n\nおしまい。\n\n内部では以下が行われる:\n\n- `/etc/sudoers`に`dlite,nfs`コマンドをパスワードなしで`sudo`できるようにする設定を追加\n- `~/Library/LaunchAgents/local.dlite.plist`に起動設定を配置\n- `~/.dlite`に起動イメージをダウンロード\n\n## 起動\n\nTmux内で起動しようとするとエラーになるので、必ずTmux外でやること。\n\n    dlite start\n\n問題がなければ以下が加えられる:\n\n- `/var/run/docker.sock` にソケットファイルを作成\n- `/etc/hosts` にdliteへの参照を追加 (デフォルトは `local.docker`。インストール時のオプションで変更可能)\n- `/etc/exports` にDLite側のホストへのNFSマウントする設定を追加\n- `~/Library/LaunchAgents/local.dlite.plist` をロードし、自動起動するように設定\n\nうまくいっていれば、`docker -H unix://var/run/docker.sock`(`export DOCKER_HOST=unix:///var/run/docker.sock`で指定も可)でdockerが使えるようになる。\n\nもしうまくいかないようなら、`sudo dlite daemon`でコマンドラインから実行して原因を突き止める。とくに、NFS周りのコンフリクトが起きていないかを確認。\n\n## まとめ\n\nホストと仮想環境の間がシームレスにつながってcoLinuxっぽさがある。こういうすっきりしたツールは楽しい。\n\n## 追記\n\n2016-05-08: 現在はDocker for Macを利用している。osxfsでファイルシステムのイベントが連携できて便利。\n\n## 参考\n\n- [GitHub - nlf/dlite: The simplest way to use Docker on OS X][dlite]\n- [Simplifying Docker on OS X](https://blog.andyet.com/2016/01/25/easy-docker-on-osx/)\n\n[dlite]: https://github.com/nlf/dlite\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-11-11-isucon3.markdown","path":"blog/2013/11/11/isucon3","layout":"post","title":"ISUCON 3 の参加記録","createdAt":"2013-11-11T00:20:00.000Z","kind":"article","comments":true,"tags":["isucon","performance"],"content":"\nWeb アプリケーションのパフォーマンスコンテスト [ISUCON 3](http://isucon.net/) に参加し、2 位の成績となった。どのような状態で当日を迎え、どのような作業を行ったのかをまとめる。\n\n\u003c!-- more --\u003e\n\n私自身はこれで三度目の ISUCON 参加となるが、今回チームを組むメンバーはみんな初めての参加ということもあり、事前の打ち合わせでは以下のようなことを話していた:\n\n- これまでの大会の説明と典型的なアプローチ\n    - 同時に、過去にとられた戦法は参考程度であること、あくまで現物のアプリケーションを元に戦略を立てるべきでアプリケーションやミドルウェアを事前に決めることは危険であるということは強調\n- よく使われるミドルウェアの概要、メリット/デメリット\n    - 主要機能のほか、キャッシュ部分の永続性の有無（単純に考えればメモリだけで処理してファイルに書き出さない方が早いが、ベンチマークをまたいでキャッシュを引き継げれば切り札になり得る）、キャッシュの時間単位(ベンチマークがフェイルしない限り極力キャッシュしたいという要望を考えると、ミリ秒単位で制御したくなることが多い) などのコンテストで重要となりそうな部分について解説\n- 今後問題として取り上げられそうなWebアプリケーションの構成要素\n    - 毎回趣向を凝らした題材が提供されるので完全に予想しきることは不可能だが、Web プログラミングで頻出する要素（予選で言えば「セッション」「ページング」のような単位）を挙げ、他にもどのようなものが考えられるか、どうやって実装できてどのような高速化が行えるかを話し合い\n\n予選までに数回休日に集まっては過去の問題を解いたり、自分たちで考えた予想アプリに対してチューニングをしてみたり、もくもく自分なりのトレーニングをしたりする会を開いていた。\n\nまた、個人的には以下のようなことを行ってきた:\n\n- よく目にしているWebアプリケーションの解析\n    - 普段 Web を閲覧しているときでも、どのようなデータ構造が想定されそれをどうやって KVS に乗せるか、画面の構成要素のうちリアルタイムで作る必要がある場所はどこでキャッシュ可能な場所はどこか、といったことを考えるようにする\n- 速度に対する感覚を養うため、アプリケーションやミドルウェアのチューニングとその際の効果測定\n    - 自分の日頃扱っているWebアプリケーションではネットワークが飽和するようなアクセスはこないため、限界性能を引き出すような使い方はしていないことが多い。「コンテンツキャッシュでさばいてアプリに届かせないのが定石っス！Nginx とか Varnish っていうのがApacheよりいいらしいっス！」みたいなレベルではなく、アプリケーションと Nginx ではどれくらいの性能差が生まれるのか、Nginx を使うのであれば [proxy\\_cache](http://nginx.org/en/docs/http/ngx_http_proxy_module.html) と比べて Unix Domain Socket でMemcached に接続した [SRCache](https://github.com/agentzh/srcache-nginx-module) ではどれくらい単位時間あたりの処理数が変わるのか、別サーバーにある Memcached と TCP 通信した場合や Redis に変えたときではどのように変わるのか、というようなことを考えられるようにしておく  \n      ベンチマーク上の最速を求めるということではなく、どういった選択肢が考えられ、それぞれにどのような性能と機能のトレードオフが発生するのかを体にしみつけておく\n\nどんなアプリケーションがくるかわからない以上、不安がぬぐいさることはできない。この時点でできることは「当日できるだけ早くウィークポイントを見つけること」、「見つけたウィークポイントに対しての効果的な対応をとれるための引き出しを増やしておくこと」であり、そのための準備をひたすら積み重ねてきた。\n\n## 予選\n\n予選問題はチームメンバーの勤め先の会議室を借りて取り組んだ。何度も事前打ち合わせで利用していたので、ストレス無く作業に打ち込めた。ただ、興奮しすぎて当日 2 時間くらいしか眠れていない状態だったため、テンションが高いのに頭があんまり働かなくてだいぶつらかった。\n\n題材はコードスニペット投稿アプリケーション、いわゆる Nopaste や Pastebin と呼ばれるたぐいで [Gist](https://gist.github.com/) を想像してもらうとわかりやすいと思う。ログインがありセッションが発生すること、公開/プライベートのフラグがあることがこれまでの題材との大きな違いであったが、ある程度予想の範囲内であったことと後述するとおりベンチマークの抜け道に気づけたので、すぐにスコアを伸ばすことができた。\n\n主な施策は以下の通り:\n\n- インフラ面\n    - Varnish の配置\n        - ページキャッシュ。Cookie の値を参照し、ユーザー別にキャッシュを管理することでログイン中のユーザーもキャッシュ対象となるよう設定\n- アプリ面\n    - markdown -\u003e html の事前変換\n         - 全件 markdown -\u003e html 化したテーブルの dump データを用意しておき、DBクリア後にリストアしようとした  \n           リストア時間が時間が40秒くらいかかり初期化制限時間60秒をだいぶ圧迫すること(\\*)、途中から初期データはほぼページキャッシュでさばけるようになっていたので、ここに力をかけるのは無駄だと判定して採用しなかった\n             - (\\*) init の間に /var/lib/mysql をまるごと差し替えるなり別のDBにアプリを接続するなりで回避できるので、これ自体はぬるい判断だったと思う\n    - 更新のないテーブルのインメモリ化や Cookie 内へユーザー情報を登録などで DB 参照回数の低減\n    - （チームメンバー担当）クエリの組み方やインデックスの追加などのRDB最適化\n        - 今回のアプリの肝となる部分であり、ひたすら調整を行ってもらっていた\n    - （チームメンバー担当）Memcached の参照方法を TCP から Unix Domain Socket に変更\n         - 初期状態では MySQL Memcached Plugin を参照しているので成績が伸びなかったが、こによりトラップを回避してくれた\n\n事後の講評などでも抜け穴があったという話があったが、後日公開された[予選のAMI](http://isucon.net/archives/32971265.html)を利用しても以下のような方法で簡単に確認できる。\n\n1. アプリを  Perl から Ruby に変更 (※ これは単純に自分がRuby に慣れているからで、他の言語では確認していない)\n2. Memcached への接続先を MySQL Memcached plugin (ポート 11211) から本当の Memcached (ポート 11212)に変更しアプリケーション起動\n3. リバースプロキシの Apache を停止\n4. Varnish をインストール\n5. 以下の設定をして Varnish を起動\n\n/etc/varnish/default.vcl\n\n    backend default { .host = \"127.0.0.1\"; .port = \"5000\"; }\n    \n    sub vcl_recv {\n      # POST リクエストやCookieがある(ログイン中)の場合は直接アプリを参照\n      if (req.request != \"GET\" || req.http.Cookie) {\n          return (pass);\n      }\n      return (lookup); # それ以外はキャッシュを探す\n    }\n    \n    sub vcl_hash {\n        hash_data(req.url);\n        hash_data(req.http.host);\n        return (hash);\n    }\n    \n    sub vcl_fetch {\n        set beresp.ttl = 1d; # キャッシュ期間は1日に設定。数字は適当で、とにかく長くしておけばいい\n        return (deliver);\n    }\n\n上記設定で初回 workload=1 で 9000 くらい、2 回目に workload=2 で24000くらいでベンチマークが Fail せずに完走することがわかる。Varnish じゃなくて他のコンテンツキャッシュを利用しても同様。\n\n予選開始直後、Sinatra のアクション単位でリクエスト回数、平均実行時間、ワーストケースの実行時間などの情報がそろえ、いざ Varnish のチューニングを行いはじめたところ、どんなにキャッシュの TTL を伸ばしてもベンチマークが通ってしまうことに気づき思わず吹き出してしまった。\n\nキャッシュで簡単にスコアが伸ばせることに気づいたのがたしかお昼前で、このあと夕方くらいまで 1 位だったと思う。その後トップは手放すこととなったが、最終的に[予選5位](http://isucon.net/archives/32951235.html \"ISUCON 本戦出場者決定のお知らせ : ISUCON公式Blog\")で決勝進出を決めることができた。\n\n### 予選後\n\nどうにも簡単に勝てすぎたので本当の自分の実力がどの程度かの不安が残るかたちとなったが、穴を見つけられたのも実力のうちと考えて本戦へ向けての準備を進める。\n\n予選が割とシンプルで簡単にキャッシュで返せるような作りだったので、本戦はよりアプリケーションよりの対応が求められるはず、そのためにはいままで以上にアプリの状況を把握する能力を高めておく必要がある。\\*stat 系のパフォーマンス調査ソフトに慣れておいたり、ログ解析のやり方探してみたり（[GoAccess](http://goaccess.prosoftcorp.com/) 使ったりだとか）、プロファイリング用のソフトウェアを把握しておいたり、調査用[アクセスカウンタ](https://gist.github.com/f440/7395268) を準備したり、とにかくすぐに分析できるようにしておいた。\n\nまた、予選の際は自分もアプリを改修し、アプリメインのメンバーもインフラ面を考慮してくれていたので、本戦で複数台構成になることにより作業がバッティングすることを懸念された。これについては事前にチーム内でそれぞれにメインとなる作業をお願いして競合しないよう取りはからった。\n\n\n## 本戦\n\n予選での経験を踏まえ、前日早めに就寝したお陰で当日は体調的には万全、先着で利用可能なミーティングスペースも確保でき順調な滑り出しだった。\n\n題材は画像投稿版の Twitter で、アップロードした画像ファイルがパブリック/フォロワーのみ/プライベートといった公開範囲に応じて閲覧可能となり、関係するメンバーの投稿が画面に自動反映される SPA (Single Page Application) だった。「ファイルアップロード」「フォロワーのアクティビティがタイムラインに表示」などで、チーム内の感想としては「やっぱりきましたね」、といった感じだったのだが、なによりアプリケーションの出来がよくてびっくりした。\n\n作業用サーバーとしては、アプリケーションが稼働している 1 台と自由に使える 4 台の計 5 台が与えられた。これまでの ISUCON では CentOS 5 系が使われており、データホテルの Web サイトでもホスティングしている VPS の OS CentOS 5.8 となっているので CentOS 5 系が来る可能性も考慮していたのだが、今回は CentOS 6.4 であった。\n\nサーバー受け取り後、バックアップをとりつつアプリケーションのプロファイリングを進めるが、どう見ても画像変換のコストだけが突出していることがわかる。データベースへのアクセスはきわめて短時間で終わっており、ボトルネックではない以上手をかけるのは無駄だと判断した。\n\n\n状況確認を踏まえ、以下のような作業を行っていった:\n\n- 事前に初期データのアイコンおよび投稿画像はリサイズをかけておく\n    - バックアップとしてローカルに全ファイルを転送していたので、そのファイルを変換して本番環境に書き戻し\n        - うっかりリサイズのサイズ間違えたり、そんなに早くない自分のマシン(Macbook Air)で実行していたので、かなりの時間がかかってしまった\n- 画像のうち、誰でも閲覧可能なファイルは Web サーバーから直接配信できるよう、公開領域にシンボリックリンクを作成\n    - 全ユーザーと全ファイルのアクセス権限を組み合わせてリンクを作成し、アクセス権限をDBに問い合わせずにレスポンスを返すというのもアイデアとしてはあったのだが、時間がかかるのでこのようにした\n- （チームメンバー担当）画像変換時にファイルシステムへファイルを保存。また画像表示時に変換済みファイルがないかのチェックを追加\n- （チームメンバー担当）ファイルシステムに保存したタイミングで、リサイズを非同期処理で実施\n    - 負荷が高くなってくるとこの部分が詰まってしまい、変換待ちが大量にたまるのであまり効率的ではなく、不採用\n\n最終的に以下の構成で計測を迎えた。フロント 4 台に Nginx を配置のうえ `try_files` でローカルにファイルがあるかどうかをチェックし、なければバックエンドサーバーに処理を委譲するようになっている。\n\n                    +----------+        +----------------+\n      Benchmark -\u003e  | Web x 4  |  ----  | Web + App + DB |\n                    +----------+        +----------------+\n\n時間がなくてこのようなかたちとなったが、今考えてもこれがよかったかどうかでいえばまったくもってよくなかった。完全にバックエンドのアプリサーバーが負荷でつぶれていたが、全サーバーでアプリを動かせばきれいに台数分スケールしていたはずだったのに、本当に悔やまれる。\n\nタイムアップ時点ではそれほどの成績ではなかったが、本戦のベンチマークを無事乗り越え、気づいたら 2 位という成績で ISUCON 3 の幕を閉じた。\n\n\n## ふりかえって\n\n正攻法ではない方法もいろいろ考えてはいたんだけど、結局のところ他のチームと比べても過激な改修は行わずに済ませた。直しやすそうなところ、目のつきやすいところではなく、手早く確実にウィークポイントを直すというのが目標だったので、それは実現できたんだと思う。\n\nけどやっぱり優勝できなかったのは心残りで、試合終了後や帰宅後のチャット上でもチームメンバーと「非同期のワーカー作る時間の無駄だった」とか「どう考えてもアプリサーバーネックだったし、他の4台でもアプリ動かしてベンチマークのワーカー増やせばトップとれたんじゃないの」とか「Macbook Air でちまちま画像変換するの失敗だった」とか、いろいろ話をしていた。1位があまりに鮮烈で、それ以外の順位は空気みたいな存在だし、やはりこの世はトップ総取りなのだなぁ、と痛感した。\n\n毎回このような場を用意していただいている LINE 社、データホテル社の方々には感謝の限りです。お弁当おいしゅうございました。出題のカヤックの皆さんもすばらしい問題をありがとうございました。\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-08-10-immutable-infrastructure.markdown","path":"blog/2013/08/10/immutable-infrastructure","layout":"post","title":"Immutable Infrastracture について","createdAt":"2013-08-10T12:00:00.000Z","kind":"article","comments":true,"tags":["infrastructure","virtualization","aws"],"content":"\nここ最近話題に上がることが多い Immutable Infrastracture と、その他仮想環境周りについての雑感。\n\n\u003c!-- more --\u003e\n\nImmutable Server や Immutable Infrastracture っていう単語がいろんなところで目に入るようになった。とくに Chad Fowler が[ブログで取り上げたり](http://chadfowler.com/blog/2013/06/23/immutable-deployments/)、[Food Fight に出たり](http://foodfightshow.org/2013/07/immutable-infrastructure.html) して、世間でも関心が高まった感じがある。\n\nプログラムを書く人にはご存じの通り、この Immutable っていうのは状態が変更出来ないことを指している。Immutable な Infrastracture っていうのは、ざっくり言うと「運用中のサーバーに変更を加えない」っていうアプローチでサーバーを管理しているスタイルのこと。\n\n(ファイルシステムを読み取り専用にする、とかそういう話じゃなくて、あくまでそういう方針でやろうっていう話)\n\nサーバーの設定を変更したくなったら、その変更を加えた新しいサーバーを用意する。アプリケーションをデプロイしたくなったら、新しくサーバーを立ち上げてそちらにデプロイを行う。稼働中のサーバーに SSH でログインして設定を変更するようなことはせず、なにかしらの変更のためにはつねにサーバーを追加していく。サービスを新サーバー群にで行うように DNS を切り替えたあと、参照されなくなったサーバーは破棄する(まるでガベージコレクションみたいだね)。\n\n\n## どうやって Immutable Infrastracture を実現するのか\n\nImmutable Infrastracture には、いわゆる [Blue Green Deployment](http://martinfowler.com/bliki/BlueGreenDeployment.html) で知られているテクニックを用いる。\n\n現在、プラットフォームとして上手に Immutable Infrastracture を実現できているのは [AWS Elastic Beanstalk](http://aws.amazon.com/jp/elasticbeanstalk/) だと思っているので、これを例に説明する。Elastic Beanstalk を「あー、AWS がやってる heroku 的なアレだろ」くらいの認識しかなければ、一度ちゃんと調べてみたほうがいい。\n\nElastic Beanstalk では、ロードバランサーとそこにぶら下がるサーバー群 (オートスケールするので、台数は伸び縮みする) が「環境(Envirnment)」というくくりで管理される。\n\nサーバーには Amazon が用意したマシンイメージを使うこともできるし、カスタマイズしたイメージを利用することも可能になっている。アプリケーションのデプロイは git push だったり、Java の war ファイルアップロードでできるので、サーバーにログインする必要はない。\n\n「環境」にはそれぞれ URL が割り振られるのだが、これは環境間ですげ替えることができる。つまり、検証環境でアプリをデプロイしたりミドルウェアの設定変更をして、確認がとれたら本番環境と入れ替えたり、問題が起きたらすぐに元に戻したりといったことがダウンタイムなく行える。\n\nNetflix のデプロイツール [asgard](https://github.com/Netflix/asgard) も同じようなことをしているし、Heroku の [preboot](https://devcenter.heroku.com/articles/labs-preboot/) も内部では同じようなことやってるんじゃないかな。(追記: Heroku の Preboot だけど、説明ページへのリンクがなくなっている。今は [Pipelines](https://devcenter.heroku.com/articles/labs-pipelines) 使えってことかも)\n\nもちろん、オンプレミスに自前の環境でこういったことを行うことも可能だろうけど、アプリケーションの切り替えに DNS の設定変更なり浮動 IP アドレスの付け替えなりが必要となってくるので、かなり面倒くさい。すでにシステムとして提供されているものを利用できるのであれば、それを使うのが現実的だとは思う。\n\n\n## 構成管理ツールの役割\n\n設定を変更するためだけに新しいマシンを作るだなんて、なんでそんなことをするのだろうと Chef や Puppet などのツールを使って変更管理している人たちは不思議に思うかもしれない。発想を逆にしてみると、仮想マシンが状態を持っているから冪等性だとか自己修復性を考慮したセットアップツールが必要になる。仮想マシンが不変だという前提にたてば、こういった処理が省けるようになるのかもしれない。\n\nとはいえ、Immutable Infrastracture を実践したとしても構成管理ツールは以下のような局面で今後も使われていくことになるだろうと思う。\n\n- ベースとなる仮想マシンのセットアップのため\n- 初回起動後の仮想マシンに対して、(仮想マシンに組み込めない or 組み込みたくない) マシンごとの変更を設定するため\n\n個人的には、今の Chef や Puppet みたいなサーバー/クライアント構成は必要なくて、もっとライトウェイトなもので十分な気がしている。\n\n\n## 仮想マシンの役割\n\n昨今の構成管理ツールブームで、サーバーセットアップの技術が成熟してきた。こういったツールをソフトウェアでいうところの autotools や ant といったビルドツールにたとえるなら、次の興味は apt や rpm といったパッケージにあたるもの、つまりセットアップ済み仮想マシンになるかと思う。\n\n- Packer は仮想マシンの作成手順や作成先を抽象化しようとしている\n- Docker は仮想マシンをまるで Github から clone するかのように共有する方法を提供している\n- AWS MarketPlace は個人/企業に仮想マシンを売り買いできる仕組みを提供している\n\n最近のプロダクトやサービスを考えてみても、仮想マシン自体の取り扱いにだんだん関心がむかっているのは確かっぽい。Immutable Infrastracture もまた、仮想マシンを仮想マシンらしく扱った運用形態といえるんじゃないかな。\n\n\n## 参考\n\n- [ImmutableServer](http://martinfowler.com/bliki/ImmutableServer.html)\n- [Rethinking building on the cloud: Part 4: Immutable Servers](http://www.thoughtworks-studios.com/blog/rethinking-building-cloud-part-4-immutable-servers)\n- [Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components - Chad Fowler](http://chadfowler.com/blog/2013/06/23/immutable-deployments/)\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-08-10-octopress-to-middleman.markdown","path":"blog/2013/08/10/octopress-to-middleman","layout":"post","title":"ブログエンジンを Octopress から Middleman に変えた","createdAt":"2013-08-10T11:30:00.000Z","kind":"article","comments":true,"tags":["octopress","middleman"],"content":"\nこのサイトは今まで [Octopress] を使って生成していたんだけど、[Middleman] に変えてみた。\n\n\u003c!-- more --\u003e\n\n元々 Octopress の設定ファイルの書き方とかがモヤモヤするものがあって(Rakefile に設定項目埋め込んであるところとか) Jekyll にしようかなと思ったんだけど、なんとなく Middleman にしてみた。その後の感想など。\n\n(もう2ヶ月くらい前の話なので、移行当時の記憶はおぼろげ)\n\n- ビルド時間が短くなった\n  - 素の jekyll はビルド早いんだけど、Octopress は結構遅いんで気になっていた。middleman は Octopress よりかは早い\n- ブログの内容はほぼそのまま使い回せた\n  - メタデータ部分を s/categories/tags/ で置換したくらいだったと思う\n- 公開されているテンプレートが少ない\n  - 自分でちまちま書いてる\n- middleman-livereload が便利\n  - [rack-livereload](https://github.com/johnbintz/rack-livereload) を使っている。ブラウザに拡張を入れなくても、Web Socket でリロードしてくれる。\n- [tilt] を使っているので、テンプレートエンジンは自由に選択できる\n  - 楽しくなって、[Slim](http://slim-lang.com/) を使って [shower](https://github.com/shower/shower) を [移植してみたりした](https://github.com/f440/middleman-miwer)。\n\nおおむね満足です。\n\n## 参考\n- [Octopress][]\n- [Middleman][]\n- [tilt][]\n\n[Octopress]: http://octopress.org/\n[Middleman]: http://middlemanapp.com/\n[tilt]: https://github.com/rtomayko/tilt\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-06-17-docker.markdown","path":"blog/2013/06/17/docker","layout":"post","title":"仮想環境構築に docker を使う","createdAt":"2013-06-17T00:13:00.000Z","kind":"article","comments":true,"tags":["docker","lxc"],"content":"\nちょっと前から [Docker][] を使っているので、その話。\n\n\u003c!-- more --\u003e\n\n## Dockr について\n\n[Docker][] は [dotcloud][] がオープンソースで公開している、コンテナ技術による仮想化ソフトウェア。\n\n以下のテクノロジーベースにしている:\n\n- [LXC](http://lxc.sourceforge.net/)\n  - [前にも書いた](/blog/2012/05/13/vps-lxc-xtradb-cluster/)。Xen とか VirtualBOX みたいにホスト内に仮想マシンを立ち上げるんじゃなくて、ホスト内の隔離された環境で仮想マシンを動かす技術。物理マシンをシミュレーションしているんじゃないってことは、VPS とか EC2 とかの仮想マシン上でも問題なく動くし、マシンを起動するプロセスが不要となるので、一瞬で使い始められるというメリットにつながっている。\n- [AUFS](http://aufs.sourceforge.net/)\n  - UnionFS(ディレクトリを重ね合わせることができる)の実装の一つ。元の仮想マシンイメージを書き換えないで、更新が発生した部分は別の場所に書き込んでいくようになっている。これにより、仮想マシンの立ち上げ時にイメージのコピーが発生しないので、すぐに使い始められる。\n\nDocker を使う前は LXC のラッパーとして取っつきにくさを緩和してくれる、とかそういうレベルだと思ったんだけど、予想はよい方向に裏切られた。\n\n[仮想マシンのイメージを可視化したもの](http://docs.docker.io/en/latest/commandline/command/images/)を見ると、まるで Git のコミットログみたいに見えると思う。実際、情報は差分で管理され、履歴を残したり分岐させたりといった操作が非常に軽量にできていて、Git を操作するかのように仮想マシンを操作できるようになっている。\n\n\n## 動かし方\n\nArch Linux や Debian で動かしている人がいるみたいだけど、公式サポートは今のところ Ubuntu のみ。Ubuntu 12.04 LTS を使っているのであれば、`curl get.docker.io | sh -x` で動くようになる。\n\nちゃんとしたやり方は [ドキュメント](http://docs.docker.io/en/latest/installation/)を見れば、特にはまることもないと思う。できるだけ新しい Ubuntu を使っておけばいい。\n\nすぐに試してみたいんなら、Vagrant 経由で簡単に使い始められる。\n\n    git clone https://github.com/dotcloud/docker.git\n    cd docker\n    vagrant up --provider virtualbox # or vagrant up --provider aws\n\n\n## 基礎的な操作方法\n\nインストールがうまくいって Docker が起動しているものとして、早速使ってみる。\n\n    $ docker\n    Usage: docker [OPTIONS] COMMAND [arg...]\n      -H=\"127.0.0.1:4243\": Host:port to bind/connect to\n\n      A self-sufficient runtime for linux containers.\n\n      Commands:\n      attach    Attach to a running container\n      build     Build a container from a Dockerfile\n      commit    Create a new image from a container's changes\n    (以下省略)\n\nコマンドがずらっと表示されるかと思う。まずは単発のコマンドをコンテナ内で実行してみる。\n\n    $ docker run base /bin/echo hi\n    Pulling repository base from https://index.docker.io/v1\n    Pulling image b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc (latest) from base\n    Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc metadata\n    Pulling b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc fs layer\n    Downloading 10240/? (n/a)\n    Pulling 27cf784147099545 metadata\n    Pulling 27cf784147099545 fs layer\n    Downloading 94863360/? (n/a)\n    Pulling image 27cf784147099545 () from base\n    hi\n\n「`docker` コマンドに run サブコマンドを指定して、`base` という仮想マシンで `/bin/echo hi` コマンドを実行する」という意味になる。仮想マシンがダウンロードされるが、これは初回実行時のみ。最後に表示された「hi」というのが今回の実行結果で、このコンテナの役割はこれで終わり。\n\n今度は作ったマシンの中に入ってみるために、`-i` と `-t` オプションで入出力できるようにして `/bin/bash` を起動してみる。\n\n    $ docker run -i -t base /bin/bash\n    root@bc43a290f0ce:/#\n\n端末から抜けるとホスト側に制御が戻る。\n\n    root@bc43a290f0ce:/# exit\n    exit\n    $\n\n今度は `-d` オプションでコマンドを実行しっぱなしにする。\n\n    $ docker run -i -t -d base /bin/ping -i 5 www.aikatsu.net\n    79365b2985c4\n    $\n\nID が返されて、すぐに端末が利用可能になる。稼働中のプロセスを確認してみる。\n\n    $ docker ps\n    ID                  IMAGE               COMMAND                CREATED             STATUS              PORTS\n    79365b2985c4        base:latest         /bin/ping -i 5 www.a   22 seconds ago      Up 21 seconds\n\n次に実行中の出力をのぞいてみよう。\n\n    $ docker logs 79365b2985c4\n    PING www.aikatsu.net (60.32.7.37) 56(84) bytes of data.\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=1 ttl=49 time=282 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=3 ttl=49 time=278 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=4 ttl=49 time=283 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=5 ttl=49 time=266 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=6 ttl=49 time=268 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=8 ttl=49 time=264 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=9 ttl=49 time=270 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=10 ttl=49 time=290 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=11 ttl=49 time=284 ms\n\n順調に動き続けているようなので、このジョブにアタッチしてみる。\n\n    $ docker attach 79365b2985c4\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=18 ttl=49 time=239 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=19 ttl=49 time=291 ms\n    64 bytes from www3.sunrise-anime.jp (60.32.7.37): icmp_req=20 ttl=49 time=275 ms\n    (出力が続く)\n\nアタッチ中の端末は `Ctrl-p Ctrl-q` でデタッチできる。(このとき use of closed network connection っていうエラーが出る場合 Ctrl-c で抜けるしかないっぽい。バグレポートは上がっているので、じきに直ると思う。)\n\n最後に`kill`でこのプロセスを消してみる。\n\n    $ docker kill 79365b2985c4\n    $ docker ps\n    $\n\n`ps`からプロセスが消えた。基礎的なコンテナの操作の説明は以上。\n\n## 詳細\n\n### コンテナ\n\nこれまでコマンドを実行したり、`kill` されたコンテナはどうなっているのか。実は全部残っている。停止したコンテナを表示するために`-a`をつける。ついでに、情報を省略しないで表示するために`-notrunc` もつける。\n\n    $ docker ps -a -notrunc\n    ID                                                                 IMAGE               COMMAND                          CREATED             STATUS              PORTS\n    79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2   base:latest         /bin/ping -i 5 www.aikatsu.net   14 minutes ago      Exit 0\n    bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503   base:latest         /bin/bash                        16 minutes ago      Exit 0\n    7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971   base:latest         /bin/echo hi                     16 minutes ago      Exit 0\n    8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb   base:latest         /bin/bash                        21 minutes ago      Exit 0\n    4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a   base:latest         /bin/echo hi                     21 minutes ago      Exit 0\n\n正常終了しているので、すべて`Exit 0`になっている。また、ID は省略表記されていたこともわかる。コンテナの実体は `/var/lib/docker/containers/\u003cID\u003e` 以下に格納されている。\n\n    $ sudo ls /var/lib/docker/containers/\n    4637bc6341706c25e066c5ccfe92e10c923bfe4955a9e8b3ce07237fda0fb34a\n    79365b2985c43a2a6977764f4dde2d375084020fbc04cc855508c417a36f88c2\n    7a666192cca72cea81cade398b22700c982fbb9271a7eca23ff51c6c504d5971\n    8b0af4fc390d762c33dadc1b149516ba95bdb70d093e991ec2df563817f55ffb\n    bc43a290f0ced4677ee7eb1a0d662cca496cc720d8db20e746dda45e4659f503\n\nどんどんたまっていくから心配かもしれないけど、各コンテナはベースイメージからの差分しかもたないので、問題にならない。もし、消したくなったら `docker rm \u003cコンテナのID\u003e` で消せる。\n\n作業領域であったコンテナを `commit` するとイメージとして使い回せるようになる。`ユーザー名/名称`にするのが作法っぽい。\n\n    $ docker commit -m \"My first container\" 4637bc634170 f440/first_container\n    02036952e5dc\n    $ docker images\n    REPOSITORY             TAG                 ID                  CREATED\n    base                   latest              b750fe79269d        12 weeks ago\n    base                   ubuntu-quantl       b750fe79269d        12 weeks ago\n    base                   ubuntu-quantal      b750fe79269d        12 weeks ago\n    base                   ubuntu-12.10        b750fe79269d        12 weeks ago\n    f440/first_container   latest              02036952e5dc        3 seconds ago\n\nこれで今後は `docker run f440/first_container` をベースにしたコンテナを作れるようになる。\n\n### イメージ\n\nもう一回イメージの一覧を内容を確認してみよう。\n\n    $ docker images\n    REPOSITORY             TAG                 ID                  CREATED\n    f440/first-container   latest              141fef9a2f57        14 seconds ago\n    base                   latest              b750fe79269d        12 weeks ago\n    base                   ubuntu-12.10        b750fe79269d        12 weeks ago\n    base                   ubuntu-quantl       b750fe79269d        12 weeks ago\n    base                   ubuntu-quantal      b750fe79269d        12 weeks ago\n\nbase イメージは latest, ubuntu-quantl, ubuntu-quantal, ubuntu-12.10 といった複数のタグがついていることがわかる。イメージは複数の名称をタグ付けできるようになっており、`base:latest`, `base:ubuntu-12.10` といった形で異なるイメージを呼び出せるようになっている。省略時は `base:latest` と同じ。\n\npull してくるイメージは [https://index.docker.io/](https://index.docker.io/) から情報を持ってくる。コマンドラインで検索したい場合は `search` コマンドを利用する。\n\n    $ docker search centos\n    Found 4 results matching your query (\"centos\")\n    NAME                          DESCRIPTION\n    centos\n    backjlack/centos-6.4-x86_64\n    creack/centos\n    mbkan/lamp                    centos with ssh, LAMP, PHPMyAdmin(root pas...\n\nローカルにキャッシュされたイメージを消すには `docker rmi \u003cイメージのID\u003e`でいい。\n\n自前で作ったイメージを [https://index.docker.io/](https://index.docker.io/)  に登録するには、あらかじめサイト上でアカウントを作っておき、 `docker login` した後に `docker push` する。イメージ名にアンダーバー使っていると `push` で失敗するのと、アップロードしたイメージを消す機能がまだなかったりするので注意。\n\nイメージの実体は `/var/lib/docker/graph/` にある。\n\n    $ docker images -a -notrunc\n    REPOSITORY          TAG                 ID                                                                 CREATED\n    base                latest              b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-12.10        b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-quantl       b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    base                ubuntu-quantal      b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc   12 weeks ago\n    \u003cnone\u003e              \u003cnone\u003e              27cf784147099545                                                   12 weeks ago\n\n    $ sudo ls -1 /var/lib/docker/graph\n    141fef9a2f57e86dd6d9aa58fe9318b0d9d71d91053079842051d9738bad6e45\n    27cf784147099545\n    b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\n    checksums\n    :tmp:\n\nここで images に ID: 27cf784147099545 というのが現れた。これは何か。`inspect` を使うとイメージの詳細を表示できる。\n\n    $ docker inspect base\n    {\n        \"id\": \"b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\",\n        \"parent\": \"27cf784147099545\",\n        \"created\": \"2013-03-23T22:24:18.818426-07:00\",\n        \"container\": \"3d67245a8d72ecf13f33dffac9f79dcdf70f75acb84d308770391510e0c23ad0\",\n        \"container_config\": {\n            \"Hostname\": \"\",\n            \"User\": \"\",\n            \"Memory\": 0,\n            \"MemorySwap\": 0,\n            \"CpuShares\": 0,\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"PortSpecs\": null,\n            \"Tty\": true,\n            \"OpenStdin\": true,\n            \"StdinOnce\": false,\n            \"Env\": null,\n            \"Cmd\": [\n                \"/bin/bash\"\n            ],\n            \"Dns\": null,\n            \"Image\": \"base\",\n            \"Volumes\": null,\n            \"VolumesFrom\": \"\"\n        }\n    }\n\nID: 27cf784147099545 は base イメージの親イメージの ID であることがわかる。イメージは差分になっているので、親のイメージが必要ということで初回実行のタイミングで base と一緒に 27cf784147099545 もダウンロードされていたのだった。\n\n### ネットワーク\n\n`docker run` 時に `-p` をつけることで、コンテナから外部にさらすポートを決められる。コンテナ側のポートはホスト側のポートに変換される際、ポート番号が変更される(49153以降になる)ので、`docker port \u003cジョブのID\u003e \u003cポート番号\u003e` あるいは `docker ps ` でポートの対応状況を確認する必要がある。\n\nドキュメントの [Expose a service on a TCP port](https://github.com/dotcloud/docker#expose-a-service-on-a-tcp-port) がわかりやすい。\n\n    # 以下、コメントは書き換えてある\n    # また、途中経過がわかりやすいように set -x しておく\n    set -x\n\n    # 4444 を晒すよう -p オプションをつけて docker run しつつ、\n    # コンテナは netcat で4444を待ち受ける\n    JOB=$(docker run -d -p 4444 base /bin/nc -l -p 4444)\n    ++ docker run -d -p 4444 base /bin/nc -l -p 4444\n    + JOB=c86c892574f7\n\n    # 4444 がローカルのどのポートに対応するのか確認\n    # docker ps でも調べることはできる\n    PORT=$(docker port $JOB 4444)\n    ++ docker port c86c892574f7 4444\n    + PORT=49166\n\n    # ルーティングによっては localhost とか 127.0.0.1 だと\n    # うまくいかないことがあるので、eth0 のIPアドレスを使おう、\n    # ってことらしい\n    IP=$(ifconfig eth0 | perl -n -e 'if (m/inet addr:([\\d\\.]+)/g) { print $1 }')\n    ++ perl -n -e 'if (m/inet addr:([\\d\\.]+)/g) { print $1 }'\n    ++ ifconfig eth0\n    + IP=10.156.137.111\n    echo hello world | nc $IP $PORT\n    + nc 10.156.137.111 49166\n    + echo hello world\n\n    # コンテナが受信したメッセージを logs で表示\n    echo \"Daemon received: $(docker logs $JOB)\"\n    ++ docker logs c86c892574f7\n    + echo 'Daemon received: hello world'\n    Daemon received: hello world\n\n### Dockerfile\n\nDSLで書かれた設定(通常ファイル名は`Dockerfile`とする)をあらかじめ用意することで、手順に従ってイメージを作ることができる。\n\n    読み込ませ方 (1)\n    docker build \u003cDockerfileのあるディレクトリ\u003e\n    # ex. docker build .\n\n    読み込ませ方 (2)\n    docker build -\n    # ex. docker build - \u003c /foo/bar/Dockerfile\n\nDockerfile の例\n\n    FROM base\n    RUN /bin/echo hi\n\nこれで、`docker build` すれば `docker run base /bin/echo hi` と同じ効果が得られる。\n\n指定できるはコマンドは以下の通り。大文字小文字は区別しないけど、引数と見分けやすいように大文字が使われる。\n\n- `FROM \u003cimage\u003e` ベースとなるイメージを指定\n- `MAINTAINER \u003cname\u003e` メンテナの名前を指定\n- `RUN \u003ccommand\u003e` ビルド中に実行したいコマンドを指定\n- `CMD \u003ccommand\u003e` 起動後のコンテナで実行したいコマンドを指定\n- `EXPOSE \u003cport\u003e [\u003cport\u003e ...]` 外部に晒すポートの指定\n- `ENV \u003ckey\u003e \u003cvalue\u003e` 環境変数の設定\n- `INSERT \u003cfile url\u003e \u003cpath\u003e` deprecated なので ADD を利用すること\n- `ADD \u003csrc\u003e \u003cdest\u003e` ファイルを配置\n\n`RUN` と `CMD` の違いがわかりにくいかもしれない。例を出す。\n\n    # RUN, CMD で指定したコマンドが実行されたとき、\n    # 標準出力と /tmp/*.log に記録を残す\n\n    $ cat \u003c\u003cSCRIPT \u003eDockerfile\n    \u003e FROM base\n    \u003e RUN /bin/echo run | tee /tmp/run.log\n    \u003e CMD /bin/echo cmd | tee /tmp/cmd.log\n    \u003e SCRIPT\n\n    # ビルドの実行\n\n    $ docker build .\n    Caching Context 10240/? (n/a)\n    FROM base ()\n    ===\u003e b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\n    RUN /bin/echo run | tee /tmp/run.log (b750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc)\n    ===\u003e d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\n    CMD /bin/echo cmd | tee /tmp/cmd.log (d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d)\n    ===\u003e 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\n    Build successful.\n    ===\u003e 60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\n\n    # run, cmd の実行結果を確認\n    # =\u003e run だけが実行されている\n\n    $ docker run 60671e99691 /bin/ls /tmp/\n    run.log\n\n    # イメージを inspect する\n    # =\u003e どうやらコンテナは記憶していることがわかる\n\n    $ docker inspect 60671e99691\n    {\n        \"id\": \"60671e9969185841032fb02f623917672c4f871a6be68e5aa575e8fdf1f94229\",\n        \"parent\": \"d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\",\n        \"created\": \"2013-06-16T16:29:14.602237Z\",\n        \"container\": \"4c54683cec90500f329dfaad2e0856cc408483be0ae3166018121d4d4b9b3282\",\n        \"container_config\": {\n            \"Hostname\": \"78c72f8ba6ad\",\n            \"User\": \"\",\n            \"Memory\": 0,\n            \"MemorySwap\": 0,\n            \"CpuShares\": 0,\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"PortSpecs\": null,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": null,\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) CMD [/bin/sh -c /bin/echo cmd | tee /tmp/cmd.log]\"\n            ],\n            \"Dns\": null,\n            \"Image\": \"d10b6bd1321d45b0228b5741c01d1f76fd0288052e56836609f9bdf217854f3d\",\n            \"Volumes\": null,\n\n    # 引数でコマンドを指定せずに run を実行\n    # =\u003e cmd で登録した内容が実行される\n\n    $ docker run 60671e99691\n    cmd\n\nつまり、`RUN` は `Dockerfile` を元にビルドしているときに参照され、`CMD` はコンテナを実行する際に参照されるということがわかる。パッケージをインストールしたりといった用途では通常 `RUN` を使う。\n\n## まとめ\n\n仮想環境の発達でプログラマブルなインフラストラクチャーは実現できてきているけど、マシンを上げたり下げたりするのにどうしても時間がかかるし、それは仕方が無いものと我慢していた。`Docker` を使ってみると、今までのそういった不満から解放されることができそう。一応開発中というステータスなのでプロダクション環境では使いづらいけど、開発やテスト、とくに構成管理ツールを設定するときなどは、この俊敏性、柔軟性は有効になると思う。\n\n## 参考\n\n- [Documentation](http://docs.docker.io/en/latest/)\n\n\n[dotCloud]: https://www.dotcloud.com/\n[Docker]: http://www.docker.io/\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-06-16-yum-cache.markdown","path":"blog/2013/06/16/yum-cache","layout":"post","title":"yum のパッケージキャッシュについて","createdAt":"2013-06-15T16:26:00.000Z","kind":"article","comments":true,"tags":["yum"],"content":"\n`/etc/yum.conf`で`keepcache=1`にしておくと、インストールしたパッケージがキャッシュされるようになる。これが無効化された状態だと、パッケージアップグレード時に問題が起きても元に戻せなくなるので有効化しておいた方がいい。\n\n\u003c!-- more --\u003e\n\nあるパッケージについて、どのバージョンが利用可能な状態かは以下で確認できる。\n\n    $ sudo yum --showduplicates list パッケージ名\n\nRHEL なら過去のバージョンまですべて手に入るけど、CentOS だとOSリリース時のバージョンと最新版しか手に入らない模様。リポジトリ上なりキャッシュなりで過去のバージョンが手に入るのであれば、`yum install` や `yum update` は以下の手順でロールバックが行える。\n\n    # yum の利用履歴を確認\n    $ sudo yum history\n\n    # 履歴から詳細を確認\n    # 未引数なら直近、引数ありなら該当する ID を表示\n    $ sudo yum history info 4\n\n    # 仮に ID 4 で問題のバージョンアップが行われたようだということが確認できたら、その ID を指定して操作をアンドゥ\n    $ sudo yum history undo 4\n\nアンドゥ(リドゥもある)では、対象パッケージおよび依存パッケージがまとめて一度に入れ替えられる。これはパッケージの操作がちゃんとトランザクションになっているため。\n\n話がそれるけど、パッケージの操作にトランザクションがかかるというのはかなり重要だ。たとえば syslog-ng から rsyslog に入れ替えるとき、単純にアンインストール、インストールの順番でやろうとするとアンインストールのタイミングで大量の Syslog 依存なパッケージが道連れになるけど、以下のようにすればひとつのトランザクションでパッケージを入れ替えることができる。(情報源: [Rsyslog Wiki](http://wiki.rsyslog.com/index.php/Install_rsyslog_with_yum))\n\n    $ sudo yum shell\n    \u003e remove syslog-ng\n    \u003e install rsyslog\n    \u003e run\n\n話がそれたついでにふれておくと、vagrant を使っているのであれば [vagrant-cachier](https://github.com/fgrehm/vagrant-cachier) を使うとパッケージのキャッシュ保存先を仮想マシン外の領域(ホストOSとの共有ディスク部分など)に変更してくれる。こうすることで、仮想マシンを破棄してもパッケージのキャッシュが永続化されるため、2回目以降はダウンロードがスキップされて高速化する。\n\n話を戻すと、世の中何が起きるかわからないので古いパッケージもとっておいたほうがいいかと。ディスク容量が気になりだしたら、`yum clean packages` を実行すればキャッシュは消せる。\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-05-07-create-ominibus-installer.markdown","path":"blog/2013/05/07/create-ominibus-installer","layout":"post","title":"omnibus を使って オムニバスインストーラーを作成する","createdAt":"2013-05-06T16:41:00.000Z","kind":"article","comments":true,"tags":["packaging"],"content":"\nChef のインストールは結構面倒くさかったんだけど、[オムニバスインストーラー][Install CHef]が出たことで状況はがらっと変わって、簡単に導入できるようになった。このオムニバスインストーラーの仕組みは汎用的に作られているので、他のツールでも適用できるという話。\n\n\u003c!-- more --\u003e\n\n## オムニバスインストーラーについて\n\nChef のオムニバスインストーラーを実行すると以下のようなディレクトリ構成でファイルが置かれる:\n\n- /opt/chef/bin/ ... Chef 関連のスクリプト\n- /opt/chef/embedded/ ... ruby インタプリタ、Chef とその他依存パッケージ\n- (/usr/bin/ ... /opt/chef/bin/ 以下のものがシンボリックリンクが配置される)\n\n以上の通り、`/opt/chef` の中に動作に必要なものがごっそり置かれる。アプリケーションレベルでプログラミングの処理系を持っちゃうというのはこれに限らずよく見る光景で、理由としてはパッケージ提供されていない最新版が使いたかったり、バージョンアップやライブラリインストールの影響範囲を限定させたかったりだと思う。\n\nここしばらくは手軽なパッケージ作成ツールとして[fpm][]がよく使われているけど、オムニバスインストーラーは[omnibus][]という「ビルドツール」＋「fpm ラッパー」といった感じのもので作られている。以下は実際に [omnibus][] を使ったインストーラー作成の手順についてまとめる。\n\n## パッケージ作成\n\n[statsd][] および [statsd][] を動かすために必要な Node.js を /opt/statsd にインストールする RPM, Deb パッケージの作成を行ってみる。\n\n### 環境\n\n- Macbook Air Mountain Lion\n- Ruby 2.0.0-p0\n- Vagrant 1.2.2\n\n### 手順\n\n    # omnibus のインストール\n    gem install omnibus\n\n    # 必要となる vagrant 用の plugin をインストール\n    vagrant plugin install vagrant-omnibus\n    vagrant plugin install vagrant-berkshelf\n\n    # プロジェクトディレクトリの作成(ディレクトリ名は `omnibus-プロジェクト` となる)\n    omnibus project statsd\n    cd omnibus-statsd\n\n    # プロジェクトディレクトリ内のファイルを適宜修正:\n        Berksfile\n          Berkshelf 用の設定。変更する必要無い。\n        Vagrantfile\n          Vagrant 用の設定。2013-06-07 現在だと CentOS 5, 6 Ubuntu 10.04, 11.04, 12.04 の設定が導入済み。\n        README.md\n        omnibus.rb.example\n          成果物を S3 上にキャッシュする場合などに利用。使わないなら気にしなくていい。\n        config/projects/statsd.rb\n          後述\n        config/software/*\n          後述\n        package-scripts/statsd/*\n          インストール時、アンインストール時などに実行したいスクリプトなど。\n\nこの中で、実際のビルドプロセスを定義するのは、config/projects/ 以下と config/software 以下になる。\n\n`config/projects/` はプロジェクトの設定を格納するディレクトリで、初期状態では statsd 用のプロジェクトファイル `config/projects/statsd.rb` が作られている。このファイルを修正していくことになる。\n\n    name \"statsd\"\n    maintainer \"f440\"\n    homepage \"https://github.com/f440/omnibus-statsd\"\n\n    install_path    \"/opt/statsd\"\n    build_version   \"0.6.0\"\n    build_iteration 1\n\n    dependency \"preparation\"\n    dependency \"node\"\n    dependency \"statsd\"\n\n    exclude \"\\.git*\"\n\nおおむね想像がつく名前だけど、dependency だけはよく分からないと思う。dependency で指定したものはプロジェクトを構成する software という扱いで、`config/software/` 以下でその設定を行っていく。\n\nsoftware の例を示す。典型的な例だと、指定した URL からダウンロードしてきたものを一時ディレクトリで展開して、`configure \u0026\u0026 make \u0026\u0026 make install` を実行、などだが今回の作業では Node.js のバイナリを展開して `/opt/embedded` 以下にコピーしているだけである。\n\n    name \"node\"\n    version \"0.10.5\"\n\n    source :url =\u003e \"http://nodejs.org/dist/v0.10.5/node-v0.10.5-linux-x64.tar.gz\",\n           :md5 =\u003e \"fb65723d395c559393201dd41e0eb275\"\n\n    relative_path \"node-v0.10.5-linux-x64\"\n\n    build do\n      command \"rsync -av . #{install_dir}/embedded/\"\n    end\n\n\n必要となる software の設定を全部そろえたらビルドを実行する。マシンの起動、Chef のインストール、omnibus の Cookbook 実行、ビルド環境構築、ビルド実行、パッケージ作成 といったことが行われることになるため、初回はかなり待つことになる。\n\n    vagrant up\n    (vagrant up centos-6 など、直接マシンを指定してもいい)\n    (もし Linux 上で作業しているのであれば、omnibus build project statsd で直接パッケージ作成を開始出来る)\n\n問題なければ、pkg/ 以下に statsd-0.6.0-1.el6.x86_64.rpm, statsd_0.6.0-1.ubuntu.12.04_amd64.deb といったファイルが出来る。\n\n## まとめ\n\nやっていることは [fpm][] でパッケージを作っているだけなんだけど、[Vagrant][] x [Berkshelf][] x [Chef][] のコンビネーションのおかげで、パッケージとそのパッケージを作るための環境が簡単に手に入るのはとてもいい。複数環境のパッケージを作る予定がなくっても、最初から[omnibus][]上でパッケージを作れるようにしておくと運用が楽そう。\n\n## 備考\n\n似たようなツールとして、[bunchr][] が存在する。\n\n## 参考\n\n- [Statsd]\n- [Install Chef]\n- [omnibus]\n- [bunchr]\n- [fpm]\n- [Vagrant]\n- [Berkshelf]\n\n[Statsd]: https://github.com/etsy/statsd\n[Install Chef]: http://www.opscode.com/chef/install/\n[omnibus]: https://github.com/opscode/omnibus-ruby\n[Chef]: http://www.opscode.com/chef/\n[Berkshelf]: http://berkshelf.com/\n[Vagrant]: http://www.vagrantup.com/\n[bunchr]: https://github.com/joemiller/bunchr\n[fpm]: https://github.com/jordansissel/fpm\n[sensu]: https://github.com/sensu\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-05-03-create-mesos-rpm-using-fpm.markdown","path":"blog/2013/05/03/create-mesos-rpm-using-fpm","layout":"post","title":"fpm で Mesos の RPM を作るまで","createdAt":"2013-05-03T08:24:00.000Z","kind":"article","comments":true,"tags":["packaging","mesos"],"content":"\n[Mesos] をインストールするとき各マシンでビルドはしんどいので、[fpm] で Mesos の RPM を作ってインストールしている。ビルドからパッケージ作成までの作業ログを残しておく。\n\n\u003c!-- more --\u003e\n\n- [fpm] は Ruby の gem や Node.js の npm などのプログラミング言語のライブラリ、あるいは直接ディレクトリから RPM やら Deb やらのパッケージを作成するソフトウェア。\n- [Mesos] はクラスタ構成のリソースをよしなに管理するソフトウェア。\n  - 今回の話では具体的な使い方までは触れない\n\n## 手順\n\n作業環境は CentOS 6.4 x86\\_64。\n\n Ruby をインストール。\n\n    sudo yum install ruby.x86_64 rubygems ruby-devel.x86_64 rpm-build.x86_64\n\nfpm をインストール。\n\n    sudo gem install fpm --no-rdoc --no-ri\n\nMesos のソースをダウンロード、展開。\n\n    curl -LO http://ftp.meisei-u.ac.jp/mirror/apache/dist/incubator/mesos/mesos-0.10.0-incubating/mesos-0.10.0-incubating.tar.gz\n    tar xf mesos-0.10.0-incubating.tar.gz\n    cd mesos-0.10.0\n\nMesos のビルドに必要なパッケージをインストール。\n\n    sudo yum install gcc-c++.x86_64 patch.x86_64 python-devel.x86_64 \\\n      cppunit-devel.x86_64 java-1.6.0-openjdk-devel.x86_64\n\nビルド。今回は、configure のオプションで Redhat っぽい配置を指定している。`/opt/mesos` とか `/usr/local/mesos` に全部まとめたければ --prefix を使うなど、このあたりはお好みで。\n`make install` 時には書き込み可能な場所を DESTDIR で指定。説明中では、`/tmp/mesos` を利用している。\n\n    JAVA_HOME=/etc/alternatives/java_sdk ./configure \\\n      --bindir=/usr/bin --sbindir=/usr/sbin --libexecdir=/usr/libexec \\\n      --localstatedir=/var --libdir=/usr/lib64 --includedir=/usr/include \\\n      --datarootdir=/usr/share\n    make\n    make install DESTDIR=/tmp/mesos\n\nfpm でパッケージを作成。詳細は fpm --help を参照。注意点としては、`--description` は RPM のメタ情報 `description`, `summary` で兼用されるので、あまり長い情報を入れると `yum search` とかがごちゃごちゃすることになる。適度に切り詰めた方がいい。\n\n    fpm -s dir -t rpm \\\n      -v 0.10.0 \\\n      -n mesos \\\n      -C /tmp/mesos \\\n      -a x86_64 \\\n      --license \"ASL 2.0\" \\\n      --url \"http://incubator.apache.org/mesos/\" \\\n      --description \"Dynamic resource sharing for clusters\" \\\n      -d python-devel \\\n      -d java-1.6.0-openjdk-devel \\\n      .\n\nRPM ファイルのメタ情報やファイル一覧をチェック。\n\n    rpm -qpi mesos-0.10.0-1.x86_64.rpm\n    rpm -qpl mesos-0.10.0-1.x86_64.rpm\n\nあとは、できあがった RPM ファイルを他のマシンに持っていってインストール。\n\n    sudo yum install ./mesos-0.10.0-1.x86_64.rpm\n\n[Mesos]: http://incubator.apache.org/mesos/\n[fpm]: https://github.com/jordansissel/fpm\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-04-06-about-ansible.markdown","path":"blog/2013/04/06/about-ansible","layout":"post","title":"構成管理ツール Ansible について","createdAt":"2013-04-06T05:50:00.000Z","kind":"article","comments":true,"tags":["cm"],"content":"\n[Ansible] というサーバーの設定を管理するツールの説明。いわゆる構成管理 (CM: Configuration Management) にカテゴライズされるもので、Puppet や Chef の親戚みたいなものと考えてもらえればだいたいあってる。\n\n\u003c!-- more --\u003e\n\n## 概要\n\nリード開発者は Michael DeHaan で、現職の AnsibleWorks の前は Redhat で [Cobbler] や [Func] に携わっていたり、Puppet labs でプロダクトマネージャーしたりしているという経歴の持ち主。\n\nAnsible は Python で書かれている。同じジャンルで Python 製というと [Salt] が有名。Chef の場合、レシピを書くためには Ruby の知識が必要となってくるけど、Ansible はどんな言語でもモジュールが書けるようになっているので、運用にあたって Python の知識は必要無い。\n\n動作の点でも Puppet や Chef などのツールとまったく異なるアプローチをしている。Puppet や Chef は、サーバーとクライアントで構成され、クライアントとなるマシンはサーバーに設定を問い合わせながら、自分自身を「あるべき状態」に収束するよう変更を加えていく。Ansible の場合、サーバー側からクライアントとなるサーバー(群)に対して直接命令を送り込み結果を得る。これは [Func]、[Capistrano]、[Fabric] などに似ているが、これらのデプロイを目的としたツールにはない「何回やっても結果が同じ」(idempotence) という CM ツールらしさはちゃんと備えている。\n\nドキュメントは12ページしかなく(ちなみに、さっき数えてみたらChefのドキュメントは2834ファイルあった) 非常に習得は簡単。サーバーを立てる必要もなく、クライアントマシンもエージェントレス、加えて短期間で学習できるので手軽感は非常に高いが、モジュール機構が強力なのできわめて実用的になっている。\n\n\n\n## 基本的な概念\n\nAnsible を理解する上で重要となる、モジュールとプレーブックについて説明する。\n\n### モジュール\n\nクライアント内での動きはモジュールという形で定義する。\n\nパッケージのインストール、サービスの起動、ユーザーやグループの作成などの基本的なモジュールはあるが、実際には環境に合わせて不足分は自分でモジュールを作っていくことになる。\n\nモジュールは簡単に作れる。モジュールが役割を端的に言うと、以下を行うだけである。\n\n- 標準入力でオプションを受け取る\n- 標準出力で実行結果を返す\n\n  - 出力形式は key=value を空白でつなげたものか JSON\n\nこれができる言語であれば、シェルスクリプトでも Perl でも問題ない。\n\n### プレーブック\n\n実際の処理では単発のモジュールでサーバーの設定が終わることはないので、モジュールの使い方をまとめたものが必要になる。Ansible では、YAML で処理をまとめたものを プレーブック (Playbook)と呼んでいる。\n\n例: Apache と PHP をインストールする (webapp.yml)\n\n    - hosts: webserver\n      user: vagrant\n      sudo: yes\n      tasks:\n        - name: install apache\n          action: yum pkg=httpd state=installed\n        - name: install php\n          action: yum pkg=php state=installed\n\n例: 実行\n\n    # ansible-playbook プレーブック名\n    $ ansible-playbook webapp.yml\n\n以上は簡単な例だが、設定ファイルを配置したり、それに併せてサービスを再起動させたりといったことも記述可能。\n\nプレーブックには以下のような内容が含まれる:\n\n- hosts: 対象のホスト\n- user: 実行ユーザー\n- vars: 変数\n- tasks: タスク\n\n`vars` の変数は、テンプレート内で展開される。設定ファイル配置時にパラメータを変更、といった場合に利用する。\n\n### インストール\n\n以下では、インストールから簡単なコマンドの実行までの例を挙げる。サーバー、クライアント双方で CentOS 6.4 を利用した。\n\nAnsible を動かすためには、Python 2.6 以上と Ansible のソースコードとごくわずかな Python パッケージだけあればよい。CentOS 6 であれば Python の条件は満たせているし、EPEL で Ansible のパッケージが提供されているので、`yum` でインストール可能。\n\n    # EPEL 有効化\n    $ sudo rpm -ivh http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm\n\n    # Ansible インストール\n    $ sudo yum install ansible\n\n他の Unix 系OSであれば、`pip install ansible` でいい。\n\n    $ sudo pip install ansible\n\n次に、サーバーからクライアントに SSH でログインできるように調整しておく。\n\n    # 以下のマシンを用意した。\n    # それぞれホスト名でアクセスできる\n    #    Ansible 実行側 ... server\n    #    変更対象 ... client1, client2\n\n    # server側で公開鍵認証用の鍵を作成\n    $ $ ssh-keygen -t rsa\n\n    # client に公開鍵を配置する\n    $ ssh-copy-id client1\n    $ ssh-copy-id client2\n\n    # 試しにログインしてみる\n    # 頻繁に実行することになるので、公開鍵にパスフレーズを\n    # 設定している場合は、ssh-agent を使ってパスフレーズの\n    # 入力を省略できるようにしておく。\n    $ ssh client1\n    $ ssh client2\n\n今度は、対象のサーバーを設定してみよう。環境変数 `ANSIBLE_HOSTS` にあるファイルでサーバーの指定が可能。\n\n    $ cat \u003cEOD \u003e~/target\n    \u003e [webserver]\n    \u003e client1\n    \u003e \n    \u003e [dbserver]\n    \u003e client2\n    \u003e EOD\n    $ export ANSIBLE_HOSTS=~/target\n\n設定の中で、`[ ]` によりグループを作っている。つまり「webserver グループに client1、dbserver グループに client2 が所属している」ということを表している。グループはオプションなので、単純にホスト名を羅列するだけでもいい。試しに、対象のホストを調べてみよう。\n\n    # ansible ホストパターン --list-hosts\n\n    # ホスト名を直接指定\n    $ ansible client1 --list-hosts\n    client1\n\n    # グループ名を指定\n    $ ansible webserver --list-hosts\n    client1\n    $ ansible dbserver --list-hosts\n    client2\n\n    # all を指定した場合、全サーバーを列挙\n    $ ansible all --list-hosts\n    client1\n    client2\n\nこれだけで準備は完了。実行してみる。\n\n    # コマンドの書式\n    ansible 対象 -m モジュール名 -a オプション\n\n    # 例 ping モジュール\n    $ ansible all -m ping\n    client2 | success \u003e\u003e {\n        \"changed\": false,\n        \"ping\": \"pong\"\n    }\n    \n    client1 | success \u003e\u003e {\n        \"changed\": false,\n        \"ping\": \"pong\"\n    }\n\n`-m` をつけないで、直接コマンドを実行することも可能。\n\n    # すべてのマシンでカーネルのバージョンを取得\n    $ ansible all -a 'uname -r'\n    client2 | success | rc=0 \u003e\u003e\n    2.6.32-358.el6.x86_64\n    \n    client1 | success | rc=0 \u003e\u003e\n    2.6.32-358.el6.x86_64\n\nプレーブックを実行したときは以下のようになる。\n\n    # 対象は webserver というグループ(client1 が所属)に対して、\n    # Apache と PHP をインストールするプレーブック、webapp.yml を実行\n    # Apache はすでにインストールされていたので、\n    # PHP のみインストールされることとなった\n\n    $ ansible-playbook webapp.yml\n    \n    PLAY [webserver] *********************\n    \n    GATHERING FACTS *********************\n    ok: [client1]\n    \n    TASK: [install apache] *********************\n    ok: [client1]\n    \n    TASK: [install php] *********************\n    changed: [client1]\n    \n    PLAY RECAP *********************\n    client1                        : ok=3    changed=1    unreachable=0    failed=0\n\n\n## その他\n\n- [Vagrant もバージョン 1.2 から Ansible でのプロビジョニングをサポート予定][vagrant ansible support]\n- 開発は活発\n- リリース名がヴァンヘイレンの曲名 (1.0 は Eruptionだった)\n- ロゴがださい (ML でも 90年代のデザインなんて言われている)\n\n## まとめ\n\nロゴのセンスは悪いけど、アプリケーション自体の仕組みはすごくセンスがいい。\n\n他の構成管理ツールと比べると、DSL を覚えるといった「ツールを使うまでののコスト」、ツールのためのサーバー構築・運用といった「ツールを使ってからのコスト」が軽微なので、よりやりたいことに目が向けられるのもうれしい。\n\n最近日本国内でも Chef の話題を聞くことが多いんだけど、Chef Server の運用とかオートスケールとのコンビネーションとかの情報はあまり聞かないので、たぶん割と小規模な環境でリモートサーバーの Chef-solo をキックみたいなケースが多いのかと思う。そういったところだと、Ansible のほうがふさわしいっていうことが多いんじゃないかな。\n\n[Ansible]: http://ansible.cc/\n[Github]: https://github.com/ansible\n[Salt]: http://saltstack.com/\n[Cobbler]: http://cobbler.github.io/\n[Func]: https://fedorahosted.org/func/\n[Fabric]: http://fabfile.org/\n[Capistrano]: http://capistranorb.com/\n[vagrant ansible support]: https://twitter.com/mitchellh/status/319914935910027264\n[cooltext]: http://cooltext.com/\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-03-17-logging-with-splunk-storm.markdown","path":"blog/2013/03/17/logging-with-splunk-storm","layout":"post","title":"イベント管理にSplunk Stormを使ってみる","createdAt":"2013-03-17T12:17:00.000Z","kind":"article","comments":true,"tags":["logging","visualization","splunk"],"content":"\n[Splunk] はおそらくイベント・ログ管理のツールとしてはおそらくもっとも有名で、日本でも販売展開しているので知っている人も多いかと思う。その splunk が [Splunk Storm] というサービスを始めている。試しに使ってみたのでその感想。\n\n\u003c!-- more --\u003e\n\n料金に応じて、格納可能なデータの容量が増える課金体系。無料でも1GBまで利用可能。\n\nデータの取り込みは以下の方法が提供されている:\n\n  - Syslog, Rsyslog, Syslog-ng などから転送\n  - TCP/UDP を使って直接登録\n    - `cat some_file.log | nc endpoind_hostname port` で登録可能\n  - HTTP API\n  - forwarder と呼ばれるクライアントプログラム\n    - ログの読み取りなども可能\n  - ファイルアップロード\n\n試しにアカウントをとってApacheのログ形式のデータをncでがんがん取り込んでみたところ、1GB を超えたところでもうこれ以上追加できないとのメールが届いた。結果、200万件以上のデータが登録できていた。\n\n複雑な検索式を使って特定の条件に合うレコードを弾き出したり、図示することができる。たとえば、ステータスコードでグルーピングしたグラフを表示するには、以下の検索式を指定する。\n\n    sourcetype=\"access_combined\" status=\"*\" | timechart count by status\n\nリアルタイムで計算しているらしく、新しい時間帯から古い時間帯へとどんどんグラフが追加されていく。\n\n![httpstatus](/images/2013-03-17-logging-with-splunk-storm/httpstatus.png)\n\nさすがというか、よくできている。\n\n[Splunk]: http://www.splunk.com/\n[Splunk Storm]: https://www.splunkstorm.com/storm/\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-03-17-fluentd-and-graphite.markdown","path":"blog/2013/03/17/fluentd-and-graphite","layout":"post","title":"FluentdのデータをGraphiteに出力するときのTips","createdAt":"2013-03-17T06:50:00.000Z","kind":"article","comments":true,"tags":["fluentd","graphite","logging","visualization"],"content":"\n[fluent-plugin-graphite] 利用時のメモ。\n\nOps界隈での可視化というと、ここ何年かは[Graphite]でグラフを作ってそれを他のツールで表示する、みたいなのが多い。[Fluentd]のデータを可視化したい場合は[GrowthForecast]が使われることが多いけど、[Graphite]使ってみるといろんなツールと組み合わせられておもしろい。\n\n\u003c!-- more --\u003e\n\nFluentd から Graphite へデータを送るのは [Fluent-plugin-graphite] を使えば簡単に実現できそうなんだけど、プラグイン側のインターフェース(`:key` や `:count` といったキーが必要)に合わせて入力のデータを整形する必要がある。\n\nこういった調整は、out\\_map を使うことで実現できる。\n\n    \u003csource\u003e\n      type tail\n      format apache\n      path /var/log/httpd/access_log\n      tag apache.access\n    \u003c/source\u003e\n    \u003cmatch apache.access\u003e\n      type map\n      map [[\"graphite.\" + tag, time, {\"key\" =\u003e \"graphite.apache.accesslog.code.\" + record[\"code\"], \"count\" =\u003e 1}]]\n      multi true\n    \u003c/match\u003e\n    \u003cmatch graphite.**\u003e\n      type graphite\n    \u003c/match\u003e\n\n[Fluentd]: http://fluentd.org/\n[Graphite]: http://graphite.wikidot.com/\n[GrowthForecast]: http://kazeburo.github.com/GrowthForecast/\n[fluent-plugin-graphite]: https://github.com/hotchpotch/fluent-plugin-graphite\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-03-17-fluentd-and-stathat.markdown","path":"blog/2013/03/17/fluentd-and-stathat","layout":"post","title":"Fluentdの出力結果をStatHatで可視化する","createdAt":"2013-03-17T06:48:00.000Z","kind":"article","comments":true,"tags":["fluentd","stathat","logging","visualization"],"content":"\n[Fluentd]で取得した情報を可視化したいとき、[fluent-plugin-growthforecast]を使って[GrowthForecast]にグラフを作る方法がよく知られている。[GrowthForecast]はインストール後すぐに使い始められるお手軽ツールなんだけど、それすら面倒くさい、自前で環境を作るのが面倒、というときには[StatHat]を使うと簡単に可視化を実現できるという話。\n\n\u003c!-- more --\u003e\n\n[StatHat]はシンプルなインターフェースで必要十分な機能があり、しかも無料で使えるというサービス。HTTPでデータを登録するだけできれいなグラフが簡単に生成できるので、幅広い用途で利用できる。\n\n以降、[Fluentd]と[StatHat]を組み合わせて利用するための設定を説明する。\n\n## 作業\n\n### StatHat\n\nStathatの[Sign up](https://www.stathat.com/sign_up)にアクセスしてメールアドレスを登録し、折り返し届くメール内のURLからパスワードを登録すればすぐ使い始められる。グラフを作るための下準備は不要。まずは、curl を使って直接 POST してみる。\n\n    curl -d \"email=登録時のメールアドレス\u0026stat=body temperature\u0026value=36.8\" http://api.stathat.com/ez\n\nすると、メールアドレス宛にグラフの追加が通知され、画面から確認できるようになる。\n\n(メールアドレスに`+`みたいなURLエンコードが必要な文字を含んでる場合は、 --data-urlencode を使って一つずつパラメータを指定すればいい)\n\nAPIを利用するために必要となるキーは、初期状態だと登録時のメールアドレスになっている。これは[設定画面](https://www.stathat.com/settings)から変更可能。タイムゾーンも修正できるので住んでいる地域に変更しておいた方がいい。\n\n### Fluentd\n\nFluentd から StatHat を利用するために[プラギン](https://github.com/f440/fluent-plugin-stathat)作ったので、これを利用する。\n\n    fluent-gem install fluent-plugin-stathat\n\nたとえば、よくある「HTTP ステータスコードのカウント」の場合、以下のような設定をすればいい。\n\n    \u003csource\u003e\n      type tail\n      format apache\n      path /var/log/httpd/access_log\n      tag apache.access\n    \u003c/source\u003e\n    \u003cmatch apache.access\u003e\n      type datacounter\n      unit minute\n      tag stathut.httpstatus\n      count_key code\n      pattern1 2xx ^2\\d\\d$\n      pattern2 3xx ^3\\d\\d$\n      pattern3 4xx ^4\\d\\d$\n      pattern4 5xx ^5\\d\\d$\n    \u003c/match\u003e\n    \u003cmatch stathut.httpstatus\u003e\n      type copy\n      \u003cstore\u003e\n        type  stathat\n        stat 2xx\n        ezkey your_email@example.com\n        count apache.access_200_count\n      \u003c/store\u003e\n      \u003cstore\u003e\n        type  stathat\n        stat 3xx\n        ezkey your_email@example.com\n        count apache.access_3xx_count\n      \u003c/store\u003e\n      \u003cstore\u003e\n        type  stathat\n        stat 4xx\n        ezkey your_email@example.com\n        count apache.access_4xx_count\n      \u003c/store\u003e\n      \u003cstore\u003e\n        type  stathat\n        stat 5xx\n        ezkey your_email@example.com\n        count apache.access_5xx_count\n      \u003c/store\u003e\n    \u003c/match\u003e\n\nこれで、こういったグラフが作れる。\n\n![httpstatus](/images/2013-03-17-fluentd-and-stathat/httpstatus.png)\n![4xx](/images/2013-03-17-fluentd-and-stathat/4xx.png)\n\n## まとめ\n\n[StatHat] 便利。\n\n[StatHat]: http://www.stathat.com/\n[Fluentd]: http://fluentd.org/\n[GrowthForecast]: http://kazeburo.github.com/GrowthForecast/\n[fluent-plugin-growthforecast]: https://github.com/tagomoris/fluent-plugin-growthforecast\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-02-08-create-chocolatey-package.markdown","path":"blog/2013/02/08/create-chocolatey-package","layout":"post","title":"chocolatey で Haskell Platform 用のパッケージを作る","createdAt":"2013-02-08T13:49:00.000Z","kind":"article","comments":true,"tags":["windows","chocolatey","haskell","chef"],"content":"\n[chocolatey] の仕組みに興味を持ったので、パッケージを作ってみる。\n\n\u003c!-- more --\u003e\n\n目標は「[Haskell Platform for Windows] からインストーラをダウンロードしてきてサイレントインストール」ができるパッケージを作って、[chocolatey] のリポジトリにパッケージを登録してみる。\n\n## 作業\n\n### パッケージ作成\n\n[ドキュメント](https://github.com/chocolatey/chocolatey/wiki/CreatePackages)では [warmup] (プロジェクトのひな形を作ったり、そこで生成されたファイル内の文字列を置換したりするプログラム) を使ったやり方が説明されているけど、うまく動かなかったのでガリガリ手作業でやっていく。\n\n前提として、chocolatey のインストール方法は済んでいるものとする。\n\nまずはテンプレートを手に入れる。github から直接ファイルをダウンロードでもいいけど、今回の手順では clone してみよう。\n\n    # git 入れてなければ インストール\n    cinst git\n    # パスを通すため、コマンドプロンプトから抜けて新しく立ち上げ直す\n\n    cd %ChocolateyInstall%\n    git clone https://github.com/chocolatey/chocolateytemplates.git\n    cd chocolateytemplates\\_templates\n\nどこでもいいので、作業用にフォルダを作ってそこにテンプレートをコピーする。\n\n    cd %USERPROFILE%\n    mkdir my_templates\n    cd my_templates\n    xcopy %ChocolateyInstall%\\chocolateytemplates\\_templates\\chocolatey HaskellPlatform /s /e /i\n\nいよいよテンプレートの中身を作っていく。\n\n    cd HaskellPlatform\n    ren __NAME__.nuspec HaskellPlatform.nuspec \n    # HaskellPlatform.nuspec と tools/chocolateyInstall.ps1 を開いてプレースホルダを変更\n    notepad HaskellPlatform.nuspec\n    notepad tools/chocolateyInstall.ps1\n    # HaskellPlatform は NSIS 製なので、\n    # http://nsis.sourceforge.net/Docs/Chapter3.html#3.2 より、\n    # サイレントインストールのためのコマンドラインオプションが /S をつければいい\n\n編集後のファイルは以下の通り\n\n- [HaskellPlatform.nuspec](https://github.com/f440/chocolatey-HaskellPlatform/blob/master/HaskellPlatform.nuspec)\n- [tools/chocolatey-HaskellPlatform.ps1](https://github.com/f440/chocolatey-HaskellPlatform/blob/master/tools/chocolateyInstall.ps1)\n\nパッケージングする。\n\n    cpack\n\nHaskellPlatform.{バージョン番号}.nupkg ができるはず。インストールしてみよう。\n\n    cinst HaskellPlatform -source %cd%\n\nうまくいけば、Haskell のサイレントインストールが始まる。\n\n### パッケージ登録\n\n[chocolatey] にパッケージを登録してみよう。パッケージの登録にはアカウントが必要。\n\n登録方法は 2 種類。\n\n1. アップロードフォームから \\*.nupkg をアップロード\n2. API キーを取得して、コマンドラインから push\n\n1 は簡単すぎるので、2 を試す。事前に [chocolatey] のアカウント画面から API キーを取得しておこう。\n\n    cinst nuget.commandline\n    NuGet SetApiKey \u003cyour key here\u003e -source http://chocolatey.org/\n    cpush HaskellPlatform.{バージョン番号}.nupkg\n\n登録が終われば、他のマシンから `cinst HaskellPlatform` でインストールできるようになる。\n\n## まとめ\n\nアンインストールの設定が用意されていない、といっただいぶ手抜きなものだけど簡単にできた。\n\n[chocolatey] 公式の github アカウントでは [Chef 用の cookbook](https://github.com/chocolatey/chocolatey-cookbook) を配布している。chef を使って Windows マシンをセットアップするとなると、パッケージマネージャがなければ [chef-cookbooks/windows] (Windows向けのResource/Provider) を使ってインストール方法をちまちま指定していくことになるわけだけど、[chocolatey] 使えば処理が抽象化できてよさげ。\n\n## 参考\n\n- [chocolatey]\n- [Haskell Platform for Windows]\n- [chocolateyのgithubアカウント](https://github.com/chocolatey/)\n\n[chocolatey]: http://chocolatey.org/\n[Haskell Platform for Windows]: http://www.haskell.org/platform/windows.html\n[warmup]: https://github.com/chucknorris/Warmup\n[chef-cookbooks/windows]: https://github.com/opscode-cookbooks/windows\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-02-03-installing-node-dot-js-on-windows-7.markdown","path":"blog/2013/02/03/installing-node-dot-js-on-windows-7","layout":"post","title":"Windows 上に node.js の開発環境を整える","createdAt":"2013-02-03T12:11:00.000Z","kind":"article","comments":true,"tags":["node.js","windows"],"content":"\n一身上の都合で Windows 7 上に Node.js の開発環境を整えたんだけど、コマンドラインからの操作だけで開発環境がそろう時代になっていたことに驚いた。\n\nそのときの作業メモ。\n\n\u003c!-- more --\u003e\n\nコマンドプロンプト起動:\n\n    # Chocolatey のインストール\n    @powershell -NoProfile -ExecutionPolicy unrestricted -Command \"iex ((new-object net.webclient).DownloadString('http://chocolatey.org/install.ps1'))\" \u0026\u0026 SET PATH=%PATH%;%systemdrive%\\chocolatey\\bin\n    # システムドライブ直下にインストールするの微妙……\n    # 追記: [システムディスクの直下にインストールする理由](https://github.com/chocolatey/chocolatey/wiki/DefaultChocolateyInstallReasoning)\n\n   \n    # パッケージのダウンロード\u0026インストール\n    cinst nodejs.install\n    # nodejs.install ではなく nodejs だけだとコマンドライン版プログラムだけ\n    # インストールされる。別途 npm を用意する必要が出てくるので\n    # パッケージを使った方が楽\n   \n    # Node.js にパスを通す\n    set PATH=%PATH%;%ProgramFiles(x86)%\\nodejs\n\n    # ちゃんと動くかどうか、試しに grunt をインストールしてみる\n    mkdir test\n    cd test\n    npm install grunt\n   \nあとは好みにあわせて開発ツールを `cinst` でインストールしていく\n\n- バージョン管理 (git ...)\n- エディタ (sublimetext2, vim ...)\n- データストア (redis, mongodb ...)\n- ユーティリティ (Gow, ...)\n\n## 参考\n\n- [node.js](http://nodejs.org/)\n- [chocolatey](http://chocolatey.org/)\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-01-03-log-visualization-using-logstalgia.markdown","path":"blog/2013/01/03/log-visualization-using-logstalgia","layout":"post","title":"logstalgia を使ってログを可視化","createdAt":"2013-01-03T09:50:00.000Z","kind":"article","comments":true,"tags":["logging","visualization"],"content":"\nWebサーバーのログでピンポンゲームの映像を生成する[logstalgia]。\n\n\u003c!-- more --\u003e\n\nhomebrew がインストール済みなら以下で動かせる。\n\n    gem install apache-loggen\n    brew install logstalgia\n    apache-loggen --rate 10 | logstalgia -\n\n[apache-loggen] はApacheのダミーログを生成してくれるスクリプト。便利。\n\n![logstalgia](/images/2013-01-03-log-visualization-using-logstalgia/logstalgia.png)\n\n## 参考\n\n- [logstalgia]\n- [apache-loggen]\n\n[logstalgia]: https://code.google.com/p/logstalgia/ \"logstalgia\"\n[apache-loggen]: http://mt.orz.at/archives/2012/11/apacherubygems.html \"apache-loggen\"\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2013-01-01-network-restriction-using-trickle.markdown","path":"blog/2013/01/01/network-restriction-using-trickle","layout":"post","title":"Trickleを使って帯域制限をする","createdAt":"2013-01-01T08:05:00.000Z","kind":"article","comments":true,"tags":["unix","linux","network"],"content":"\nネットワーク経由で大量のデータをやりとりしたいが、メインのサービスには影響を与えたくないという場合がよくある。`rsync`や`scp` など、大きなファイルの転送を考慮されたコマンドではネットワーク帯域を制限するオプションが用意されていることも多いが、自作のツールなどに帯域制限を実装するとなるとかなり面倒くさいことになる。\n\n\u003c!-- more --\u003e\n\nLinux で帯域制限をしたい場合、tc や cgroup を使う方法がよく知られている。ただ、「あるコマンドにネットワークが占領されないように穏やかに実行したい」というニーズに対しては大げさで、またオプションが難解だったり管理権限が必要だったりといったことから二の足を踏む感じのものだった。もっと普段使いに適したツールがないものかと探していたところ、こういったシーンでは[Tricle][trickle]がかなり有効だと言うことがわかった。\n\n## インストール\n\nDebian, Ubuntu なら公式からパッケージが提供されている。RHEL 系 OS であれば、EPEL にパッケージがあるのでそちらを利用。\n\n## 使い方\n\n### trickle\n\nコマンドの前に `trickle` をつけるだけで、簡単に帯域制限が実現できる。とりあえず、「`-d n`で n KByte/sec にダウンロードが制限」、「`-u n`で n KByte/sec に制限」だけ覚えておけばいい。\n\n    # wget のダウンロード速度を 20 KBpsに制限する例\n    #  (本当は wget も curl も --limit-rate オプションが元々あるので、こんなことしなくても大丈夫)\n    trickle -d 20 wget --verbose http://ftp.jaist.ac.jp/pub/Linux/ArchLinux/iso/2012.12.01/archlinux-2012.12.01-dual.iso\n\n実行時、`trickled` が見つからないというメッセージが出るが、これは`-s`(standaloneモード)をつけることで抑制できる。\n\n### trickled\n\n`trickled` というプログラムも利用できるようになって、`tricle`と同様にオプション`-d`, `-u`が設定可能。`trickled`を一度起動するとデーモンとなり、以降`trickle`を使って起動したコマンドの帯域は、`trickled`起動時のオプションで設定した値までに制限される。複数個のプログラムを `trickle` で起動した場合、使用している帯域の総和が `trickled`の設定値に従うことになる。\n\n## 参考\n\n- [配布元][trickle]\n- [仕組み](http://monkey.org/~marius/trickle/trickle.pdf)\n\n[trickle]: http://monkey.org/~marius/pages/?page=trickle \"trickle公式\"\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-10-07-sub-for-subcommands.markdown","path":"blog/2012/10/07/sub-for-subcommands","layout":"post","title":"サブコマンドを sub で処理する","createdAt":"2012-10-07T13:13:00.000Z","kind":"article","comments":true,"tags":["cli","shell","unix"],"content":"\n[sub][] は [37signals][] が公開しているスクリプト群。サブコマンド付きのコマンドを作りたいとき、補完やヘルプメッセージなどの便利な機能を提供してくれる。\n\n\u003c!-- more --\u003e\n\n## 使い方\n\n以下の簡単なコマンドを作って、動作を確認してみることにする。\n\n    ex. browse safari http://google.com/\n    \n    コマンド browse にサブコマンドでブラウザ(safari, chrome, opera, ...)を与え、\n    最後の引数で渡された URL が開く。URL が渡されなければ、ブラウザの起動のみ行う。\n\nなお、確認はすべて Mac OS X 10.8 上 の zsh で行っている。\n\n### 初期化\n\n    $ git clone git://github.com/37signals/sub.git browse\n    $ cd browse\n    $ ./prepare.sh browse\n    # 以下のメッセージが表示される\n\n    Preparing your 'browse' sub!\n    Done! Enjoy your new sub! If you're happy with your sub, run:\n    \n        rm -rf .git\n        git init\n        git add .\n        git commit -m 'Starting off browse'\n        ./bin/browse init\n    \n    Made a mistake? Want to make a different sub? Run:\n        git add .\n        git checkout -f\n    Thanks for making a sub!\n\n言われたとおり、コマンドを実行\n\n    $ rm -rf .git\n    $ git init\n    $ git add .\n    $ git commit -m 'Starting off foo'\n    $ ./bin/foo init\n    # 以下のメッセージが表示される。パスは作業ディレクトリに応じて変わる。\n\n    # Load browse automatically by adding\n    # the following to ~/.zshenv:\n    \n    eval \"$(/XXXXXXXX/browse/bin/browse init -)\"\n\n最後に表示されるコマンドを実行することにより、補完が有効になる(XXXXXXXX は作業ディレクトリに応じて変わる)。`browse he[tab]` を実行してみよう。\n\n    $ browse help\n    Usage: browse \u003ccommand\u003e [\u003cargs\u003e]\n    Some useful browse commands are:\n       commands               List all browse commands\n    \n    See 'browse help \u003ccommand\u003e' for information on a specific command.\n\n無事ヘルプが表示されたら、セットアップはうまくいっている。\n\n### サブコマンド作成\n\nまずはディレクトリ構造を見てみよう。\n\n    $ gfind ! -path './.git/*'\n    .\n    ./.git\n    ./bin\n    ./bin/browse\n    ./completions\n    ./completions/browse.bash\n    ./completions/browse.zsh\n    ./libexec\n    ./libexec/browse\n    ./libexec/browse-commands\n    ./libexec/browse-completions\n    ./libexec/browse-help\n    ./libexec/browse-init\n    ./libexec/browse-sh-shell\n    ./LICENSE\n    ./share\n    ./share/browse\n    ./share/browse/example\n\nlibexec/browse-SUBCOMMAND  形式でファイルを作れば、サブコマンドを追加できる。早速追加してみよう。\n\n    $ vim libexec/browse-safari\n    \n        #!/usr/bin/env bash\n        set -e\n        open -a safari $1\n        \n    $ chomod a+x libexec/browse-safari\n\nサブコマンドはシェル補完できるので、`browse saf[tab] http://google.com` といった入力が可能。問題が無ければブラウザが起動する。 ただ、これだけだと使い方がわかりづらいので、ヘルプを追加してみる。\n\n    $ vim libexec/browse-safari\n    \n        #!/usr/bin/env bash\n        #\n        # Usage: browse safari [URL]\n        # Summary: safari で指定の URL を開く\n        # Help: safari を利用して、引数で渡された URL を開く\n        # 何も URL を指定しなければ、ブラウザの起動のみ\n        \n        set -e\n        \n        open -a safari $1\n\n\nヘルプに反映されていることを確認。\n\n    $ browse help safari\n    Usage: browse safari [URL]\n\n    safari を利用して、引数で渡された URL を開く\n    何も URL を指定しなければ、ブラウザの起動のみ\n\n引数なしの `help` もメッセージが変わっている。\n\n    $ browse help\n    Usage: browse \u003ccommand\u003e [\u003cargs\u003e]\n    \n    Some useful browse commands are:\n       commands               List all browse commands\n       safari                 safari で指定の URL を開く\n    \n    See 'browse help \u003ccommand\u003e' for information on a specific command.\n\nあとは、libexec-chrome, libexec-opera, ... とサブコマンドを追加していくことができる。\n\n## 雑感\n\nプログラムを書いてもシェルの補完設定までは手が回らないことが多いので、簡単にサポートしてくれる仕組みが提供されているのはかなりよかった。\n\nシェルスクリプトの書き方はかなりばらつきがあり、自分の周りでも割とフリーダムな状況になっていたので、邪魔にならない程度のフレームワークがあればいいな、と思っていた。そういう用途にも合っていると思う。\n\n## 参考\n\n- [37signalsのブログでの紹介][blog]\n- [GitHubのリポジトリ][sub]\n\n[blog]: http://37signals.com/svn/posts/3264-automating-with-convention-introducing-sub\n[sub]: https://github.com/37signals/sub\n[37signals]: http://37signals.com/\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-09-22-head-tail.markdown","path":"blog/2012/09/22/head-tail","layout":"post","title":"head と tail の行数指定方法","createdAt":"2012-09-22T10:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"head と tail を使うとき、行数指定方法について。動作確認は GNU coreutils 8.19 で行っている。\n\n\u003c!-- more --\u003e\n\nhead と tail のは`-n 数字`(あるいは`--line 数字`)で出力する行数を指定できる。\n\n    \n    # 先頭3行を表示\n    $ seq 10 | head -n 3\n    1\n    2\n    3\n    \n    # 末尾3行を表示\n    $ seq 10 | tail -n 3\n    8\n    9\n    10\n    \n\nオプションで与える数字には、プラスがつく場合、マイナスがつく場合、何もつかない場合が考えられるが、記号がついた場合に通常異なる挙動をとる場合が出てくる。\n\n`head -n -数字`の場合は、「末尾から指定した行数を除いたもの」となる:\n\n    \n    $ seq 10 | head -n -3\n    1\n    2\n    3\n    4\n    5\n    6\n    7\n    \n\n`tail -n +数字` の場合は、「先頭から数えて指定した行以降のもの」となる:\n\n    \n    $ seq 10 | tail -n +3\n    3\n    4\n    5\n    6\n    7\n    8\n    9\n    10\n    \n\nまとめると以下の通り:\n\n\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth\u003eコマンド\u003c/th\u003e \u003cth\u003e-n -行数\u003c/th\u003e \u003cth\u003e-n 行数\u003c/th\u003e \u003cth\u003e-n\n+行数\u003c/th\u003e \u003c/tr\u003e\u003c/thead\u003e\u003ctr\u003e\u003ctd\u003ehead\u003c/td\u003e \u003ctd\u003e末尾から\n\n指定行数を除いて表示\u003c/td\u003e \u003ctd\u003e先頭から\n\n指定行数表示\u003c/td\u003e \u003ctd\u003e先頭から\n\n指定行数表示\u003c/td\u003e \u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etail\u003c/td\u003e \u003ctd\u003e先頭から\n\n指定行数以降を表示\u003c/td\u003e \u003ctd\u003e末尾から\n\n指定行数表示\u003c/td\u003e \u003ctd\u003e先頭から\n\n数えて指定した行以降表示\u003c/td\u003e \u003c/tr\u003e\u003c/table\u003e\n\n`-n 数字` の代わりに `-数字` で指定することもできるけど、-n のオプションで負数を指定しているときと混同するのでやめた方がいい。\n\n`-` や `+` オプションは境界値がどうなっているか忘れがちだし、これが必要となるような局面では `awk` を使った方が直感的に表現できる。\n\n    \n    # 3行目から5行目を表示\n    $ seq 10 | awk 3\u003c=NR \u0026\u0026 NR\u003c=5\n    3\n    4\n    5\n    \n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-09-22-riemann.markdown","path":"blog/2012/09/22/riemann","layout":"post","title":"イベント処理ツール riemannを使う","createdAt":"2012-09-22T09:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"イベント処理ツール [riemann](http://aphyr.github.com/riemann/)を使ってみたのでその感想。\n\n\u003c!-- more --\u003e\n\n  * サーバーは clojure で書かれている \n    * 設定ファイルは S 式\n  * クライアントは各言語版がある [http://aphyr.github.com/riemann/clients.html](http://aphyr.github.com/riemann/clients.html)\n  * サーバーの状態は riemann-dash という sinatra でできた Web 画面から確認できる\n  * クライアントからのメッセージはイベントと呼んでる \n    * host, service, state, time, description, tags, metric, ttl というパラメータを持っている\n  * サーバー、クライアント間は Protocol Buffer で通信する\n\n公式サイトではサーバーの tar ball と deb パッケージを配布している。動かすためには、Java で実行するだけ。\n\n    \n    $ wget [http://aphyr.com/riemann/riemann-0.1.2.tar.bz2](http://aphyr.com/riemann/riemann-0.1.2.tar.bz2)\n    $ tar xf riemann-0.1.2.tar.bz2\n    $ cd riemann-0.1.2\n    $ bin/riemann etc/riemann.config\n    \n\n設定ファイルを S 式でがりがりかけるのはおもしろくって、riemann だとこんな感じに設定できる:\n\n    \n    # 公式サイトの設定例から引用 [http://aphyr.github.com/riemann/configuring.html](http://aphyr.github.com/riemann/configuring.html)\n    \n    ; You can use any options for [https://github.com/drewr/postal.](https://github.com/drewr/postal.)\n    ;\n    ; (mailer {:from \"riemann@trioptimum.com\"\n    ;          :host \"mx1.trioptimum.com\"\n    ;          :user \"foo\"\n    ;          :pass \"bar\"})\n    \n    (streams\n      (where (and (service \"web server\")\n                  (state \"exception\"))\n             (tagged \"controller\"\n                     (email \"5551234567@txt.att.net\"))\n             (tagged \"view\"\n                     (email \"delacroix@trioptimum.com\" \"bronson@trioptimum.com\"))\n             (tagged \"model\"\n                     (email \"staff@vonbraun.mil\"))))\n    \n\n「イベント x あるいは y が n 秒以内に m 回発生したらアラート」みたいなのも設定できるみたいなので、監視ツールと組み合わせてもおもしろそう。\n\nソフトウェアの内容や使いかっては、 [fluentd](http://fluentd.org/)\nととても近いように感じた。それぞれ公式サイトに掲げられているメッセージを比較してみると、fluentd は「Fluentd is a lightweight and flexible log collector」で、riemann は「Riemann is an event stream processor」だった。fluentd はイベントを集計できる形式でログとして残すこと、riemannはイベントストリームから特定の状況をリアルタイムで見つけだすことが主眼ということかな。\n\n## 参考\n\n  * [公式サイト](http://aphyr.github.com/riemann/)\n  * [紹介ビデオ](http://vimeo.com/45807716)\n  * [紹介ビデオ](http://blog.boundary.com/2012/03/12/boundary-tech-talks-march-6th-2012/)\n  * [作者 Kyle Kingsbury](https://twitter.com/aphyr)\n  * [利用事例](http://labs.amara.org/2012-07-16-metrics.html)\n\n### 関連するサービス、同類のソフトウェア\n\n  * [fluentd](http://fluentd.org/)\n  * [boundary](http://boundary.com/)\n  * [amazon cloudwatch](http://aws.amazon.com/en/cloudwatch/)\n  * [loggly](http://www.loggly.com/)\n  * その他多くの監視ツール\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-09-22-tumbler-to-octopress.markdown","path":"blog/2012/09/22/tumbler-to-octopress","layout":"post","title":"TumblerからOctopressへの移行","createdAt":"2012-09-22T08:28:00.000Z","kind":"article","comments":true,"tags":["octopress"],"content":"\nTumblerでブログ書いていたけど、ローカルで記事書く =\u003e フォームに貼り付け =\u003e プレビューのサイクルが結構面倒くさいな、と常々思っていたので、Octopressに移行した。\n\nホスティングには [Github Pages][] を利用している。\n\n\u003c!-- more --\u003e\n\n##  手順\n\n### 設定\n\n    $ git clone git://github.com/imathis/octopress.git octopress\n\n    # テーマ入れ替える    \n    $ git clone git://github.com/tommy351/Octopress-Theme-Slash.git .themes/slash\n    $ rake 'install[slash]' # zsh だとクォートなりエスケープするなりしないと、[, ] がメタ文字として解釈される\n    # .themes/slash/{source,sass} がルートディレクトリにコピーされる\n    \nこのままだと header の canonical が設定されないかったので、同梱テンプレート `.themes/classic/source/_includes/head.html` を参考に `./source/_includes/head.html` をちょっとといじった。\n    \n### Tumbler の記事をインポート\n\n[ブログの過去記事](http://tsurayogoshi.tumblr.com/archive)を全部インポートする\n( 参考: [Goodbye Tumblr. Hello, Octopress Powered by Jekyll and Markdown!][] )\n\n    \n    $ wget -O source/tumblr.rb https://raw.github.com/stephenmcd/jekyll/master/lib/jekyll/migrators/tumblr.rb\n    $ vim source/tumblr.rb # format=\"md\" =\u003e format=\"markdown\" に書き換え\n    $ ruby -rubygems -e 'require \"./source/tumblr\"; Jekyll::Tumblr.process(\"http://tsurayogoshi.tumblr.com\", format=\"markdown\", grab_images=true)'\n    $ mv _posts/tumblr/* source/_posts/\n    $ mv post source/\n\n後は細かい調整\n\n- 画像のパスが tumblr を参照しているので、全部ダウンロードして `source/images`\n  以下に保存\n- 記事のメタデータ部分\n  - `comments: true`を追加\n  - `tags` を `categories` に書き換え。\n- 各種外部サイト向けパーツの設定\n\n`source/post` には、tumbler と同じURLでアクセスしたとき、移行後のコンテンツにアクセスするリダイレクト設定が入っている。tumbler の頃からカスタムドメインを使っていた場合は、後述のドメイン設定で前と同じドメインにすればいい。\n\n### ドメインの設定\n\n独自ドメインを使う場合、source/ 以下に CNAME というファイルを作り、そこにドメインを書いておく。その後、指定の IP アドレスに名前を向ける。\n\n何度かIPアドレスが変更になっているみたいで、別のIPアドレスを利用した説明がネットに残っているけど、古いものだとカスタムドメインが使えるけどusername.github.comからのカスタムドメインへのリダイレクトが有効にならなかったりするので、ちゃんと\n[公式の説明](https://help.github.com/articles/setting-up-a-custom-domain-with-pages)のもの\nを参照すること。\n\n### Github Pages へデプロイ\n\n[ドキュメント](http://octopress.org/docs/deploying/github/)を読めばわかるので詳細は割愛。\n\n`source` ディレクトリの中身が `public` 以下に展開されて、ここがプレビュー領域となる。`public` の中身が `_deploy` にコピーされて、ここが Github Pages に pushされる。\n\ngit リポジトリのうち `master` ブランチがは公開用、`source` が編集用となる。ルーディレクトリに `source` ブランチ、公開用の `_deploy` ディレクトリに `maste` ブランチという二つのリポジトリが配置されることになる。\n\n\n## 感想\n\nvim で書く =\u003e すぐに確認 =\u003e github にデプロイ =\u003e 公開の流れは気持ちいい。tumblr の頃と同じく、markdown で書けるのもとても具合がいい。\n\n[Github Pages]: http://pages.github.com/\n[Goodbye Tumblr. Hello, Octopress Powered by Jekyll and Markdown!]: http://blog.assimov.net/blog/2012/03/24/tumblr-to-octopress-powered-by-jekyll-and-markdown/\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-07-30-roundsman-capistrano-chef-solo.markdown","path":"blog/2012/07/30/roundsman-capistrano-chef-solo","layout":"post","title":"roundsmanを使ってcapistranoからchef-soloを実行する","createdAt":"2012-07-29T15:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"管理対象のサーバー台数が少ない場合など、[chef](http://www.opscode.com/chef/)のサーバーを運用するコストとベネフィットを天秤にかけてみて、ああこれどう考えても労力ペイできないな、でも設定ファイルを手動で管理するのはやだな、といったときに[roundsman](https://g\nithub.com/iain/roundsman)を使うといいという話。\n\n\u003c!-- more --\u003e\n\n[roundsman](https://github.com/iain/roundsman)は、chefのレシピを転送して[chef-solo](http://wiki.opscode.com/display/chef/Chef+Solo)を実行する[capistrano](https://github.com/capistrano/capistrano)向けライブラリ。アプリケーションのリリースタイミングに併せてインフラ設定の変更が必要になることは往々にしてあるので、[capistrano](https://github.com/capistrano/capistrano)を使ってデプロイとインフラ設定変更を一括適\n用できるのは便利だ。\n\nここでは、Railsアプリを対象に[roundsman](https://github.com/iain/roundsman)適用までの作業を簡単にまとめる。\n\n## 手順\n\nまずは適当なRailsプロジェクトを作るところから。\n\n    \n    PROJECT=\"my_fantastic_project\"\n    rails new $PROJECT\n    cd $PROJECT\n    \n    $EDITOR Gemfile\n      # 追加\n      gem roundsman, :require =\u003e false\n      gem capistrano, :require =\u003e false\n    \n    bundle install --path vendor/bundle\n    \n    # capistranoのCapfile、config/deploy.rbを生成\n    bundle exec capify .\n    \n\nchefのcookbooksは`config/cookbooks`に配置する。場所は設定で変更可能。このディレクトリだけ別リポジトリにしておくと、ほかのプロジェクトでも転用できて便利なのでそうしてる。\n\nconfig/deploy.rbを調整する。サーバーの種別ごとにデプロイを切り替えたいので、マルチステージを有効化。\n\n    \n    $EDITOR config/deploy.rb\n    \n    # 追加\n    # require roundsman/capistrano\n    # require capistrano/ext/multistage\n    \n\nサーバーグループの設定を`config/deploy/*.rb`に配置。これについては、[capistrano/ext/multistage](https:\n//github.com/capistrano/capistrano/wiki/2.x-Multistage-Extension)の説明を参照。\n\nあとは`config/deploy.rb`でrecipeを実行するタスクを追加し、`config/deploy/*.rb`の中でattributeを設定していく。\n\n    \n    config/deploy.rb:\n    \n        namespace :chef do\n          set :care_about_ruby_version, false\n    \n          # 一括して適用\n          task :default do\n            roundsman.run_list fetch(:run_list)\n          end\n    \n          # 個別にレシピ適用 (ex. nginx)\n            namespace :nginx do\n              task :install do\n                roundsman.run_list \"recipe[nginx]\"\n              end\n            end\n    \n          end\n    \n\n[githubにある設定方法の説明](https://github.com/iain/roundsman#configuration)だと、config/ステージ名.rb に設定を書いている。\n\n    \n    config/deploy/*.rb:\n    \n        set :nginx, :user =\u003e \"nginx\", \"worker_process\" =\u003e 1, …\n        set :run_recipe, :user =\u003e \"nginx\", \"worker_process\" =\u003e 1, …\n    \n\nただ、これだとattributesの管理がcapistranoの中にべったり書くことになってしまい、chef-soloを手で実行したいときとか面倒くさい。そのため、attributesの値はknifeやchef-\nsoloで読めるようなjsonを作って、config/roles 以下で管理している。\n\nroles ディレクトリはアプリのアップデートと関係なく更新していくことになるので、別リポジトリで管理した方がいい。\n\n    \n    ファイル構成(抜粋)\n    \n      ├── Capify\n      ├── Gemfile\n      └── config\n            ├── cookbooks\n            ├── deploy\n            └── roles\n    \n    config/deploy.rb:\n    \n      # jsonファイルを取り込む関数を追加\n      require active_support/core_ext/hash/deep_merge\n      def load_role(*roles)\n        json = {}\n        roles.each do |role|\n          json_path = \"#{File.dirname(__FILE__)}/roles/#{role}.json\"\n          json.deep_merge! JSON.load(File.new(json_path))\n        end\n        json.each {|k,v| set (k.to_sym), v }\n     end\n    \n    config/deploy/*.rb:\n    \n      # 読み込みたいjsonファイルを指定\n      load_role \"web\"\n    \n    config/roles/*.json:\n    \n     例: config/roles/web.json\n      {\n         \"nginx\" : {\n          \"user\" : \"nginx\",\n          \"worker_processes\" : 1,\n        …\n         \"run_list\" : [ \"recipe[nginx]\", ... ]\n      }\n    \n\n以上で準備が整った。これで実行できるようになる。\n\n    \n    # 一括適用\n    bundle exec cap ステージ名 chef\n    \n    # cookbook を指定して適用\n    bundle exec cap ステージ名 chef:nginx\n    \n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-06-05-glusterfs.markdown","path":"blog/2012/06/05/glusterfs","layout":"post","title":"分散ファイルシステム GlusterFS を使う","createdAt":"2012-06-04T15:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"Webアプリケーションを構築する上で、運用中に発生したファイルをローカルのファイルシステム上に保管すると、スケールを阻害するため好ましくないことが多い。\n\n\u003c!-- more --\u003e\n\nそのため、アプリケーションの設計の段階からCDNの利用したり、ファイルの管理だけ別のサービスに切り出したりすることを考慮すべきだけど、いろいろなしがらみのた\nめにどうしてもファイルを複数台のサーバーで共有するようなシステム形態にせざるを得ないことが往々にしてある。\n\nサーバー間のファイル共有のための方法として、[lsyncd](http://code.google.com/p/lsyncd/) や[DRBD](http://www.drbd.org/)を使ったり、NASを介したりするなど様々な方法があるけど、[GlusterFS](http://www.gluster.or\ng/) がとても便利。特別な機器を必要とせず、すでにある環境に対して導入でき、信頼性とスケーラビリティのあるクラスタリングファイルシステムを手早く構築するこ\nとができる。\n\nGlusterFS を簡単に説明すると、以下のような特徴がある:\n\n  * 分散型ファイルシステム \n    * SPOFになるような特殊ノードも必要ない\n  * NFSやCIFSでマウント可能 \n    * 先日発表された 3.3.0 で、HDFSとの互換性できてHadoopから処理できるようになったり、OpenStack Object Storage API互換の REST APIが提供されたりでいろいろ熱い感じになっている\n  * ストライピングで性能を上げたり、レプリケーションで耐障害性をあげたりすることが可能\n\n今回は仮想マシンで動作を検証するまでの流れをまとめる。\n\n## 環境構築\n\n作業環境として、Mac OS X Lion上のVirtualBoxを利用し、仮想マシンとしてはCentOS 6.2\nx86_64を使う。Windowsでやる場合は`vagrant ssh`が動かないので、そのあたりを読み替えればできると思う。\n\nはじめにCentOS 6.2のマシンイメージを作る。\n\n    \n    $ gem install vagrant veewee\n    $ mkdir work\n    $ cd work\n    $ vagrant basebox define CentOS-6.2-x86_64-minimal CentOS-6.2-x86_64-minimal\n    $ vagrant basebox build CentOS-6.2-x86_64-minimal # マシンイメージのビルド\n    $ vagrant basebox validate CentOS-6.2-x86_64-minimal # チェック\n    $ vagrant basebox export CentOS-6.2-x86_64-minimal\n    $ vagrant box add CentOS-6.2-x86_64-minimal CentOS-6.2-x86_64-minimal.box\n    $ cd ..\n    $ rm -rf ./work\n    \n\n次にクラスタ構成の設定。\n\n    \n    $ mkdir -p ~/Documents/vagrant/glusterfs/ # 作業用ディレクトリ作成\n    $ cd ~/Documents/vagrant/glusterfs/\n    $ vim Vagrantfile # 編集\n    \n\n[https://gist.github.com/2868494](https://gist.github.com/2868494)\n\n    \n    $ vagrant up # 3台の仮想マシン起動\n    \n\n必要となる仮想マシンがそろったので、glusterfsのセットアップを始める。\n\n    \n    $ cd ~/Documents/vagrant/glusterfs # この中は 共有ディレクトリを通して、仮想マシンの/vagrantからも参照可能\n    $ curl -LO [http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-3.3.0-1.el6.x86_64.rpm](http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-3.3.0-1.el6.x86_64.rpm)\n    $ curl -LO [http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-fuse-3.3.0-1.el6.x86_64.rpm](http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-fuse-3.3.0-1.el6.x86_64.rpm)\n    $ curl -LO [http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-server-3.3.0-1.el6.x86_64.rpm](http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-server-3.3.0-1.el6.x86_64.rpm)\n    \n\n仮想マシンに必要となるパッケージをインストールしておく。\n\n    \n    $ brew install parallel # 一台ずつ設定するの面倒なので、gnu parallel 使う\n    $ parallel vagrant ssh {} -c sh -c \"sudo yum -y install wget fuse fuse-libs\" ::: host1 host2 host3\n    $ parallel vagrant ssh {} -c sh -c \"sudo yum install -y /vagrant/glusterfs-*\" ::: host1 host2 host3 # パッケージインストール\n    $ parallel vagrant ssh {} -c sh -c \"/usr/sbin/glusterfs -V\" ::: host1 host2 host3 # 動作確認\n    $ parallel vagrant ssh {} -c sh -c \"sudo /sbin/service iptables stop\" ::: host1 host2 host3 # iptables 停止\n    $ parallel vagrant ssh {} -c sh -c \"sudo /sbin/service glusterd start\" ::: host1 host2 host3 # 起動\n    \n\n以降、`$` から始まるのはホストOS、`hostX$` から始まるのは仮想マシン上のターミナルの説明とする。\n\n## ストレージプール作成\n\nストレージプールと呼ばれる、サーバー間の信頼済みネットワークを作成する。\n\n    \n    $ vagrant ssh host1\n    \n    host1$ sudo gluster peer probe 192.168.56.11 # host2 をプールに追加\n    host1$ sudo gluster peer probe 192.168.56.12 # host3 をプールに追加\n    # 自ホスト(host1)の追加は不要\n    \n\n## ボリューム作成\n\nストレージプールを構成したら、ボリュームを作成する。\n\nボリュームは「分散するかどうか」「レプリケーションするかどうか」「ストライピングするかどうか」を選ぶことになる。組み合わせることも可能。ひとまず2台構成で分\n散、ストライピング、レプリケーションのそれぞれについて試してみる。\n\n### 分散\n\nファイルをストレージ内のどこかしらに保存しておく形態。追加すればするほど大きなストレージとなるけど、冗長性などは確保されない。\n\nhost1, host2 で分散ボリュームを作ってみる。\n\n    \n    $ parallel vagrant ssh {} -c sh -c \"sudo mkdir -p /export/vol\" ::: host1 host2\n    $ vagrant ssh host1\n    \n    host1$ sudo gluster volume create vol 192.168.56.10:/export/vol 192.168.56.11:/export/vol\n    \n\n### ストラインピング\n\n性能向上を目的として、ファイルを複数に分割して保存しておく形態。RAID0みたいな感じ。\n\nhost2, host3 でストライピングボリュームを作ってみる。\n\n    \n    $ parallel vagrant ssh {} -c sh -c \"sudo mkdir -p /export/vol-striping\" ::: host2 host3  \n    $ vagrant ssh host1\n    \n    host1 $ sudo gluster volume create vol-striping stripe 2 192.168.56.11:/export/vol-striping 192.168.56.12:/export/vol-striping\n    \n\n### レプリケーション\n\nデータの複製を作って、複数の場所に保管しておく形態。RAID1みたいな感じ。信頼性が高くなり、ファイルの読み込みも早くなる。\n\nhost1, host3 でレプリケーションボリュームを作ってみる。\n\n    \n    $ parallel vagrant ssh {} -c sh -c \"sudo mkdir -p /export/vol-replica\" ::: host1 host3\n    $ vagrant ssh host1\n    \n    host1$ sudo gluster volume create vol-replica replica 2 192.168.56.10:/export/vol-replica 192.168.56.12:/export/vol-replica\n    host1$ sudo gluster volume start vol-replica\n    \n\n## 利用\n\n### マウント\n\nOSにマウントしてみる。マウント方法にはNFSやCIFSなども選べるけど、ここではネイティブのglusterfs形式を選んでみる。\n\n    \n    $ vagrant ssh host1\n    \n    host1$ sudo mkdir -p /mnt/{vol,vol-striping,vol-replica}\n    host1$ sudo mount -t glusterfs 192.168.56.10:/vol /mnt/vol # 分散\n    host1$ sudo mount -t glusterfs 192.168.56.11:/vol-striping /mnt/vol-striping # ストライピング\n    host1$ sudo mount -t glusterfs 192.168.56.12:/vol-replica /mnt/vol-replica # レプリケーション    \n    \n\n### 動作確認\n\nはじめに、マウントした結果を見てみる。\n\n    \n    $ df -h /mnt/*\n    Filesystem            Size  Used Avail Use% Mounted on\n    192.168.56.10:vol      17G  1.9G   14G  12% /mnt/vol\n    192.168.56.12:vol-replica\n                          8.4G  949M  7.0G  12% /mnt/vol-replica\n    192.168.56.11:vol-striping\n                           17G  1.9G   14G  12% /mnt/vol-striping\n    \n\n分散、ストライピングは2台分を足し合わせた結果になっている。レプリケーションは2台に同じデータが分散されるので、ディスク効率は50%に下がる。\n\n#### 分散\n\n適当にファイルを作ってみる。\n\n    \n    host1$ sudo touch /mnt/vol/{1..9}\n    \n    # 保管先をチェック\n    \n    host1$ ls /export/vol/ # 1  5  7  8  9\n    \n    host2$ ls /export/vol/ # 2  3  4  6\n    \n\nファイルがばらばらと格納されていることがわかる。\n\n### ストライピング\n\n    \n    host1$ sudo vi /mnt/vol-striping/sample.txt # 10M強データをテキストデータを書き込み\n    \n    host1$ du -s /mnt/vol-striping/sample.txt # 10256と表示された\n    host1$ ls -l /mnt/vol-striping/sample.txt # サイズが 10484785 と表示された\n    \n    # 保管先をチェック\n    host2$ du -s /export/vol-striping/sample.txt # 5128 と表示された\n    host2$ ls -l /export/vol-striping/sample.txt # サイズが 10354688 と表示された\n    \n    host3$ du -s /export/vol-striping/sample.txt # 5128 と表示された\n    host3$ ls -l /export/vol-striping/sample.txt # サイズが 10484785 と表示された\n    \n\nduの結果（ディスクのセクタ）はちょうど半分ずつに分割されるけど、ファイルの実際のサイズは元ファイルと同じ場合と異なる場合の2パターンが検出できた。これは、\nファイルがスパースファイルなっているため、見かけ上のサイズと実際にディスク上で利用しているサイズが異なっていることが原因。\n\n### レプリケーション\n\n適当なファイルを作ってみる。\n\n    \n    host1$ sudo dd if=/dev/urandom of=/mnt/vol-replica/dummy bs=1M count=10\n    host1$ sha1sum /mnt/vol-replica/dummy # 54b5c383e96d511249f9393de060c3219549e030 だった\n    \n    # 保管先をチェック\n    host1$ sha1sum /export/vol-replica/dummy # 54b5c383e96d511249f9393de060c3219549e030 だった\n    \n    host2$ sha1sum /export/vol-replica/dummy # 54b5c383e96d511249f9393de060c3219549e030 だった\n    \n\n同じ内容のファイルが複数箇所に保存されることがわかった。\n\n## メモ\n\nなんとなくでも使い始められちゃうくらい簡単に使えるけど、[ドキュメント](http://gluster.org/community/documentatio\nn/index.php/Main_Page)の[PDF](http://www.gluster.org/wp-\ncontent/uploads/2012/05/Gluster_File_System-3.3.0-Administration_Guide-en-\nUS.pdf) がわかりやすくコンパクトにまとまっていて、全体像を理解するのはここからここから始めるといいと思う。\n\n## 参考\n\n  * [Gluster Community のドキュメント](http://www.gluster.org/community/documentation/index.php/Main_Page)\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-06-03-sphinx-guard-livereload.markdown","path":"blog/2012/06/03/sphinx-guard-livereload","layout":"post","title":"sphinxの更新をguard-livereloadで検知してブラウザを自動リロードする","createdAt":"2012-06-02T15:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"sphinxでドキュメントを書く際に生じる「文章の記述 =\u003e ビルド =\u003e\nブラウザでの確認」という一連のサイクルを人力でやるのは効率が悪い。いろいろな省力化対策が考えられるが、ここでは guard-livereloadを使って、文章のビルドとブラウザのリロードを自動化する方法を説明する。\n\n\u003c!-- more --\u003e\n\n## 作業環境\n\n検証に使った環境は以下の通り。環境に依存する部分は少ないので、他のOSでも動くと思う。\n\n  * Mac OS X Lion\n  * ruby 1.9.3-p194\n  * sphinx 1.1.3\n\n## 事前準備\n\n### サーバー側準備\n\n用意するのは3ファイル\n\n  * Gemfile … 必要なライブラリをまとめてインストールするための設定ファイル\n  * Gaurdfile … ファイルシステム監視の設定ファイル\n  * Procfile … Webサーバーとファイル監視を起動するための設定ファイル\n\n[https://gist.github.com/2862843](https://gist.github.com/2862843)\n\nこれら3ファイルをsphinxの作業ディレクトリ内に配置する。製生後のhtmlファイルは`buld/html`ディレクトリに格納されていることを期待した設定になっているので、必要であれば適宜修正する。\n\nファイルの設置が終わったら、ライブラリをインストールする。\n\n    \n    bundle install\n    \n\n### ブラウザ側準備\n\n好きなブラウザにlivereloadのブラウザ拡張をインストールする。\n\n[http://help.livereload.com/kb/general-use/browser-\nextensions](http://help.livereload.com/kb/general-use/browser-extensions)\n\n## 利用方法\n\nサーバー側でファイルの監視とlivereloadを開始する。\n\n    \n    foreman start\n    \n\nブラウザで http://localhost:3000/ (3000以外にしたい場合は Procfile 内で変更)にアクセスしてlivereloadのブラウザ拡張を有効化すれば、あとはファイルの更新に合わせて自動的にビルドとブラウザのリロードが行われる。\n\n## 参考\n\n  * [LiveReloadが超気持ちいい2011](http://aligach.net/diary/20110925.html) Livereloadの詳しい説明\n  * [Auto Reload](https://addons.mozilla.org/en-US/firefox/addon/auto-reload/) ローカルファイルの更新を検知してFirefoxをリロードしてくれるアドオン。試してみたけど、自分の環境ではリロードがうまく動かなかった。\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-05-13-vps-lxc-xtradb-cluster.markdown","path":"blog/2012/05/13/vps-lxc-xtradb-cluster","layout":"post","title":"さくらのVPSにLXCで仮想環境構築してXtraDB Cluster動かす","createdAt":"2012-05-12T15:00:00.000Z","kind":"article","comments":true,"tags":["mysql","lxc","linux"],"content":"ほんの数年前までVPSといえばLinode、Slicehostなどの海外のサーバーしか選択肢がなかった。ls を実行しても一呼吸おくほどのレイテンシーがあるような環境で、128MBくらいのメモリを何とかやりくりしてRailsを動かしていたが、現在では月1000円程度で用途によっては手に余るようなスペックが手に入るようになっている。そんなVPSの余ったリソースを使って、仮想環境をたてようというのが今回の目的だ。\n\n\u003c!-- more --\u003e\n\n[LXC](http://lxc.sourceforge.net/)は、他の仮想化方式と比べてオーバーヘッドが少なくきびきび動くし、必要であればCPUやメモリの制限をつけることもできる。RPMやDebのパッケージ作成をしたり、Chefのcookbook作成などで独立した環境を手軽に構築したい人には特に有用に思う。また、簡単にネットワークが作れるので、複数台構成のソフトウェアを1台のマシンのなかで動かすことが出来る。今回は動作確認として [PerconaXtraDB Cluster](http://www.percona.com/software/percona-xtradb-cluster/)を動かしてみることにする。\n\n## 前提について\n\n作業環境は以下を想定している。\n\n  * さくらのVPS(v3) 1G \n    * CentOS 6.2 x86_64\n  * LXC 0.7.5\n\nCentOSを使っているのはデフォルトのOSイメージだからというのが理由。\n\n今回の内容をEC2上で実行する場合、Amazon Linux でもほとんど同様の設定で作業を行えることを確認しているけど、もっ と新しいOS、たとえば Ubuntu 12.04 LTS を使えば後述する cgroupの設定、bridgeの設定が不要となるためより簡単に行える。CentOS 6で実施したときだけ遭遇するような問題に何度もぶつかったので、出来るだけ新しいOSを使った方がいい。\n\n仮想環境としては、lxcに同梱されているテンプレートを利用してUbuntuを、またOSイメージの作成からCentOSを構築する。\n\n## 構築方法\n\n以降の作業はすべて root で行うものとする。\n\n### ネットワークの設定\n\n仮想環境とのやりとりで使うブリッジを作る。\n\n    \n    # yum install bridge-utils\n    # vim /etc/sysconfig/network-scripts/ifcfg-lxcbr0\n    \n        DEVICE=lxcbr0\n        TYPE=Bridge\n        BOOTPROTO=none\n        IPADDR=10.0.3.1\n        NETMASK=255.255.255.0\n        ONBOOT=yes\n    \n    # ifup lxcbr0 # 起動\n    \n\n### cgroup\n\n    \n    # mount | grep cgroup # cgroup がないこと確認\n    # mkdir -p /cgroup\n    # printf \"none\t\t\t/cgroup\t\tcgroup\tdefaults\t\t0 0\n    \" \u003e\u003e /etc/fstab\n    # mount -a\n    # mount | grep cgroup # cgroup があること確認\n    \n\n### lxc セットアップ\n\n    \n    # yum install libcap-devel docbook-utils\n    # yum groupinstall \"Development Tools\"\n    \n    # wget [http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz](http://lxc.sourceforge.net/download/lxc/lxc-0.7.5.tar.gz)\n    # tar xf lxc-0.7.5.tar.gz\n    # cd lxc-0.7.5\n    # ./configure\n    # make rpm # この途中で /usr/lib64/lxc/{template,rootfs} がインストールされるのかなり狂ってる\n    # rpm -ivh ~/rpmbuild/RPMS/x86_64/lxc-0.7.5-1.x86_64.rpm\n       (~/rpmbuild になければ、/usr/src/rpm から探す)\n    # mkdir -p /var/lib/lxc\n    \n\n### dnsmasq (DHCP, DNS サーバー) セットアップ\n\n環境を増やすごとに毎回NICの設定を編集するのは手間なので、ホスト側で dncp, dns の設定をする。\n\n    \n    # yum install dnsmasq\n    # vim /etc/dnsmasq.conf\n    \n        コメントを外して有効化する、編集するなどで以下の設定を行う\n        domain は自分の使いたい名前にすればいい\n    \n        domain-needed\n        bogus-priv\n        interface = lxcbr0\n        listen-address = 127.0.0.1\n        listen-address = 10.0.3.1\n        expand-hosts\n        domain = lxc\n        dhcp-range = 10.0.3.50,10.0.3.200,1h\n    \n    # service dnsmasq reload\n    \n\n### ネットワークセットアップ\n\n仮想環境から外部へのやりとりが出来るようにネットワークの設定を変更する。\n\n    \n    # sysctl -w net.ipv4.ip_forward=1\n    # sed -i -re s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/ /etc/sysctl.conf\n    # iptables -A POSTROUTING -s 10.0.3.0/24 -t nat -j MASQUERADE\n    # service iptables save # 設定を /etc/sysconfig/iptables に保存\n    \n\n### 仮想環境構築 (1) 同梱のスクリプトを使った Ubuntu のインストール\n\nlxcに同梱のスクリプト /usr/lib64/lxc/templates/lxc-ubuntu を使ってUbuntuをインストールする。\n\n基本的な設定ファイルを作る。\n\n    \n    # cd\n    # vim lxc.conf\n    \n        lxc.network.type=veth\n        lxc.network.link=lxcbr0\n        lxc.network.flags=up\n    \n\n今回は Ubuntu を導入するので、そのために必要なプログラムをインストールする。\n\n    \n    # yum install --enablerepo=epel debootstrap dpkg\n    \n\nこれで準備が出来たので、実際に仮想環境を動かしてみる。\n\n    \n    # lxc-create -t ubuntu -f lxc.conf -n vm0\n       -t がテンプレートの名前。 -t ubuntu なら /usr/lib64/lxc/templates/lxc-ubuntu が読み込まれる\n          オプションでバージョンが指定可能だが、lxc 0.7.5 に同梱されているテンプレートのデフォルトだと Ubuntu 10.04 が選ばれる。\n       -f がさっき作った設定ファイルの場所\n       -n が環境の名前。今回は vm0 とした。 /var/lib/lxc/vm0 にファイルがおかれる\n    # lxc-start -n vm0 -l debug -o debug.out -d\n       -l はデバッグレベル、-o はデバッグの場所を指定。安定して起動するようになったらつけなくていい\n    # lxc-console -n vm0\n      一回エンター押した後、ユーザー root パスワード root でログイン\n      抜けるときは Ctrl-a q\n    \n      lxc-console をしても何も表示されない状態になったら、以下を施して再起動\n    \n    # vim /var/lib/lxc/vm0/rootfs/etc/init/lxc.conf\n    \n      telinit を差し込む\n    \n        --- /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf.orig 2012-02-07 10:28:25.000000000 +0900\n        +++ /var/lib/lxc/vm0/rootfs/etc/init/lxcguest.conf      2012-05-06 22:43:21.606098530 +0900\n        @@ -12,5 +12,6 @@\n            touch /var/run/utmp\n            chown root:utmp /var/run/utmp\n            initctl emit --no-wait net-device-added INTERFACE=lo || true\n        +   telinit 3\n            exit 0\n         end script\n    \n\nlxc-console だとCtrl-aが使えなくて不便なので、今後はsshでログインしたい。テンプレートが自動的にOpenSSHをインストールしてくれるが、ちゃんと起動しない。仕方が無いので、update-rc.d で起動するように設定\n\n    \n      仮想環境内で実行\n    # update-rc.d ssh enable\n    \n\n固定IPアドレスを振りたい場合は、設定を変更する。\n\n    \n      ホスト側からの変更\n    # vim /var/lib/lxc/vm0/config\n    \n      lxc.network.ipv4 = 10.0.3.2/24\n    \n      仮想環境の中で変更\n    # vim /etc/network/interfaces\n    \n        変更前\n        auto lo\n        iface lo inet loopback\n    \n        auto eth0\n        iface eth0 inet dhcp\n    \n        変更後\n        auto lo\n        iface lo inet loopback\n    \n        iface eth0 inet static\n            address 10.0.3.2\n            netmask 255.255.255.0\n            gateway 10.0.3.1\n    \n\n仮想環境の破棄は lxc-destroy で行う\n\n    \n    # lxc-destroy -n vm0\n    \n\n### 仮想環境構築 (2) 独自に構築した CentOS 6 のインストール\n\nlxc-console の標準テンプレートでは CentOS が用意されていないので、自力でセットアップする。\n\n#### イメージ作成\n\n基本的に [Centos6/Installation/Minimal installation using yum](http://wiki.1tux.org/wiki/Centos6/Installation/Minimal_installation_using_yum) の通り。ただし 64 bit 版をインストールする\n\n    \n    # mkdir /t\n    # cd /t\n    # wget [http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm](http://mirrors.kernel.org/centos/6/os/x86_64/Packages/centos-release-6-2.el6.centos.7.x86_64.rpm)\n    # rpm2cpio centos-release-6-2.el6.centos.7.x86_64.rpm  | cpio -idm\n    # sed -i s/$releasever/6/g ./etc/yum.repos.d/*\n    # yum --installroot=/t groupinstall base\n    # yum --installroot=/t install dhclient\n    # rm centos-release*.rpm\n    # chroot /t\n    \n      // ここから後はchroot内\n    \n    # passwd # パスワード変更\n    \n    # rm -f /dev/null\n    # mknod -m 666 /dev/null c 1 3\n    # mknod -m 666 /dev/zero c 1 5\n    # mknod -m 666 /dev/urandom c 1 9\n    # ln -s /dev/urandom /dev/random\n    # mknod -m 600 /dev/console c 5 1\n    # mknod -m 660 /dev/tty1 c 4 1\n    # chown root:tty /dev/tty1\n    \n    # mkdir -p /dev/shm\n    # chmod 1777 /dev/shm\n    # mkdir -p /dev/pts\n    # chmod 755 /dev/pts\n    \n    # cp -a /etc/skel/. /root/.\n    \n    # cat \u003e /etc/resolv.conf \u003c\u003c END\n    # Google public DNS\n    nameserver 8.8.8.8\n    nameserver 8.8.4.4\n    END\n    \n    # cat \u003e /etc/hosts \u003c\u003c END\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n    END\n    \n    # cat \u003e /etc/sysconfig/network \u003c\u003c END\n    NETWORKING=yes\n    HOSTNAME=localhost\n    END\n    \n    # cat \u003e /etc/sysconfig/network-scripts/ifcfg-eth0  \u003c\u003c END\n    DEVICE=eth0\n    ONBOOT=yes\n    BOOTPROTO=dhcp\n    END\n    \n    # cat \u003e /etc/fstab \u003c\u003c END\n    /dev/root               /                       rootfs   defaults        0 0\n    none                    /dev/shm                tmpfs    nosuid,nodev    0 0\n    END\n    \n    # cat \u003e /etc/init/lxc-sysinit.conf \u003c\u003c END\n    start on startup\n    env container\n    \n    pre-start script\n            if [ \"x$container\" != \"xlxc\" -a \"x$container\" != \"xlibvirt\" ]; then\n                    stop;\n            fi\n            telinit 3\n            initctl start tty TTY=console\n            exit 0;\n    end script\n    END\n    \n    # exit\n    \n    // ここから後はchroot外\n    \n    # cd /t\n    # tar cvfz /centos6-lxc-root.tgz .\n    \n\n#### 設定\n\n    \n    # mkdir /var/lib/lxc/vm0\n    # cd /var/lib/lxc/vm0\n    # mkdir rootfs\n    # cd rootfs\n    # tar xfz /centos6-lxc-root.tgz --numeric-owner\n    # cd /var/lib/lxc/vm0\n    \n    # cat \u003e/var/lib/lxc/vm0/config \u003c\u003c END\n    lxc.network.type=veth\n    lxc.network.link=lxcbr0\n    lxc.network.flags=up\n    lxc.network.veth.pair=veth-vm0\n    lxc.utsname = vm0\n    \n    lxc.tty = 1\n    lxc.pts = 1024\n    lxc.rootfs = /var/lib/lxc/vm0/rootfs\n    lxc.mount  = /var/lib/lxc/vm0/fstab\n    lxc.arch = x86_64\n    lxc.cap.drop = sys_module mac_admin\n    \n    lxc.cgroup.devices.deny = a\n    # Allow any mknod (but not using the node)\n    lxc.cgroup.devices.allow = c *:* m\n    lxc.cgroup.devices.allow = b *:* m\n    # /dev/null and zero\n    lxc.cgroup.devices.allow = c 1:3 rwm\n    lxc.cgroup.devices.allow = c 1:5 rwm\n    # consoles\n    lxc.cgroup.devices.allow = c 5:1 rwm\n    lxc.cgroup.devices.allow = c 5:0 rwm\n    # /dev/{,u}random\n    lxc.cgroup.devices.allow = c 1:9 rwm\n    lxc.cgroup.devices.allow = c 1:8 rwm\n    lxc.cgroup.devices.allow = c 136:* rwm\n    lxc.cgroup.devices.allow = c 5:2 rwm\n    # rtc\n    lxc.cgroup.devices.allow = c 254:0 rwm\n    #fuse\n    lxc.cgroup.devices.allow = c 10:229 rwm\n    #tun\n    lxc.cgroup.devices.allow = c 10:200 rwm\n    #full\n    lxc.cgroup.devices.allow = c 1:7 rwm\n    #hpet\n    lxc.cgroup.devices.allow = c 10:228 rwm\n    #kvm\n    lxc.cgroup.devices.allow = c 10:232 rwm\n    END\n    \n    # cat \u003e fstab  \u003c\u003c END\n    proc            /var/lib/lxc/vm0/rootfs/proc         proc    nodev,noexec,nosuid 0 0\n    sysfs           /var/lib/lxc/vm0/rootfs/sys          sysfs defaults  0 0\n    END\n    \n\n#### 起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.out -d\n    # lxc-console -n vm0\n    \n    OpenSSH がなければ入れておく\n    # yum install openssh-server\n    # service sshd start\n    \n\n## 動作確認 (Percona XtraDB Cluster の稼働確認)\n\n動作確認として Percona XtraDB Cluster を動かしてみる。\n\nすでにこれまでの作業を通して vm0 としてCentOS 6がインストール済みとする。\n\n### ホスト側設定\n\n  * 構成 \n    * ホスト, IPアドレス 10.0.3.1\n    * 仮想0 vm0, IPアドレス 10.0.3.2\n    * 仮想1 vm1, IPアドレス 10.0.3.3\n    * 仮想2 vm2, IPアドレス 10.0.3.4\n\n各仮想環境に簡単にアクセスできるように hosts を設定しておく。ホスト側に設定しておけば、dnsmasq のおかげで仮想側でも名前が引けるようになる。\n\n    \n    # vim /etc/hosts\n        以下を追記\n        10.0.3.2 vm0\n        10.0.3.3 vm1\n        10.0.3.4 vm2\n    \n\n### コピー元(vm0) 設定\n\n    \n    # ssh vm0\n      ここからはvm0の中\n    \n      固定IPアドレスを設定\n    # vim /var/lib/lxc/vm1/rootfs/etc/sysconfig/network-scripts/ifcfg-eth0\n        DEVICE=eth0\n        ONBOOT=yes\n        BOOTPROTO=static\n        IPADDR=10.0.3.3\n        NETMASK=255.255.255.0\n        GATEWAY=10.0.3.1\n    \n      XtraDB Cluster インストール\n    # rpm -Uhv [http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm](http://repo.percona.com/testing/centos/6/os/noarch/percona-testing-0.0-1.noarch.rpm)\n    # rpm -Uhv [http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm](http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm)\n    # yum install Percona-XtraDB-Cluster-server Percona-XtraDB-Cluster-client\n    # cat \u003e /etc/my.cnf \u003c\u003cEND\n    [mysqld]\n    binlog_format=ROW\n    wsrep_provider=/usr/lib64/libgalera_smm.so\n    wsrep_cluster_address=gcomm://\n    wsrep_slave_threads=2\n    wsrep_cluster_name=lxccluster\n    wsrep_sst_method=rsync\n    wsrep_node_name=node0\n    innodb_locks_unsafe_for_binlog=1\n    innodb_autoinc_lock_mode=2\n    END\n    \n    # poweroff\n    \n\n### コピー、起動\n\n    \n    # lxc-clone -n vm1 -o vm0\n      -n はこれから作る仮想環境の名前\n      -o はコピー元の仮想環境の名前\n    # lxc-clone -n vm1 -o vm0\n    # vim /var/lib/lxc/vm1/config\n      vm0をvm1に置換 (vm2ではvm2に置換)\n      IPアドレスを10.0.3.2 -\u003e 10.0.3.3 に変更 (vm2では 10.0.3.4に変更)\n    # vim /var/lib/lxc/vm1/rootfs/etc/my.cnf\n        wsrep_cluster_address=gcomm:// をwsrep_cluster_address=gcomm://10.0.3.2 に変更\n        wsrep_node_name=node0 を wsrep_node_name=node1 に変更 (vm2ではnode2に変更)\n    \n      同様にvm0からvm2のコピーを実施\n    \n\n3つの環境が完成したら起動\n\n    \n    # lxc-start -n vm0 -l debug -o debug.0.out -d\n    # lxc-start -n vm1 -l debug -o debug.1.out -d\n    # lxc-start -n vm2 -l debug -o debug.2.out -d\n    \n\n### 動作確認\n\nvm0 にログインして実行\n\n    \n    # mysql -u root\n      データベース、テーブル作成\n    mysql\u003e create database t;\n    mysql\u003e use t;\n    mysql\u003e create table sample (\n    id int not null primary key auto_increment,\n    value int\n    );\n    \n    データ投入\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e insert into sample set value = 1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    \n\nIDがスキップしながらインサートされることがわかる。引き続き、他の環境でもデータを入れてみる。\n\nvm1 にログインして実行\n\n    \n    mysql\u003e use t;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    +----+-------+\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e insert into sample set value =  1;\n    mysql\u003e select * from sample;\n    +----+-------+\n    | id | value |\n    +----+-------+\n    |  2 |     1 |\n    |  5 |     1 |\n    |  8 |     1 |\n    |  9 |     1 |\n    | 12 |     1 |\n    | 15 |     1 |\n    +----+-------+\n    \n\n同様のことがvm2でも起きる。\n\nこれにより、XtraDB Cluster の以下の動作が確認出来た。\n\n  * すべてのサーバーで書き込みと参照がおこなえること\n  * オートインクリメントがバッティングしないように、値が自動的にオフセットをつけて挿入されること\n\n## メモ\n\n### 外部から仮想環境へ直接アクセスしたい場合\n\nたとえば、外部からポート10080でアクセスされたとき、仮想環境の 10.0.3.51 のポート 80 へ転送させたい場合は iptables\nで以下のような設定をする。\n\n    \n    # vim /etc/syscofig/iptables\n        -A POSTROUTING -s 10.0.3.0/24 -j MASQUERADE の下に以下を追加\n        -A PREROUTING -i eth0 -p tcp --dport 10080 -j DNAT --to-destination 10.0.3.51:80\n    # service iptables condrestart\n    # iptables -L -t nat # NATテーブルから設定追加を確認\n    \n\n### 新しい Ubuntu を入れたい場合\n\n元の手順だとlucid (10.04) がインストールされるが、たとえば oneiric (11.10) であれば以下でインストール可能。\n\n    \n    # cp -a /usr/share/debootstrap/scripts/lucid  /usr/share/debootstrap/scripts/oneiric\n        lucid は /usr/share/debootstrap/scripts/gutsy のシンボリックリンクで、他のリリースも同様。とにかくファ イル名が参照できるようにシンボリックリンクをコピーしておけばいい。\n    # lxc-create -t ubuntu -f lxc.conf -n vm0 -- --trim -r oneiric\n        lxc-create ではなく -r はテンプレートへの引数\n    \n\n### 他の OS もインストールしてみたい場合\n\n/usr/lib64/lxc/templates/ には lxc-busybox,lxc-debian,lxc-fedora,lxc-lenny,lxc-\nopensuse,lxc-sshd,lxc-ubuntu の テンプレートがある。これ以外の環境が必要であれば、「lxc guset\nOS名」とかで検索してみる。\n\n## 参考\n\n  * [http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc](http://www.activestate.com/blog/2011/10/virtualization-ec2-cloud-using-lxc)\n  * [http://wiki.debian.org/LXC](http://wiki.debian.org/LXC)\n  * [https://help.ubuntu.com/12.04/serverguide/lxc.html](https://help.ubuntu.com/12.04/serverguide/lxc.html)\n  * [http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104](http://www.lacerta.be/d7/content/lxc-installation-ubuntu-server-1104)\n  * [http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6](http://wiki.1tux.org/wiki/Lxc/Installation/Guest/Centos/6)\n  * [http://www.percona.com/doc/percona-xtradb-cluster/index.html](http://www.percona.com/doc/percona-xtradb-cluster/index.html)\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2012-01-03-rundeck-jenkins-java.markdown","path":"blog/2012/01/03/rundeck-jenkins-java","layout":"post","title":"rundeckをセットアップして、jenkins上のjava成果物をデプロイする","createdAt":"2012-01-02T15:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"rundeck でjenkins上の成果物をデプロイしよう、という話。\n\n\u003c!-- more --\u003e\n\n## rundeck について\n\n[公式サイト](http://rundeck.org/)\n\nITオペレーションのコンサルやってる[DTO Solution](http://www.dtosolutions.com/)（Depops関連の資料とかでよく会社名は見かけますね）が作っているデプロイ用のツール。元々は[ControlTier](http://doc36.controltier.org/wiki/Main_Page)っていう管理ツールがあって、そこから分家した。ControlTierはサーバー/クライアントモデルだけど、サーバー側しか用意しなくていいRundeckのほうがお手軽度高い。\n\n複数のサーバーを対象に状態を変更するのが目的で、[capistrano](https://github.com/capistrano/capistrano)とか[fabric](http://docs.fabfile.org)とかと同じジャンル。GUIで操作するのが特徴なので、[webistrano](https://github.com/peritor/webistrano)\nとかに近い。\n\nGUI（笑）みたいに思うかもしれないけど、画面上から履歴が確認できたり、ブラウザがあればどこからでもデプロイ出来るのって、運用の敷居下げるのに貢献してくれると思う。\n\n## rundeck 設定\n\n### インストール\n\nとりあえずインストールしてみる。以降の説明は、rundeck インストールサーバー、デプロイ対象サーバーともに CentOS 5 の場合。\n\n[http://rundeck.org/docs/RunDeck-Guide.html#installing-rundeck](http://rundeck.org/docs/RunDeck-Guide.html#installing-rundeck) , [http://kb.dtosolutions.com/wiki/Rundeck_on_CentOS](http://kb.dtosolutions.com/wiki/Rundeck_on_CentOS) 参照。CentOSならyumで簡単にインストールできる。\n\n    \n    $ sudo rpm -Uvh [http://repo.rundeck.org/latest.rpm](http://repo.rundeck.org/latest.rpm)\n    $ sudo yum install rundeck\n    \n\nまずはインストールされたファイルを確認してみよう。\n\n    \n    $ rpm -ql rundeck\n    $ rpm -ql rundeck-config\n    \n\n以下のようなことがわかる。\n\n  * 設定系のファイル /etc/rundeck は本体と別の RPM (rundeck-config) に入っている\n  * /var/lib/rundeck 以下にシステム関係のデータがおかれて、/var/rundeck 以下にユーザーが作成したデータをおくっぽい\n    * 僕は試してないけど、保存先はDBも使えるみたい。http://rundeck.org/docs/RunDeck-Guide.html#relational-database\n\n### 起動\n\nさっそく起動してみる。\n\n    \n    $ sudo /sbin/service rundeckd start\n    \n\n既定のポートは 4440 なので http://RUNDECK_HOST:4440/ にアクセスしてみる。\n\n![ログイン画面](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx82ozkABF1qz5yk8.png)\n\nアクセスできなければ、ちゃんと起動出来てるかどうかとか、iptables が邪魔していないかとか確認。この時点ではまだログイン出来ない。\n\nログインできるようにするために、ユーザーを作る。[公式サイトの説明](http://rundeck.org/docs/RunDeck-Guide.html\n#managing-logins)\n\nパスワードのハッシュ化はmd5sumコマンドとかでもいいけど、手順に沿って付属のライブラリ使ってみる。RPMでインストールすると、説明文中の$RUNDECK_BASE相当がないので、読み替えて以下のように実行\n\n    \n    $ cd /var/lib/rundeck/\n    $ java -cp exp/webapp/WEB-INF/lib/jetty-6.1.21.jar:exp/webapp/WEB-INF/lib/jetty-util-6.1.21.jar org.mortbay.jetty.security.Password f440 secret_password\n    OBF:1vny1vn61unn1z7e1vu91ytc1r3x1xfj1r411yta1vv11z7o1uob1vnw1vn4\n    MD5:be6cb1069f01cd207e6484538367bd1d\n    CRYPT:f4Ou7EnVsEzMg\n    \n\nユーザー一覧に追加\n\n    \n    $ sudo vim /etc/rundeck/realm.properties\n    // 末尾に以下を追加\n    f440: MD5:be6cb1069f01cd207e6484538367bd1d,admin,user\n    \n\nこれで利用可能になった。ユーザー情報を読み込むために、サービス再起動 （今後もユーザー設定の変更ごとに再起動させる）\n\n    \n    $ sudo /sbin/service rundeckd restart\n    \n\nアクセス出来るようになったはず。ログインしてみる。\n\n![ログイン直後](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx838slUR51qz5yk8.png)\n\nやりましたね。\n\nプロジェクトの名前はお好きに。SSHのキーについては、RPMインストール時に作られるrundeckユーザーのキーが`/home/rundeck/.ssh/rundeck.id_rsa`なので、ここにしておくと手間が少なくて済む。他の値については、今回はデフォルトで。\n\nこの後説明するホストやジョブはプロジェクト単位で管理していくことになる。\n\n![Run](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx840qzSI71qz5yk8.png)\n\n左上のメニューに注目\n\n  * Run … 一回こっきりのコマンド。非定型な処理（緊急でアプリケーションサーバー順々に再起動かけたいとか）はここから実行できる。capistrano の `cap shell` みたいなイメージ\n  * Job … 複数のコマンドや条件を保存はここに登録。\n  * Histoly … 実行履歴が確認出来る。\n\n最初は localhost だけがホストに登録されているから、Run を選択後、真ん中の入力フォームからコマンドを実行してみる。\n\n![exec_uname](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx84c0tw7g1qz5yk8.png)\n\n実行できた。\n\n### ホスト追加\n\nローカルホストにばかりいじっていても不毛なので、ホストを追加していく。ユーザー同様ホストについても設定ファイルを編集する必要がある。\n\nrundeck をインストールしたサーバーで公開鍵をメモ\n\n    \n    rundeck$ sudo su - rundeck\n    rundeck$ cat .ssh/rundeck.id_rsa.pub # 出力結果をメモ\n    \n\n各ホストに作業用ユーザー「deploy」を追加する - パスワード不要でsudo可能。 - パスワードは設定しない - SSH\nの鍵認証でパスフレーズ無しにログイン可能\n\n    \n    以下、デプロイ対象サーバー(仮に192.168.10.10とする)\n    192.168.10.10$ sudo /sbin/useradd deploy\n    192.168.10.10$ sudo /usr/sbin/visudo\n    \n      // tty が使えないとSSH経由のコマンドに問題が起きるので、無効化する\n      Defaults:deploy    !requiretty\n      deploy  ALL=(ALL)       NOPASSWD: ALL\n    \n    192.168.10.10$ mkdir -m 700 /home/deploy/.ssh\n    192.168.10.10$ sudo vim /home/deploy/.ssh/authorized_keys # rundeck の公開鍵を登録\n    192.168.10.10$ sudo chown -R deploy.deploy /home/deploy/.ssh\n    \n\nいったん rundeck 側からログインしてみる。\n\n    \n     ここからはまた rundeck をインストールしたサーバーの話\n    $ sudo su - rundeck\n    $ touch .ssh/config\n    $ chmod 600 .ssh/config\n    $ vim .ssh/config\n        このあともがしがしサーバー追加していくので、LAN内のマシンについては指紋チェック無効化しておく\n        Host 192.168.*.*\n            StrictHostKeyChecking no\n    $ ssh -i ~/.ssh/rundeck.id_rsa deploy@192.168.10.10\n    \n\n次に設定ファイル編集\n\n    \n    $ sudo vim /var/rundeck/projects/example/etc/resources.xml\n    \n    以下のような行を追加。name, hostname, username 辺りは重要だけど、それ以外は適当に指定。絞り込みの時に使えるので、tags あたりはしっかり入力しておいたほうがいい。\n    \u003cnode name=\"target1\" description=\"適当\" tags=\"適当\" hostname=\"192.168.10.10\" osArch=\"適当\" osFamily=\"適当\" osName=\"適当\" osVersion=\"適当\" username=\"deploy\" /\u003e\n    \n\nこれで再起動させればrundeck側からホストが操作できるようになっているはず。target1(192.168.115.60), target2(192.168.115.61)を追加して画面から確認してみる。\n\n![フィルタ変更](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx96zd197m1qz5yk8.png)\n\nフィルタの変更でとりあえず全部外して、全台表示にする。\n\n![フィルタ変更後](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx96zqf2uJ1qz5yk8.png)\n\n確認できた。ここでコマンドを打てば全台に適用される。\n\n### ジョブ追加\n\nサーバーがセットアップ出来たので、ジョブを追加していく。メニューから`Jobs`を選択して、`New job`をクリックすればいい。全部画面に書いてあるけど、一応説明すると:\n\n  * Saved this job? ジョブを保存するかどうか \n    * Job Name … 名前\n    * Group … グループ。スラッシュ区切りで入力しておくと、階層構造で表示してくれる\n    * Description … 説明\n    * UUID … UUID （これとは別に、ジョブを作ると勝手にID割り振られる）\n  * Project どのプロジェクトを対象とするか\n  * Workflow \n    * Keepgoing エラーで止まるかそのまま進むか\n    * Strategy ノードが3台、ステップが2個あったとして、node1-step1, node1-step2, node1-step3, node2-step1 … とすすむのがNode-oriented、Node-oriented、node1-step1, node2-step1, node3-step1 と進んでいくのが Step-oriented\n  * Step 実行するステップを指定。各行の右端にマウスをあわせると（入れ替えたり編集、削除したりできる） \n    * Command … コマンドの実行（Runでやったのと同じ）\n    * Script … 複数行のスクリプトの実行\n    * Script file … サーバー上にあるスクリプトファイルの実行\n    * Job Reference … 他のジョブを実行\n  * Dispatch to Nodes … これを選択しないと、ローカルホストだけで実行される。選択すると実行対象の絞り込み画面が表示されるので、タグやホスト名、その他条件を設定する。\n  * Log level … ログの出力多寡を決定\n\nサクサク作れるので、必要に応じてがしがし増やしていく。\n\n![](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx99jg4sze1qz5yk8.png)\n\n## jenkins との連動\n\nrundeck は[jenkins](http://jenkins-ci.org/)および[rundeckプラグイン](https://wiki\n.jenkins-ci.org/display/JENKINS/RunDeck+Plugin)と連動して利用することが出来る。\n\nプラグインはjenkinsのプラグイン管理画面に表示されるので、それを選択するだけでいい。\n\n### jenkins から rundeck をキックする\n\njenkinsでビルド完了→rundeckでデプロイ→jenkinsで統合テスト実施、といった0-clickのデプロイパイプが作れるようになる。[0-clickは革命](http://www.otsune.com/diary/2008/09/11/1.html#200809111) 。\n\njenkinsの設定方法は[プラグインの説明ページ](https://wiki.jenkins-ci.org/display/JENKINS/RunDeck+Plugin#RunDeckPlugin-DeploymentPipeline)参照。\n\n### rundeck からjenkins上の成果物を選択できるようにする。\n\nrundeckで手動デプロイするとき、成果物名やビルドの名前を選択できるようにする。\n\njenkins rundeck プラグインは以下を利用出来るようにしてくれる:\n\n  * 特定の成果物を起点に、ビルド履歴とその際の成果物を提供するAPI\n  * 特定のビルドを起点に、その最新成果物一覧を提供するAPI\n\nrundeck でジョブを実行するとき、ユーザー入力を受け付けることが出来るんだけど、この選択項目には外部から取得したJSONなども設定できる。この機能と先ほどのAPIを組み合わせることで実現出来る。\n\nやってみよう。ジョブの保存画面でオプションを選択。\n\n![オプション設定画面](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx9bfrJrHn1qz5yk8.png)\n\n![オプション設定画面2](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx9c296d8I1qz5yk8.png)\n\n  * Option Name … 変数名として使われる値。artifact を指定\n  * Description … 適当\n  * Default Value … 未設定でいい\n  * Allowed Value … Remote URL からAPIのURLを指定（URLの形式は[ドキュメント参照](https://wiki.jenkins-ci.org/display/JENKINS/RunDeck+Plugin#RunDeckPlugin-OptionProvider)）\n  * Restrictions … 値の形式チェック。Remote URLの値しか指定させたくないので、「Enforced from Allowed Values」を指定\n  * Requirement … 必須かどうか。必須なので、当然「yes」\n  * Multi-valued … 複数の値をとれるようにするか。複数の成果物を同時にデプロイしたい、とかであれば使えるかもしれないけど、今回は「No」\n  * Usage … ここで指定した値をステップの部分でどのように使えばいいのか説明してくれている。\n\n併せて、受け取った値を表示するだけのステップを作ってみる。\n\n![ステップ](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx9cj2ACEP1qz5yk8.png)\n\n実行してみよう。最初にビルド番号を聞かれる。\n\n![成果物一覧](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx9cobBFg31qz5yk8.png)\n\n適当に選んで「Run Job Now」すると、さっき作った変数を表示するだけのスクリプトが動いて、指定した成果物のURLが表示され、連携がうまくいっていたことが確認出来る。\n\n![実行結果](/images/2012-01-03-rundeckjenkinsjava/tumblr_lx9dlhjwmr1qz5yk8.png)\n\n詳細なデプロイ手順については、各環境ごとにあるだろうからアレンジしてもらえればと思う。\n\n## まとめ\n\nホストやユーザーの設定をいちいちファイル編集しなくちゃいけなかったりするのが、ちょっとかっこわるいかな。ただ、UIはわかりやすいし、セットアップも簡単なので\n、気軽に試してみるといいと思う。\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2011-12-31-mysql-sandbox-blackhole.markdown","path":"blog/2011/12/31/mysql-sandbox-blackhole","layout":"post","title":"MySQL::Sandboxで環境を作ってBLACKHOLEエンジンを試す","createdAt":"2011-12-30T15:00:00.000Z","kind":"article","comments":true,"tags":["mysql"],"content":"ちょっとMySQLのBLACKHOLEエンジン使って調べたいことがあったんだけど、\n[MySQL::Sandbox](http://mysqlsandbox.net/) 使うとレプリケーション環境が簡単に構築できて便利。\n\n\u003c!-- more --\u003e\n\n以下は、MySQL::SandboxのセットアップからBLACKHOLEエンジン使うところまでの記録。\n\n### 前提\n\n  * 確認した環境は Debian 6.0.3, perl v5.12.3\n  * cpanm \u0026 perlbrew インストール済み（`cpan`コマンドでも問題無いはず）\n  * MySQL 5.5 はサーバーを起動するためにlibaio1が必要なので、あらかじめパッケージをインストールしておく\n\n### 手順\n\n#### インストール\n\n`cpanm` でMySQL::Sandbox をインストール。\n\n    \n    $ cpanm MySQL::Sandbox\n    \n\nこれで make_sandbox などのコマンド群がインストールされる。\n\nデータファイルは以下のようなファイル構成を取る\n\n  * $HOME/opt/mysql 以下のサブディレクトリにサーバーを設置 (環境変数 SANDBOX_BINARY で変更可能)\n  * $HOME/sandboxes 以下のサブディレクトリにMySQLのデータや起動スクリプトを設置 (環境変数 SANDBOX_HOME で変更可能)\n\n#### サーバーセットアップ\n\nサーバーを起動してみる。\n\n    \n    # 使用するディレクトリ設定\n    $ export SANDBOX_BINARY=$HOME/opt/mysql\n    $ export SANDBOX_HOME=$HOME/opt/sandboxes\n    # ソース取得\n    $ cd SANDBOX_HOME\n    $ curl -L -o mysql-5.5.19-linux2.6-x86_64.tar.gz http://www-jp.mysql.com/get/Downloads/MySQL-5.5/mysql-5.5.19-linux2.6-x86_64.tar.gz/from/http://ftp.jaist.ac.jp/pub/mysql/\n    # Sandbox 作成\n    $ make_sandbox $SANDBOX_BINARY/mysql-5.5.19-linux2.6-x86_64.tar.gz\n    \n\nこれで$SANDBOX_BINARY/5.5.19 にサーバーが、また $SANDBOX_HOME/msb_5_5_19 以下にインスタンスが作成される。\n\nこの時点で起動出来ているはずなので、接続してみる。以下で mysql クライアントが起動するはず。\n\n    \n    $ $SANDBOX_HOME/msb_5_5_19/use\n    \n\n`use`以外には、`start`, `stop`, `status` などの名前から動作が推測できそうなコマンド群がある。$SANDBOX_HOMEには複数のサンドボックスを操作するコマンド `use_all`, `start_all`, `stop_all` などがある。\n\n    \n    $ make_replication_sandbox 5.5.19 \n    # 第二引数はtar.gzまでのパスでもいいが、一度展開されたらバージョン番号指定でもいい。make_sandboxも同様\n    \n\n以上で、 master, node1, node2 というサーバーが起動する。node1, node2 は masterを参照したレプリケーション構成となる。デフォルトでノードは2台だが、`--how_many_nodes` オプションで台数は変更可能。\n\n同様に[Circular recplication](http://dev.mysql.com/doc/refman/5.1/ja/replication-\ntopology-circular.html)(日本語訳だとなんになるんだろう)も簡単に作れる。マスター/マスターレプリケーションはCircular replication が2台のみの構成だった場合に同じ。\n\n    \n    $ make_replication_sandbox --circular=4 5.5.19\n    \n\nこれで node1 -\u003e node2, node2 -\u003e nod3, node3 -\u003e node4, node4 -\u003e node1 という循環関係のレプリケーションが作れる。\n\n使い終わったら止めておこう。\n\n    \n    $ $SANDBOX_HOME/stop_all\n    \n\n### BLACKHOLEエンジンを試す\n\n準備ができたので、[BLACKHOLEエンジン](http://dev.mysql.com/doc/refman/5.1/ja/blackhole-storage-engine.html)を使ってみる。BLACKHOLEエンジンはバイナリログは記録するが、データは残さないストレージエンジンのこと。\n\nまずは、サンドボックス作成\n\n$ make_replication_sandbox --circular=3 5.5.19\n\nデフォルトストレージエンジンをnode1, node3はInnoDB、node2 はBLACKHOLEに変更\n\n    \n    $ echo default_storage_engine=InnoDB \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node1/my.sandbox.cnf\n    $ echo default_storage_engine=BLACKHOLE \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node2/my.sandbox.cnf\n    $ echo default_storage_engine=InnoDB \u003e\u003e $SANDBOX_HOME/rcsandbox_5_5_19/node3/my.sandbox.cnf\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/restart_all\n    \n\nnode2, node3 のレプリケーションだけ再開して、node1 -\u003e node2 -\u003e node3 の2階層スレーブにする。\n\n    \n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node2/use -e start slave\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node3/use -e start slave\n    \n\n実験のための構成が完成した。ここで、node1 にデータを流し込んだとき、node2 にはデータが残らなくて、node3 にデータが出来たら成功。\n\nサンプルデータには、[Sample database with test suite](https://launchpad.net/test-\ndb/)を利用する。\n\n    \n    $ curl -LO [http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2](http://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2)\n    $ tar xf employees_db-full-1.0.6.tar.bz2\n    $ cd employees_db\n    # InnoDB が決めうちで設定されているので、コメントアウト\n    $ sed -i -re s/^\\s+(set storage_engine = InnoDB;)/-- \\1/ *.sql\n    # 取り込み\n    $ $SANDBOX_HOME/rcsandbox_5_5_19/node1/use  -t \u003c ./employees.sql\n    \n\n結果\n\n    \n    $ cd $SANDBOX_HOME\n    $ du -sh rcsandbox_5_5_19/node?/data/ibdata1\n    219M    rcsandbox_5_5_19/node1/data/ibdata1\n    18M     rcsandbox_5_5_19/node2/data/ibdata1\n    219M    rcsandbox_5_5_19/node3/data/ibdata1\n    \n\nnode2 だけデータファイルがふくらまない。\n\n    \n    $ ls -l rcsandbox_5_5_19/node2/data/\n    合計 357652\n    drwx------ 2 f440 f440      4096 2011-12-31 17:15 employees/\n    -rw-rw---- 1 f440 f440   5242880 2011-12-31 17:14 ib_logfile0\n    -rw-rw---- 1 f440 f440   5242880 2011-12-31 17:11 ib_logfile1\n    -rw-rw---- 1 f440 f440  18874368 2011-12-31 17:14 ibdata1\n    -rw-rw---- 1 f440 f440        88 2011-12-31 17:16 master.info\n    -rw-rw---- 1 f440 f440      5925 2011-12-31 17:14 msandbox.err\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 mysql/\n    -rw-rw---- 1 f440 f440      6273 2011-12-31 17:14 mysql-bin.000001\n    -rw-rw---- 1 f440 f440 168403385 2011-12-31 17:16 mysql-bin.000002\n    -rw-rw---- 1 f440 f440        38 2011-12-31 17:14 mysql-bin.index\n    -rw-rw---- 1 f440 f440       315 2011-12-31 17:14 mysql_sandbox15902-relay-bin.000005\n    -rw-rw---- 1 f440 f440 168399499 2011-12-31 17:16 mysql_sandbox15902-relay-bin.000006\n    -rw-rw---- 1 f440 f440        76 2011-12-31 17:14 mysql_sandbox15902-relay-bin.index\n    -rw-rw---- 1 f440 f440         5 2011-12-31 17:14 mysql_sandbox15902.pid\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 performance_schema/\n    -rw-rw---- 1 f440 f440        75 2011-12-31 17:16 relay-log.info\n    drwx------ 2 f440 f440      4096 2011-12-31 17:11 test/\n    \n\nnode2 のバイナリログ、リレーログはちゃんと出来ているので、BLACKHOLEエンジンの適用を確認できた。\n\n### メモ\n\n  * `$SANDBOX_HOME/clear_all` でデータ消せるの便利。\n  * MySQL::Sandbox 3.0.19 からは、[Percona や Maria DB などの派生DBも扱える](http://mysqlsandbox.net/news.html)みたい。いろいろなバージョンで試すことが多いのでうれしい。\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2011-12-24-superviser.markdown","path":"blog/2011/12/24/superviser","layout":"post","title":"プロセス管理にsuperviserを使う","createdAt":"2011-12-23T15:00:00.000Z","kind":"article","comments":true,"tags":null,"content":"ちょっと前にsupervisorを使ったのでやり方をまとめておく。\n\n\u003c!-- more --\u003e\n\n公式サイト: [supervisor](http://supervisord.org/)\n\nsupervisorはプログラムの起動、停止を管理するツール。daemontools\nがわかるなら、あんな感じのソフトだと思ってもらえれば早い。daemontoolsよりも標準的なディレクトリ構成で使えるぶん、導入障壁は低いと思う。\n\n最近のLinuxなら、パッケージですぐインストール出来る。以降の説明は debian (6.0.3)、superviser 3 系によるもの。\n\n## インストール\n\n    \n    $ sudo apt-get install superviser\n    \n\n### 確認\n\n    \n    $ sudo service supervisor status # 起動確認（名前表示されない……）\n     is running\n    $ sudo service supervisor stop # 停止\n    Stopping supervisor: supervisord.\n    $ sudo service supervisor start # 起動\n    Starting supervisor: supervisord.\n    \n\n## 例\n\n例のために、自分のホームにインストールしてあったmemcachedを起動してみる。\n\nまずは、コマンドラインから問題無く起動出来ることを確認\n\n    \n    $ /home/f440/opt/bin/memcached # 起動\n    \n\nプログラムはフォアグラウンドで起動する。memcached の例で言うと、-d オプション (run as a daemon) はつけない。\n\n設定ファイルを配置する。もともと /etc/supervisor/supervisord.conf 内で /etc/supervisor/conf.d/\\*.conf を Include するよう設定されていた読み込むように設定されていたので、以下のようなファイルを作った。\n\n    \n    $ cat /etc/supervisor/conf.d/memcached.conf\n    [program:memcached]\n    command=/home/f440/opt/bin/memcached\n    user=f440\n    \n\nデフォルトだと設定したプログラムは自動起動する(autostart=true)なので、supervisor を再起動させれば memcachedも起動する。memcached がいつまでも起動しなければ /var/log/supervisor 以下のログを確認する。\n\n何らかの原因で停止したとき、自動起動して欲しければ autorestart=true を指定しておく。\n\n## コマンドで操作\n\n起動中は supervisorctl 経由で操作ができるようになる。\n\n    \n    $ sudo supervisor \n    memcached                        RUNNING    pid 24928, uptime 0:08:21\n    supervisor\u003e # help でヘルプを表示\n    supervisor\u003e exit\n    $ sudo supervisor status # 直接引数を渡すこともできる\n    memcached                        RUNNING    pid 24928, uptime 0:08:21\n    \n\n## Webで操作\n\n設定を追加するとWebから操作できるようになる。外部から好き放題できるので、安全な場所以外では適切な権限設定必須。\n\nポート9001で、どこからでもアクセスできるよう設定\n\n    \n    diff --git a/supervisor/supervisord.conf b/supervisor/supervisord.conf\n    index 61b3020..86e04f2 100644\n    --- a/supervisor/supervisord.conf\n    +++ b/supervisor/supervisord.conf\n    @@ -4,6 +4,9 @@\n     file=/var/run//supervisor.sock   ; (the path to the socket file)\n     chmod=0700                       ; sockef file mode (default 0700)\n    \n    +[inet_http_server]\n    +port=*:9001\n    +\n     [supervisord]\n     logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)\n     pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)\n    \n\n設定ファイル再読込\n\n    \n     $ sudo supervisor reload\n    \n\nアクセス出来るようになる\n\n![Web管理画面](/images/2011-12-24-superviser/tumblr_lx9eq9SHXa1qz5yk8.jpg)\n\n終わり。\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2011-12-17-github-yum.markdown","path":"blog/2011/12/17/github-yum","layout":"post","title":"githubを使ってyumリポジトリを公開する","createdAt":"2011-12-16T15:00:00.000Z","kind":"article","comments":true,"tags":["git","github","centos","rpm","yum"],"content":"ubuntuには[PPA](https://launchpad.net/ubuntu/+ppas)という仕組みがあり、プロジェクトホスティングサービス[launchpad](https://launchpad.net/) と連携してパッケージを簡単に配布する仕組みが用意されている。今回は、githubを使ってリポジトリにpushしたら自動的にRPMパッケージを公開する方法をまとめる。\n\n\u003c!-- more --\u003e\n\n※\nPPAだとサーバサイドで各種プラットフォームにビルドしてくれるが、そこまではサポートしない。あくまで配布だけ。RPMをどうやって作るのかについても触れない。\n\n以下は[haproxy](http://haproxy.1wt.eu/)\nを公開するときの例。haproxyのソースにはRPMのspecファイルが含まれているので、簡単にrpmが作成出来る。\n\n## 手順\n\n### git リポジトリを作成\n\n[Create a New Repository - github](https://github.com/repositories/new)\nからリポジトリを作成。\n\n### ディレクトリ構造を作成\n\n以下はCentOS 5 64bit 版とソースRPMを配布する場合\n\n    \n    $ mkdir -p haproxy-rpm/centos/5/os/{SRPMS,x86_64}\n    $ cd haproxy-rpm\n    \n\n### ファイル設置\n\n    \n    $ cp /some/path/haproxy-1.4.18-1.src.rpm centos/5/os/SRPMS/\n    $ cp /some/path/haproxy-1.4.18-1.x86_64.rpm centos/5/os/x86_64/\n    \n\n### メタデータ作成\n\n    \n    $ sudo yum install -y createrepo\n    $ createrepo centos/5/os/SRPMS/\n    $ createrepo centos/5/os/x86_64/\n    \n\n### commit \u0026 push\n\n    \n    $ git add .\n    $ git ci -m initial commit  \n    $ git remote add origin git@github.com:f440/haproxy-rpm.git\n    $ git push -u origin master\n    \n\n## 利用方法\n\n/etc/yum.repos.d 以下にわかりやすい名前でファイルを作る。\n\n    \n    サンプル /etc/yum.repos.d/haproxy-rpm-f440.repo (1行目やnameは適宜変更する)\n    \n    [haproxy-rpm-f440]\n    name=haproxy-CentOS-$releasever\n    baseurl=https://raw.github.com/f440/haproxy-rpm/master/centos/5/os/x86_64/\n    enabled=1\n    gpgcheck=0\n    \n\nあとは通常通り `sudo yum install haproxy` でインストール可能。\n\n## 備考\n\n  * createrepo はdebian, ubuntuなどにもコマンドが用意されているので、ビルド以外の作業はRHEL系以外のOSでもよい\n  * mercurialを使いたければ、[bitbucket](https://bitbucket.org)でも似たような手順で公開可能。その際は baseurl の部分に `baseurl=https://bitbucket.org/f440/haproxy-rpm/raw/tip/centos/$releasever/os/$basearch/` のような形式で記述する\n\n"},{"localPath":"/home/f440/go/src/github.com/f440/f440.github.com/content/2011-12-17-pv.markdown","path":"blog/2011/12/17/pv","layout":"post","title":"プログレスバーを簡単に表示できるコマンド pv","createdAt":"2011-12-16T15:00:00.000Z","kind":"article","comments":true,"tags":["unix","pv"],"content":"プログレスバーを簡単に表示できる `pv` について説明する。\n\n\u003c!-- more --\u003e\n\n## インストール方法\n\n自分の環境(debian 6.0.3)だと `apt`でインストール出来る。RHEL系なら[ここ](http://pkgs.repoforge.org/pv/)かな。\n\n## 使い方\n\n端的に言うと、「cat ＋ 標準エラー出力にプログレスバー」という動きを取る。\n\n    \n    f440@abhoth[10]:~$ yes | pv \u003e/dev/null\n     529MB 0:00:08 [67.2MB/s] [       \u003c=\u003e                                         ]\n    \n\n8秒で合計529MB、秒間67.2MBくらいで「y」の文字が `pv` を通り抜けてるのがわかる。 `-l`オプションをつけると行モードになり、転送量ではなく転送行数を調べられる。\n\n    \n    f440@abhoth[10]:~$ yes | pv -l \u003e/dev/null\n     435k 0:00:10 [45.9k/s] [          \u003c=\u003e                                        ]\n    \n\n10秒で435行、秒間45900行くらいが通り抜けてるのがわかる。\n\n他にもおもしろいオプションとして `-L` っていうのがあって、パイプから出てく流量を制限することが出来る。\n\n## 用途\n\n`mysqldump` とか `mysql`にくっつけてダンプ、リストアの完了時刻を予想する、とかかな。\n\nWebで見てると、`nc` とか `tcpdump` とかと組み合わせてる例がある。\n\n自分で作ったコマンドとかに、簡単に進行状況表示がくっつけられるのは便利。\n\n"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"EL7aLKI83tZi_FFCsIrzy","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>